"""
LaborLab 2 - ì¸ê³¼ì¶”ë¡  ë¶„ì„ ë©”ì¸ íŒŒì´í”„ë¼ì¸

ì „ì²´ íŒŒì´í”„ë¼ì¸:
1. ê²½ë¡œë¥¼ í†µí•´ ë°ì´í„° ë¡œë“œ
1-1. (Test mode) ì „ì²˜ë¦¬ê³¼ì •ì´ ì˜ ë˜ëŠ”ì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ë¡œë“œëœ ë°ì´í„°ì˜ ì•ì—ì„œ 5000ê°œë§Œ ì˜ë¼ì„œ ì‚¬ìš©
2. ë°ì´í„° ì „ì²˜ë¦¬
3. ë°ì´í„° ë³‘í•©
4. train test split (1:99)
5. causal graph ë¡œë“œí•´ì„œ ì‹¤í—˜ì •ì˜
6. ê° ì‹¤í—˜ë³„ estimation - refutation - prediction ì§„í–‰ í›„ ê²°ê³¼ì €ì¥
"""
import argparse
import pandas as pd
import warnings
from pathlib import Path
import os
import time
import itertools

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# DoWhy ë¡œê±° ë ˆë²¨ ì„¤ì •
import logging as dowhy_logging
dowhy_logging.getLogger("dowhy.causal_estimator").setLevel(dowhy_logging.WARNING)
dowhy_logging.getLogger("dowhy.causal_estimators").setLevel(dowhy_logging.WARNING)

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì„í¬íŠ¸
from .utils import (
    load_all_data,
    preprocess_and_merge_data,
    clean_dataframe_for_causal_model,
    create_causal_graph,
    extract_treatments_from_graph,
    find_all_graph_files,
    setup_logging,
    load_config,
    run_single_experiment,
    run_inference,
    save_predictions_to_excel
)
from . import estimation
from datetime import datetime
import json


def main():
    parser = argparse.ArgumentParser(description="LaborLab 2 ì¸ê³¼ì¶”ë¡  ë¶„ì„ íŒŒì´í”„ë¼ì¸")
    
    default_config = os.environ.get(
        "config.json"
    )
    
    
    args = parser.parse_args()
    
    script_dir = Path(__file__).parent.parent
    
    if os.path.isabs(args.config):
        config_path = Path(args.config)
    else:
        config_path = script_dir / args.config
    
    if not config_path.exists():
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        print(f"   í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
        print(f"   ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {script_dir}")
        return
    
    print(f"ğŸ“„ ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
    config = load_config(config_path)
    
    # ì„¤ì • ê°’ ì¶”ì¶œ
    data_dir = config.get("data_dir", "data")
    seis_data_dir = config.get("seis_data_dir", "seis_data")
    graph_data_dir = config.get("graph_data_dir", "graph_data")
    output_dir = config.get("output_dir", "log")
    graphs = config.get("graphs", [])
    treatments = config.get("treatments", [])
    outcomes = config.get("outcomes", ["ACQ_180_YN"])
    estimators = config.get("estimators", ["tabpfn"])
    auto_extract_treatments = config.get("auto_extract_treatments", False)
    api_key = config.get("api_key", None) or os.environ.get("LLM_API_KEY", None)
    limit_data = config.get("limit_data", False)  # 5000ê°œ ì œí•œ ì˜µì…˜
    limit_size = config.get("limit_size", 5000)  # ì œí•œí•  ë°ì´í„° í¬ê¸°
    mode = config.get("mode", "learning")  # "learning" ë˜ëŠ” "inference"
    checkpoint_dir = config.get("checkpoint_dir", "data/checkpoint")  # checkpoint ë””ë ‰í† ë¦¬
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    data_dir_path = script_dir / data_dir
    
    # ========================================================================
    # 1. ê²½ë¡œë¥¼ í†µí•´ ë°ì´í„° ë¡œë“œ
    # ========================================================================
    print("="*80)
    print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ì‹œì‘")
    print("="*80)
    
    file_list, causal_graph = load_all_data(
        str(data_dir_path), 
        seis_data_dir, 
        graph_file=None
    )
    
    # ========================================================================
    # 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° 3. ë°ì´í„° ë³‘í•©
    # ========================================================================
    print("\n" + "="*80)
    print("2ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ ë° 3ï¸âƒ£ ë°ì´í„° ë³‘í•© ì‹œì‘")
    print("="*80)
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì•ˆë‚´
    if limit_data:
        print(f"\n(Test mode): ì „ì²˜ë¦¬ ì „ì— ê° íŒŒì¼ì—ì„œ ì• {limit_size}ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    print("âš¡ JSON íŒŒì¼ 4ê°œ(ì´ë ¥ì„œ, ìê¸°ì†Œê°œì„œ, ì§ì—…í›ˆë ¨, ìê²©ì¦) ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
    preprocessing_start = time.time()
    
    merged_df = preprocess_and_merge_data(file_list, str(data_dir_path), api_key=api_key, limit_data=limit_data, limit_size=limit_size)
    print(f"âœ… ìµœì¢… ë³‘í•© ë°ì´í„°: {len(merged_df)}ê±´, {len(merged_df.columns)}ê°œ ë³€ìˆ˜")
    
    preprocessing_elapsed = time.time() - preprocessing_start
    print(f"â±ï¸ ì „ì²˜ë¦¬ ë° ë³‘í•© ì™„ë£Œ! ì†Œìš” ì‹œê°„: {preprocessing_elapsed:.2f}ì´ˆ")
    
    # ========================================================================
    # ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
    # ========================================================================
    graph_files = []
    
    if auto_extract_treatments:
        print(f"\nğŸ” ê·¸ë˜í”„ íŒŒì¼ì—ì„œ ìë™ìœ¼ë¡œ treatment ì¶”ì¶œ ì¤‘...")
        found_graphs = find_all_graph_files(data_dir_path, graph_data_dir)
        graph_files = [str(g) for g in found_graphs]
        
        if not graph_files:
            print(f"âš ï¸ {graph_data_dir} í´ë”ì—ì„œ ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"âœ… {len(graph_files)}ê°œì˜ ê·¸ë˜í”„ íŒŒì¼ ë°œê²¬")
    else:
        for graph in graphs:
            if isinstance(graph, str):
                graph_path = data_dir_path / graph_data_dir / graph
                if graph_path.exists():
                    graph_files.append(str(graph_path))
                else:
                    graph_path = Path(graph)
                    if graph_path.exists():
                        graph_files.append(str(graph_path))
    
    if not graph_files:
        print("âŒ ìœ íš¨í•œ ê·¸ë˜í”„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ========================================================================
    # 5. causal graph ë¡œë“œí•´ì„œ ì‹¤í—˜ì •ì˜
    # ========================================================================
    print("\n" + "="*80)
    print("5ï¸âƒ£ Causal Graph ë¡œë“œ ë° ì‹¤í—˜ ì •ì˜")
    print("="*80)
    
    # ëª¨ë“  ê·¸ë˜í”„ì˜ ë³€ìˆ˜ ìˆ˜ì§‘
    all_graph_variables = set()
    for graph_file in graph_files:
        graph_path = Path(graph_file)
        try:
            causal_graph = create_causal_graph(str(graph_path))
            all_graph_variables.update(causal_graph.nodes())
        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({graph_path.name}): {e}")
    
    # treatment ìë™ ì¶”ì¶œ
    graph_treatments_map = {}
    graph_outcomes_map = {}
    
    if auto_extract_treatments:
        print(f"\nğŸ” ê° ê·¸ë˜í”„ íŒŒì¼ì—ì„œ treatment ì •ë³´ ì¶”ì¶œ ì¤‘...")
        
        for graph_file in graph_files:
            graph_path = Path(graph_file)
            extracted_treatments = extract_treatments_from_graph(graph_path)
            
            if extracted_treatments:
                graph_treatments_map[graph_file] = [t["treatment_var"] for t in extracted_treatments if t.get("treatment_var")]
                if extracted_treatments[0].get("outcome"):
                    graph_outcomes_map[graph_file] = extracted_treatments[0]["outcome"]
                print(f"   âœ… {graph_path.name}: {len(graph_treatments_map[graph_file])}ê°œì˜ treatment ë°œê²¬")
    
    # ì‹¤í—˜ ì¡°í•© ìƒì„± (learningê³¼ inference ëª¨ë‘ì—ì„œ ì‚¬ìš©)
    sorted_estimators = []
    if "linear_regression" in estimators:
        sorted_estimators.append("linear_regression")
    if "tabpfn" in estimators:
        sorted_estimators.append("tabpfn")
    for est in estimators:
        if est not in sorted_estimators:
            sorted_estimators.append(est)
    
    if auto_extract_treatments and graph_treatments_map:
        experiment_combinations = []
        for graph_file in graph_files:
            graph_treatments = graph_treatments_map.get(graph_file, treatments)
            graph_outcome = graph_outcomes_map.get(graph_file, outcomes[0] if outcomes else "ACQ_180_YN")
            
            for treatment in graph_treatments:
                for estimator in sorted_estimators:
                    experiment_combinations.append((graph_file, treatment, graph_outcome, estimator))
    else:
        experiment_combinations = list(itertools.product(
            graph_files,
            treatments,
            outcomes,
            sorted_estimators
        ))
    
    total_experiments = len(experiment_combinations)
    print(f"\nğŸ“Š ì´ {total_experiments}ê°œì˜ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n")
    
    # ë°ì´í„° ì •ë¦¬
    all_treatments = set()
    all_outcomes = set()
    for graph_file in graph_files:
        if graph_file in graph_treatments_map:
            all_treatments.update(graph_treatments_map[graph_file])
        if graph_file in graph_outcomes_map:
            all_outcomes.add(graph_outcomes_map[graph_file])
    if not auto_extract_treatments:
        all_treatments.update(treatments)
    if not graph_outcomes_map:
        all_outcomes.update(outcomes)
    
    essential_vars = all_treatments | all_outcomes | {"SEEK_CUST_NO", "JHNT_CTN", "JHNT_MBN"}
    # HOPE_JSCD3_NAMEì€ ê·¸ë˜í”„ì— í¬í•¨ë˜ì§€ ì•Šì§€ë§Œ ë°ì´í„°ì—ëŠ” ìœ ì§€í•´ì•¼ í•¨ (ì§ì¢…ì†Œë¶„ë¥˜ë³„ ë¶„ë¦¬ìš©)
    stratification_vars = {"HOPE_JSCD3_NAME"}
    required_vars = list(all_graph_variables | essential_vars | stratification_vars)
    
    merged_df_clean = clean_dataframe_for_causal_model(
        merged_df, 
        required_vars=required_vars, 
        logger=None
    )
    
    data_variables = set(merged_df_clean.columns)
    vars_to_keep = (all_graph_variables | essential_vars | stratification_vars) & data_variables
    vars_to_remove = data_variables - vars_to_keep
    
    if vars_to_remove:
        print(f"ğŸ—‘ï¸ ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ ì œê±° ì¤‘ ({len(vars_to_remove)}ê°œ)...")
        merged_df_clean = merged_df_clean[list(vars_to_keep)]
    
    print(f"âœ… ì •ë¦¬ëœ ë°ì´í„°: {len(merged_df_clean)}ê±´, {len(merged_df_clean.columns)}ê°œ ë³€ìˆ˜")
    print("="*80 + "\n")
    
    # ========================================================================
    # ëª¨ë“œì— ë”°ë¥¸ ë¶„ê¸°: learning ë˜ëŠ” inference
    # ========================================================================
    if mode == "inference":
        # ========================================================================
        # Inference ëª¨ë“œ: checkpoint ë¡œë“œ í›„ ë°”ë¡œ prediction
        # ========================================================================
        print("="*80)
        print("ğŸ”® Inference ëª¨ë“œ ì‹¤í–‰")
        print("="*80)
        
        # checkpoint ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
        checkpoint_dir_path = script_dir / checkpoint_dir
        checkpoint_dir_path.mkdir(parents=True, exist_ok=True)
        
        # ë¡œê±° ì„¤ì •
        logger = None
        if not config.get("no_logs", False):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir_path = script_dir / output_dir
            output_dir_path.mkdir(exist_ok=True)
            logger = setup_logging(
                log_dir=output_dir_path,
                log_filename=f"inference_{timestamp}.log"
            )
            if logger:
                logger.info(f"Inference ëª¨ë“œ ì‹œì‘ - {timestamp}")
        
        # ì‹¤í—˜ ì¡°í•©ì€ ì´ë¯¸ ìœ„ì—ì„œ ìƒì„±ë¨
        total_experiments = len(experiment_combinations)
        print(f"\nğŸ“Š ì´ {total_experiments}ê°œì˜ inference ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n")
        
        # Inference ì‹¤í–‰
        results = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir_path = script_dir / output_dir
        output_dir_path.mkdir(exist_ok=True)
        
        for idx, (graph_file, treatment, outcome, estimator) in enumerate(experiment_combinations, 1):
            experiment_id = f"exp_{idx:04d}_{Path(graph_file).stem}_{treatment}_{outcome}_{estimator}"
            
            print(f"\n[{idx}/{total_experiments}] Inference ì‹¤í–‰ ì¤‘...")
            
            try:
                result = run_inference(
                    merged_df_clean=merged_df_clean,
                    graph_file=graph_file,
                    checkpoint_dir=checkpoint_dir_path,
                    treatment=treatment,
                    outcome=outcome,
                    estimator=estimator,
                    logger=logger,
                    experiment_id=experiment_id
                )
                result["experiment_id"] = experiment_id
                result["graph_name"] = Path(graph_file).stem
                result["treatment"] = treatment
                result["outcome"] = outcome
                result["estimator"] = estimator
                results.append(result)
                
            except Exception as e:
                print(f"âŒ Inference ì‹¤íŒ¨: {experiment_id}")
                print(f"   ì—ëŸ¬: {e}")
                results.append({
                    "status": "failed",
                    "experiment_id": experiment_id,
                    "error": str(e)
                })
        
        # ê²°ê³¼ ì €ì¥
        results_file = output_dir_path / f"inference_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        success_count = sum(1 for r in results if r.get("status") == "success")
        failed_count = sum(1 for r in results if r.get("status") == "failed")
        
        print(f"\n{'='*80}")
        print("ğŸ“‹ Inference ì™„ë£Œ")
        print(f"{'='*80}")
        print(f"ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}")
        print(f"ì„±ê³µ: {success_count}")
        print(f"ì‹¤íŒ¨: {failed_count}")
        print(f"ê²°ê³¼ íŒŒì¼: {results_file}")
        print(f"{'='*80}\n")
        
        return
    
    # ========================================================================
    # Learning ëª¨ë“œ: ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ (estimation â†’ refutation â†’ prediction)
    # ========================================================================
    print("="*80)
    print("ğŸ“ Learning ëª¨ë“œ ì‹¤í–‰")
    print("="*80)
    
    # ========================================================================
    # 4. train test split (1:99) - ì‹¤ì œë¡œëŠ” run_analysis_without_preprocessingì—ì„œ ìˆ˜í–‰
    # ========================================================================
    # ì£¼ì˜: train/test splitì€ ê° ì‹¤í—˜ ì‹¤í–‰ ì‹œ run_analysis_without_preprocessing ë‚´ë¶€ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ì¤€ë¹„ë§Œ í•©ë‹ˆë‹¤.
    
    # ========================================================================
    # 6. ê° ì‹¤í—˜ë³„ estimation - refutation - prediction ì§„í–‰ í›„ ê²°ê³¼ì €ì¥
    # ========================================================================
    print("="*80)
    print("6ï¸âƒ£ ê° ì‹¤í—˜ë³„ estimation - refutation - prediction ì§„í–‰")
    print("="*80)
    
    # ê²°ê³¼ ì €ì¥
    results = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir_path = script_dir / output_dir
    output_dir_path.mkdir(exist_ok=True)
    
    results_file = output_dir_path / f"batch_experiments_{timestamp}.json"
    csv_results_file = output_dir_path / f"experiment_results_{timestamp}.csv"
    
    csv_columns = [
        'graph_name', 'treatment', 'estimator', 'ate_value',
        'placebo_passed', 'placebo_pvalue',
        'unobserved_passed', 'unobserved_pvalue',
        'subset_passed', 'subset_pvalue',
        'dummy_passed', 'dummy_pvalue',
        'f1_score', 'auc', 'duration_seconds'
    ]
    
    pd.DataFrame(columns=csv_columns).to_csv(csv_results_file, index=False, encoding='utf-8-sig')
    
    # ë¡œê±° ì„¤ì •
    logger = None
    if not config.get("no_logs", False):
        logger = setup_logging(
            log_dir=output_dir_path,
            log_filename=f"batch_experiments_{timestamp}.log"
        )
        if logger:
            logger.info(f"ë°°ì¹˜ ì‹¤í—˜ ì‹œì‘ - {timestamp}")
            logger.info(f"ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}")
    
    # ì‹¤í—˜ ì‹¤í–‰
    for idx, (graph_file, treatment, outcome, estimator) in enumerate(experiment_combinations, 1):
        experiment_id = f"exp_{idx:04d}_{Path(graph_file).stem}_{treatment}_{outcome}_{estimator}"
        
        print(f"\n[{idx}/{total_experiments}] ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
        
        result = run_single_experiment(
            merged_df_clean=merged_df_clean,
            graph_file=graph_file,
            treatment=treatment,
            outcome=outcome,
            estimator=estimator,
            experiment_id=experiment_id,
            logger=logger
        )
        
        results.append(result)
        
        if result["status"] == "failed":
            print(f"âŒ ì‹¤íŒ¨: {experiment_id}")
            if result.get("error"):
                print(f"   ì—ëŸ¬: {result['error']}")
        
        # CSVì— ê²°ê³¼ ì¶”ê°€
        if result["status"] == "success":
            csv_row = {
                'graph_name': result.get('graph_name', ''),
                'treatment': result.get('treatment', ''),
                'estimator': result.get('estimator', ''),
                'ate_value': result.get('ate_value'),
                'placebo_passed': result.get('placebo_passed'),
                'placebo_pvalue': result.get('placebo_pvalue'),
                'unobserved_passed': result.get('unobserved_passed'),
                'unobserved_pvalue': result.get('unobserved_pvalue'),
                'subset_passed': result.get('subset_passed'),
                'subset_pvalue': result.get('subset_pvalue'),
                'dummy_passed': result.get('dummy_passed'),
                'dummy_pvalue': result.get('dummy_pvalue'),
                'f1_score': result.get('f1_score'),
                'auc': result.get('auc'),
                'duration_seconds': result.get('duration_seconds')
            }
            
            try:
                existing_df = pd.read_csv(csv_results_file, encoding='utf-8-sig')
            except (FileNotFoundError, pd.errors.EmptyDataError):
                existing_df = pd.DataFrame(columns=csv_columns)
            
            new_row_df = pd.DataFrame([csv_row])
            updated_df = pd.concat([existing_df, new_row_df], ignore_index=True)
            updated_df.to_csv(csv_results_file, index=False, encoding='utf-8-sig')
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (JSON)
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        success_count = sum(1 for r in results if r["status"] == "success")
        failed_count = sum(1 for r in results if r["status"] == "failed")
        print(f"\nâœ… ì„±ê³µ: {success_count}, âŒ ì‹¤íŒ¨: {failed_count}")
    
    # ìµœì¢… ìš”ì•½
    print(f"\n{'='*80}")
    print("ğŸ“‹ ë°°ì¹˜ ì‹¤í—˜ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}")
    print(f"ì„±ê³µ: {success_count}")
    print(f"ì‹¤íŒ¨: {failed_count}")
    print(f"JSON ê²°ê³¼ íŒŒì¼: {results_file}")
    print(f"CSV ê²°ê³¼ íŒŒì¼: {csv_results_file}")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
