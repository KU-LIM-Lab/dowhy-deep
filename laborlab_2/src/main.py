"""
LaborLab 2 - ì¸ê³¼ì¶”ë¡  ë¶„ì„ ë©”ì¸ íŒŒì´í”„ë¼ì¸

main.pyì™€ run_batch_experiments.pyë¥¼ ë³‘í•©í•œ í†µí•© íŒŒì´í”„ë¼ì¸
"""
import argparse
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import warnings
from pathlib import Path
import logging
from datetime import datetime
import os
import sys
import json
import re
import time
import itertools
from sklearn.model_selection import train_test_split

# DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import dowhy
from dowhy import CausalModel
import networkx as nx

# ë¡œì»¬ DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

# ëª¨ë“ˆ ì„í¬íŠ¸
from . import preprocess
from . import estimation
from .utils import (
    extract_treatments_from_graph,
    find_all_graph_files,
    setup_logging
)

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# DoWhy ë¡œê±° ë ˆë²¨ ì„¤ì •
import logging as dowhy_logging
dowhy_logging.getLogger("dowhy.causal_estimator").setLevel(dowhy_logging.WARNING)
dowhy_logging.getLogger("dowhy.causal_estimators").setLevel(dowhy_logging.WARNING)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False


def create_causal_graph(graph_file):
    """
    DOT í˜•ì‹ ê·¸ë˜í”„ íŒŒì¼ì„ ì½ì–´ì„œ NetworkX ì¸ê³¼ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (DOT í˜•ì‹)
    
    Returns:
        nx.DiGraph: ì¸ê³¼ ê·¸ë˜í”„ ê°ì²´
    """
    return _parse_dot_graph(graph_file)


def _parse_dot_graph(graph_file):
    """DOT í˜•ì‹ ê·¸ë˜í”„ íŒŒì¼ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    try:
        # pydotì„ ì‚¬ìš©í•˜ì—¬ DOT íŒŒì¼ ì½ê¸°
        import pydot
        graphs = pydot.graph_from_dot_file(graph_file)
        if not graphs:
            raise ValueError(f"DOT íŒŒì¼ì—ì„œ ê·¸ë˜í”„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_file}")
        
        # ì²« ë²ˆì§¸ ê·¸ë˜í”„ ì‚¬ìš©
        dot_graph = graphs[0]
        
        # NetworkX ê·¸ë˜í”„ë¡œ ë³€í™˜
        G = nx.drawing.nx_pydot.from_pydot(dot_graph)
        
        # ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ë³€í™˜ (digraphì¸ ê²½ìš°)
        if not G.is_directed():
            with open(graph_file, 'r', encoding='utf-8') as f:
                content = f.read()
            if content.strip().startswith('digraph'):
                G = G.to_directed()
        
        return G
    except ImportError:
        # pydotì´ ì—†ìœ¼ë©´ ìˆ˜ë™ íŒŒì‹±
        return _parse_dot_manual(graph_file)
    except Exception as e:
        # pydot íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ íŒŒì‹± ì‹œë„
        try:
            return _parse_dot_manual(graph_file)
        except Exception as e2:
            raise ValueError(f"DOT íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}. ìˆ˜ë™ íŒŒì‹±ë„ ì‹¤íŒ¨: {e2}")


def _parse_dot_manual(graph_file):
    """DOT í˜•ì‹ì„ ìˆ˜ë™ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤ (pydot ì—†ì´)."""
    with open(graph_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    G = nx.DiGraph()
    
    # digraphì¸ì§€ í™•ì¸
    is_digraph = content.strip().startswith('digraph')
    
    # subgraph cluster_treatments ë¸”ë¡ ì œê±°
    content_without_subgraph = re.sub(
        r'subgraph\s+cluster_treatments\s*\{[^}]*\}',
        '',
        content,
        flags=re.DOTALL
    )
    
    # ë…¸ë“œ ì •ì˜ ì°¾ê¸°
    node_pattern = r'([A-Za-z_][A-Za-z0-9_]*)\s*\[[^\]]*label\s*=\s*"([^"]+)"'
    for match in re.finditer(node_pattern, content_without_subgraph):
        node_id = match.group(1)
        label = match.group(2)
        if not re.match(r'^T\d+$', node_id):
            G.add_node(node_id, label=label)
    
    # ì—£ì§€ ì°¾ê¸°
    edge_pattern = r'([A-Za-z_][A-Za-z0-9_]*)\s*->\s*([A-Za-z_][A-Za-z0-9_]*)'
    for match in re.finditer(edge_pattern, content_without_subgraph):
        source = match.group(1)
        target = match.group(2)
        if not re.match(r'^T\d+$', source) and not re.match(r'^T\d+$', target):
            G.add_edge(source, target)
    
    # ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ë³€í™˜
    if is_digraph and not G.is_directed():
        G = G.to_directed()
    
    return G


def load_all_data(data_dir, seis_data_dir, graph_file=None):
    """
    ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°(JSON)ë¥¼ ëª¨ë‘ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        seis_data_dir (str): seis_data ë””ë ‰í† ë¦¬ ì´ë¦„
        graph_file (str, optional): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        tuple: (íŒŒì¼ê²½ë¡œ_ë¦¬ìŠ¤íŠ¸, ì¸ê³¼ê·¸ë˜í”„)
    """
    data_path = Path(data_dir)
    
    # 1. ì •í˜• ë°ì´í„° íŒŒì¼ ê²½ë¡œ í™•ì¸ (seis_data í´ë”ì—ì„œ)
    structured_data_path = data_path / seis_data_dir / "seis_data.csv"
    
    if not structured_data_path.exists():
        raise FileNotFoundError(f"ì •í˜• ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {structured_data_path}")
    
    print(f"âœ… ì •í˜• ë°ì´í„° íŒŒì¼ ê²½ë¡œ: {structured_data_path}")
    
    # 2. ë¹„ì •í˜• ë°ì´í„°(JSON) íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    seis_data_path = data_path / seis_data_dir
    file_list = []
    
    json_files = [
        ("resume.json", "ì´ë ¥ì„œ"),
        ("coverletters.json", "ìê¸°ì†Œê°œì„œ"),
        ("trainings.json", "ì§ì—…í›ˆë ¨"),
        ("licenses.json", "ìê²©ì¦")
    ]
    
    # ì •í˜• ë°ì´í„° íŒŒì¼ì„ ë¨¼ì € ì¶”ê°€
    file_list.append(str(structured_data_path))
    
    # JSON íŒŒì¼ ê²½ë¡œ ì¶”ê°€
    for filename, json_type in json_files:
        json_path = seis_data_path / filename
        if json_path.exists():
            file_list.append(str(json_path))
            print(f"âœ… {json_type} íŒŒì¼ ê²½ë¡œ ì¶”ê°€: {json_path}")
        else:
            print(f"âš ï¸ {json_type} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    # 3. ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ (graph_fileì´ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ì²« ë²ˆì§¸ ê·¸ë˜í”„ ì‚¬ìš©)
    if graph_file is None:
        graph_data_path = data_path / "graph_data"
        if graph_data_path.exists():
            graph_files = list(graph_data_path.glob("graph_*.dot"))
            if graph_files:
                graph_file = sorted(graph_files)[0]
                print(f"âš ï¸ ê·¸ë˜í”„ íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•„ {graph_file.name}ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_data_path}")
        else:
            raise FileNotFoundError(f"ê·¸ë˜í”„ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_data_path}")
    else:
        graph_file = Path(graph_file)
    
    if not graph_file.exists():
        raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_file}")
    
    causal_graph = create_causal_graph(str(graph_file))
    print(f"âœ… ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {causal_graph.number_of_nodes()}ê°œ ë…¸ë“œ, {causal_graph.number_of_edges()}ê°œ ì—£ì§€")
    
    return file_list, causal_graph


def clean_dataframe_for_causal_model(df, required_vars=None, logger=None):
    """
    CausalModel ìƒì„± ì „ì— ë°ì´í„°í”„ë ˆì„ì„ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    df_clean = df.copy()
    cols_to_drop = []
    
    if required_vars is None:
        required_vars = []
    
    for col in df_clean.columns:
        if df_clean[col].dtype == 'object':
            if len(df_clean) > 0:
                non_null_values = df_clean[col].dropna()
                if len(non_null_values) > 0:
                    first_val = non_null_values.iloc[0]
                    is_logger_object = isinstance(first_val, logging.Logger) or 'Logger' in str(type(first_val))
                    is_invalid_type = not isinstance(first_val, (str, int, float, bool, type(None)))
                    
                    if is_logger_object or is_invalid_type:
                        if col in required_vars:
                            if logger:
                                logger.warning(f"í•„ìˆ˜ ë³€ìˆ˜ '{col}'ì˜ ê°’ì´ ê°ì²´ íƒ€ì…({type(first_val).__name__})ì´ì–´ì„œ NaNìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                            else:
                                print(f"âš ï¸ í•„ìˆ˜ ë³€ìˆ˜ '{col}'ì˜ ê°’ì´ ê°ì²´ íƒ€ì…({type(first_val).__name__})ì´ì–´ì„œ NaNìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                            df_clean[col] = np.nan
                        else:
                            cols_to_drop.append(col)
    
    if cols_to_drop:
        df_clean = df_clean.drop(columns=cols_to_drop)
    
    return df_clean


def preprocess_and_merge_data(file_list, data_dir, api_key=None):
    """
    Preprocessor í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    """
    preprocessor = preprocess.Preprocessor([], api_key=api_key)
    absolute_file_list = [str(Path(f).resolve()) for f in file_list]
    merged_df = preprocessor.get_merged_df(absolute_file_list)
    print(f"âœ… ëª¨ë“  ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ì™„ë£Œ")
    return merged_df


def save_predictions_to_excel(df_with_predictions, output_dir=None, filename=None, logger=None):
    """ì˜ˆì¸¡ê°’ì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ì„ Excel íŒŒì¼ë¡œ ì €ì¥"""
    if output_dir is None:
        script_dir = Path(__file__).parent.parent
        output_dir = script_dir / "log"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"predictions_{timestamp}.xlsx"
    
    filepath = output_dir / filename
    
    df_with_predictions.to_excel(filepath, index=False, engine='openpyxl')
    
    if logger:
        logger.info(f"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
    
    return str(filepath)


def run_analysis_without_preprocessing(
    merged_df_clean: pd.DataFrame,
    graph_file: str,
    treatment: str,
    outcome: str,
    estimator: str,
    logger=None,
    experiment_id: str = None
):
    """
    ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ê³¼ì¶”ë¡  ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        step_times = {}
        step_start = time.time()
        
        if experiment_id:
            print(f"\n{'='*80}")
            print(f"ì‹¤í—˜ ID: {experiment_id}")
            print(f"ê·¸ë˜í”„: {Path(graph_file).name}")
            print(f"Treatment: {treatment}, Outcome: {outcome}")
            print(f"Estimator: {estimator}")
            print(f"{'='*80}\n")
        
        # 1. ê·¸ë˜í”„ ë¡œë“œ
        print("1ï¸âƒ£ ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ ì¤‘...")
        step_start = time.time()
        causal_graph = create_causal_graph(graph_file)
        step_times['ê·¸ë˜í”„ ë¡œë“œ'] = time.time() - step_start
        
        # 2. ë°ì´í„° í•„í„°ë§
        print("2ï¸âƒ£ ê·¸ë˜í”„ ë³€ìˆ˜ì— ë§ê²Œ ë°ì´í„° í•„í„°ë§ ì¤‘...")
        step_start = time.time()
        
        graph_variables = set(causal_graph.nodes())
        data_variables = set(merged_df_clean.columns)
        essential_vars = {treatment, outcome, "SEEK_CUST_NO", "JHNT_CTN", "JHNT_MBN"}
        vars_to_keep = (graph_variables | essential_vars) & data_variables
        df_for_analysis = merged_df_clean[list(vars_to_keep)].copy()
        
        missing_vars = [var for var in [treatment, outcome] if var not in df_for_analysis.columns]
        if missing_vars:
            raise ValueError(f"í•„ìˆ˜ ë³€ìˆ˜ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤: {missing_vars}")
        
        step_times['ë°ì´í„° í•„í„°ë§'] = time.time() - step_start
        
        # 3. Train/Test Split
        print("3ï¸âƒ£ Train/Test Split ì¤‘ (80/20)...")
        step_start = time.time()
        
        outcome_data = df_for_analysis[outcome]
        is_binary = outcome_data.nunique() <= 2 and outcome_data.dtype in ['int64', 'int32', 'bool']
        
        if is_binary:
            df_train, df_test = train_test_split(
                df_for_analysis,
                test_size=0.2,
                random_state=42,
                stratify=outcome_data
            )
        else:
            df_train, df_test = train_test_split(
                df_for_analysis,
                test_size=0.2,
                random_state=42
            )
        
        step_times['Train/Test Split'] = time.time() - step_start
        
        # 4. ì¸ê³¼ëª¨ë¸ ìƒì„±
        print("4ï¸âƒ£ ì¸ê³¼ëª¨ë¸ ìƒì„± ì¤‘...")
        step_start = time.time()
        model = CausalModel(
            data=df_train,
            treatment=treatment,
            outcome=outcome,
            graph=causal_graph
        )
        step_times['ì¸ê³¼ëª¨ë¸ ìƒì„±'] = time.time() - step_start
        
        # 5. ì¸ê³¼íš¨ê³¼ ì‹ë³„
        print("5ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì‹ë³„ ì¤‘...")
        step_start = time.time()
        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
        step_times['ì¸ê³¼íš¨ê³¼ ì‹ë³„'] = time.time() - step_start
        
        # 6. ì¸ê³¼íš¨ê³¼ ì¶”ì •
        print("6ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì¶”ì • ì¤‘...")
        step_start = time.time()
        estimate = estimation.estimate_causal_effect(
            model,
            identified_estimand,
            estimator,
            logger
        )
        step_times['ì¸ê³¼íš¨ê³¼ ì¶”ì •'] = time.time() - step_start
        
        # 7. ì˜ˆì¸¡
        print("7ï¸âƒ£ ì˜ˆì¸¡ ì¤‘...")
        step_start = time.time()
        essential_vars_for_pred = {treatment, outcome}
        df_test_clean = clean_dataframe_for_causal_model(
            df_test,
            required_vars=list(essential_vars_for_pred),
            logger=logger
        )
        metrics, df_with_predictions = estimation.predict_conditional_expectation(
            estimate, df_test_clean, logger=logger
        )
        step_times['ì˜ˆì¸¡'] = time.time() - step_start
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        if experiment_id:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"predictions_{experiment_id}_{timestamp}.xlsx"
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"predictions_{timestamp}.xlsx"
        
        step_start = time.time()
        excel_path = save_predictions_to_excel(df_with_predictions, filename=filename, logger=logger)
        step_times['ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥'] = time.time() - step_start
        
        # 8. ê²€ì¦ í…ŒìŠ¤íŠ¸
        print("8ï¸âƒ£ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        step_start = time.time()
        validation_results = estimation.run_validation_tests(
            model,
            identified_estimand,
            estimate,
            logger
        )
        step_times['ê²€ì¦ í…ŒìŠ¤íŠ¸'] = time.time() - step_start
        
        # 9. ë¯¼ê°ë„ ë¶„ì„
        print("9ï¸âƒ£ ë¯¼ê°ë„ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
        step_start = time.time()
        sensitivity_df = estimation.run_sensitivity_analysis(
            model,
            identified_estimand,
            estimate,
            logger
        )
        step_times['ë¯¼ê°ë„ ë¶„ì„'] = time.time() - step_start
        
        # 10. ì‹œê°í™”
        print("ğŸ”Ÿ ì‹œê°í™” ìƒì„± ì¤‘...")
        step_start = time.time()
        heatmap_path = estimation.create_sensitivity_heatmap(
            sensitivity_df,
            logger
        ) if not sensitivity_df.empty else None
        step_times['ì‹œê°í™” ìƒì„±'] = time.time() - step_start
        
        # 11. ìš”ì•½ ë³´ê³ ì„œ
        print("1ï¸âƒ£1ï¸âƒ£ ìµœì¢… ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥ ì¤‘...")
        step_start = time.time()
        estimation.print_summary_report(estimate, validation_results, sensitivity_df)
        step_times['ìš”ì•½ ë³´ê³ ì„œ'] = time.time() - step_start
        
        total_time = sum(step_times.values())
        step_times['ì „ì²´'] = total_time
        
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! (ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ)")
        
        return {
            "status": "success",
            "estimate": estimate,
            "validation_results": validation_results,
            "sensitivity_df": sensitivity_df,
            "metrics": metrics,
            "excel_path": excel_path,
            "step_times": step_times,
            "train_size": len(df_train),
            "test_size": len(df_test)
        }
        
    except Exception as e:
        if logger:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


def run_single_experiment(
    merged_df_clean,
    graph_file: str,
    treatment: str,
    outcome: str,
    estimator: str,
    experiment_id: str,
    logger=None
) -> dict:
    """ë‹¨ì¼ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤"""
    start_time = datetime.now()
    try:
        result = run_analysis_without_preprocessing(
            merged_df_clean=merged_df_clean,
            graph_file=graph_file,
            treatment=treatment,
            outcome=outcome,
            estimator=estimator,
            logger=logger,
            experiment_id=experiment_id
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        metrics = result.get("metrics", {})
        estimate = result.get("estimate")
        validation_results = result.get("validation_results", {})
        
        # ATE ê°’ ì¶”ì¶œ
        ate_value = None
        if estimate and hasattr(estimate, 'value'):
            ate_value = estimate.value
        
        # Refutation ê²°ê³¼ ì¶”ì¶œ
        refutation_data = {}
        refutation_types = ['placebo', 'unobserved', 'subset', 'dummy']
        for ref_type in refutation_types:
            ref_result = validation_results.get(ref_type)
            if ref_result is not None:
                if ref_type == 'placebo':
                    effect_change = abs(ref_result.new_effect - ref_result.estimated_effect)
                    refutation_data[f'{ref_type}_passed'] = effect_change < 0.01
                elif ref_type == 'unobserved':
                    change_rate = abs(ref_result.new_effect - ref_result.estimated_effect) / abs(ref_result.estimated_effect) if abs(ref_result.estimated_effect) > 0 else float('inf')
                    refutation_data[f'{ref_type}_passed'] = change_rate < 0.2
                elif ref_type == 'subset':
                    effect_change = abs(ref_result.new_effect - ref_result.estimated_effect)
                    change_rate = abs(ref_result.estimated_effect) > 0 and abs(effect_change / ref_result.estimated_effect) or float('inf')
                    refutation_data[f'{ref_type}_passed'] = change_rate < 0.1
                elif ref_type == 'dummy':
                    refutation_data[f'{ref_type}_passed'] = abs(ref_result.new_effect) < 0.01
                
                p_value = estimation.calculate_refutation_pvalue(ref_result, ref_type)
                refutation_data[f'{ref_type}_pvalue'] = p_value
            else:
                refutation_data[f'{ref_type}_passed'] = None
                refutation_data[f'{ref_type}_pvalue'] = None
        
        return {
            "experiment_id": experiment_id,
            "status": "success",
            "duration_seconds": duration,
            "graph": graph_file,
            "graph_name": Path(graph_file).stem,
            "treatment": treatment,
            "outcome": outcome,
            "estimator": estimator,
            "ate_value": ate_value,
            "metrics": metrics,
            "accuracy": metrics.get("accuracy") if metrics else None,
            "f1_score": metrics.get("f1_score") if metrics else None,
            "auc": metrics.get("auc") if metrics else None,
            "excel_path": result.get("excel_path"),
            "train_size": result.get("train_size"),
            "test_size": result.get("test_size"),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            **refutation_data
        }
    except Exception as e:
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        return {
            "experiment_id": experiment_id,
            "status": "failed",
            "duration_seconds": duration,
            "graph": graph_file,
            "treatment": treatment,
            "outcome": outcome,
            "estimator": estimator,
            "error": str(e),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
        }


def load_config(config_path: Path) -> dict:
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config


def run_batch_experiments(config: dict, base_dir: Path):
    """ë°°ì¹˜ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤"""
    data_dir = config.get("data_dir", "data")
    seis_data_dir = config.get("seis_data_dir", "seis_data")
    graph_data_dir = config.get("graph_data_dir", "graph_data")
    output_dir = config.get("output_dir", "log")
    graphs = config.get("graphs", [])
    treatments = config.get("treatments", [])
    outcomes = config.get("outcomes", ["ACQ_180_YN"])
    estimators = config.get("estimators", ["tabpfn"])
    auto_extract_treatments = config.get("auto_extract_treatments", False)
    api_key = config.get("api_key", None) or os.environ.get("LLM_API_KEY", None)
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    data_dir_path = base_dir / data_dir
    
    # ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
    graph_files = []
    
    if auto_extract_treatments:
        print(f"ğŸ” ê·¸ë˜í”„ íŒŒì¼ì—ì„œ ìë™ìœ¼ë¡œ treatment ì¶”ì¶œ ì¤‘...")
        found_graphs = find_all_graph_files(data_dir_path, graph_data_dir)
        graph_files = [str(g) for g in found_graphs]
        
        if not graph_files:
            print(f"âš ï¸ {graph_data_dir} í´ë”ì—ì„œ ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"âœ… {len(graph_files)}ê°œì˜ ê·¸ë˜í”„ íŒŒì¼ ë°œê²¬")
    else:
        for graph in graphs:
            if isinstance(graph, str):
                graph_path = base_dir / data_dir / graph_data_dir / graph
                if graph_path.exists():
                    graph_files.append(str(graph_path))
                else:
                    graph_path = Path(graph)
                    if graph_path.exists():
                        graph_files.append(str(graph_path))
    
    if not graph_files:
        print("âŒ ìœ íš¨í•œ ê·¸ë˜í”„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # treatment ìë™ ì¶”ì¶œ
    graph_treatments_map = {}
    graph_outcomes_map = {}
    
    if auto_extract_treatments:
        print(f"\nğŸ” ê° ê·¸ë˜í”„ íŒŒì¼ì—ì„œ treatment ì •ë³´ ì¶”ì¶œ ì¤‘...")
        
        for graph_file in graph_files:
            graph_path = Path(graph_file)
            extracted_treatments = extract_treatments_from_graph(graph_path)
            
            if extracted_treatments:
                graph_treatments_map[graph_file] = [t["treatment_var"] for t in extracted_treatments if t.get("treatment_var")]
                if extracted_treatments[0].get("outcome"):
                    graph_outcomes_map[graph_file] = extracted_treatments[0]["outcome"]
                print(f"   âœ… {graph_path.name}: {len(graph_treatments_map[graph_file])}ê°œì˜ treatment ë°œê²¬")
    
    # ì‹¤í—˜ ì¡°í•© ìƒì„±
    sorted_estimators = []
    if "linear_regression" in estimators:
        sorted_estimators.append("linear_regression")
    if "tabpfn" in estimators:
        sorted_estimators.append("tabpfn")
    for est in estimators:
        if est not in sorted_estimators:
            sorted_estimators.append(est)
    
    if auto_extract_treatments and graph_treatments_map:
        experiment_combinations = []
        for graph_file in graph_files:
            graph_treatments = graph_treatments_map.get(graph_file, treatments)
            graph_outcome = graph_outcomes_map.get(graph_file, outcomes[0] if outcomes else "ACQ_180_YN")
            
            for treatment in graph_treatments:
                for estimator in sorted_estimators:
                    experiment_combinations.append((graph_file, treatment, graph_outcome, estimator))
    else:
        experiment_combinations = list(itertools.product(
            graph_files,
            treatments,
            outcomes,
            sorted_estimators
        ))
    
    total_experiments = len(experiment_combinations)
    print(f"\nğŸ“Š ì´ {total_experiments}ê°œì˜ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n")
    
    # ì „ì²˜ë¦¬ ìˆ˜í–‰
    print("="*80)
    print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
    print("="*80)
    
    preprocessing_start = time.time()
    
    file_list, _ = load_all_data(str(data_dir_path), seis_data_dir, graph_file=None)
    
    print("âš¡ JSON íŒŒì¼ 4ê°œ(ì´ë ¥ì„œ, ìê¸°ì†Œê°œì„œ, ì§ì—…í›ˆë ¨, ìê²©ì¦) ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
    merged_df = preprocess_and_merge_data(file_list, str(data_dir_path), api_key=api_key)
    print(f"âœ… ìµœì¢… ë³‘í•© ë°ì´í„°: {len(merged_df)}ê±´, {len(merged_df.columns)}ê°œ ë³€ìˆ˜")
    
    # ëª¨ë“  ê·¸ë˜í”„ì˜ ë³€ìˆ˜ ìˆ˜ì§‘
    all_graph_variables = set()
    for graph_file in graph_files:
        graph_path = Path(graph_file)
        try:
            causal_graph = create_causal_graph(str(graph_path))
            all_graph_variables.update(causal_graph.nodes())
        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({graph_path.name}): {e}")
    
    all_treatments = set()
    all_outcomes = set()
    for graph_file in graph_files:
        if graph_file in graph_treatments_map:
            all_treatments.update(graph_treatments_map[graph_file])
        if graph_file in graph_outcomes_map:
            all_outcomes.add(graph_outcomes_map[graph_file])
    if not auto_extract_treatments:
        all_treatments.update(treatments)
    if not graph_outcomes_map:
        all_outcomes.update(outcomes)
    
    essential_vars = all_treatments | all_outcomes | {"SEEK_CUST_NO", "JHNT_CTN", "JHNT_MBN"}
    required_vars = list(all_graph_variables | essential_vars)
    
    merged_df_clean = clean_dataframe_for_causal_model(
        merged_df, 
        required_vars=required_vars, 
        logger=None
    )
    
    data_variables = set(merged_df_clean.columns)
    vars_to_keep = (all_graph_variables | essential_vars) & data_variables
    vars_to_remove = data_variables - vars_to_keep
    
    if vars_to_remove:
        print(f"ğŸ—‘ï¸ ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ ì œê±° ì¤‘ ({len(vars_to_remove)}ê°œ)...")
        merged_df_clean = merged_df_clean[list(vars_to_keep)]
    
    preprocessing_elapsed = time.time() - preprocessing_start
    print(f"â±ï¸ ì „ì²˜ë¦¬ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {preprocessing_elapsed:.2f}ì´ˆ")
    print(f"âœ… ì •ë¦¬ëœ ë°ì´í„°: {len(merged_df_clean)}ê±´, {len(merged_df_clean.columns)}ê°œ ë³€ìˆ˜")
    print("="*80 + "\n")
    
    # ê²°ê³¼ ì €ì¥
    results = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir_path = base_dir / output_dir
    output_dir_path.mkdir(exist_ok=True)
    
    results_file = output_dir_path / f"batch_experiments_{timestamp}.json"
    csv_results_file = output_dir_path / f"experiment_results_{timestamp}.csv"
    
    csv_columns = [
        'graph_name', 'treatment', 'estimator', 'ate_value',
        'placebo_passed', 'placebo_pvalue',
        'unobserved_passed', 'unobserved_pvalue',
        'subset_passed', 'subset_pvalue',
        'dummy_passed', 'dummy_pvalue',
        'f1_score', 'auc', 'duration_seconds'
    ]
    
    pd.DataFrame(columns=csv_columns).to_csv(csv_results_file, index=False, encoding='utf-8-sig')
    
    # ë¡œê±° ì„¤ì •
    logger = None
    if not config.get("no_logs", False):
        logger = setup_logging(
            log_dir=output_dir_path,
            log_filename=f"batch_experiments_{timestamp}.log"
        )
        if logger:
            logger.info(f"ë°°ì¹˜ ì‹¤í—˜ ì‹œì‘ - {timestamp}")
            logger.info(f"ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}")
    
    # ì‹¤í—˜ ì‹¤í–‰
    for idx, (graph_file, treatment, outcome, estimator) in enumerate(experiment_combinations, 1):
        experiment_id = f"exp_{idx:04d}_{Path(graph_file).stem}_{treatment}_{outcome}_{estimator}"
        
        print(f"\n[{idx}/{total_experiments}] ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
        
        result = run_single_experiment(
            merged_df_clean=merged_df_clean,
            graph_file=graph_file,
            treatment=treatment,
            outcome=outcome,
            estimator=estimator,
            experiment_id=experiment_id,
            logger=logger
        )
        
        results.append(result)
        
        if result["status"] == "failed":
            print(f"âŒ ì‹¤íŒ¨: {experiment_id}")
            if result.get("error"):
                print(f"   ì—ëŸ¬: {result['error']}")
        
        # CSVì— ê²°ê³¼ ì¶”ê°€
        if result["status"] == "success":
            csv_row = {
                'graph_name': result.get('graph_name', ''),
                'treatment': result.get('treatment', ''),
                'estimator': result.get('estimator', ''),
                'ate_value': result.get('ate_value'),
                'placebo_passed': result.get('placebo_passed'),
                'placebo_pvalue': result.get('placebo_pvalue'),
                'unobserved_passed': result.get('unobserved_passed'),
                'unobserved_pvalue': result.get('unobserved_pvalue'),
                'subset_passed': result.get('subset_passed'),
                'subset_pvalue': result.get('subset_pvalue'),
                'dummy_passed': result.get('dummy_passed'),
                'dummy_pvalue': result.get('dummy_pvalue'),
                'f1_score': result.get('f1_score'),
                'auc': result.get('auc'),
                'duration_seconds': result.get('duration_seconds')
            }
            
            try:
                existing_df = pd.read_csv(csv_results_file, encoding='utf-8-sig')
            except (FileNotFoundError, pd.errors.EmptyDataError):
                existing_df = pd.DataFrame(columns=csv_columns)
            
            new_row_df = pd.DataFrame([csv_row])
            updated_df = pd.concat([existing_df, new_row_df], ignore_index=True)
            updated_df.to_csv(csv_results_file, index=False, encoding='utf-8-sig')
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (JSON)
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        success_count = sum(1 for r in results if r["status"] == "success")
        failed_count = sum(1 for r in results if r["status"] == "failed")
        print(f"\nâœ… ì„±ê³µ: {success_count}, âŒ ì‹¤íŒ¨: {failed_count}")
    
    # ìµœì¢… ìš”ì•½
    print(f"\n{'='*80}")
    print("ğŸ“‹ ë°°ì¹˜ ì‹¤í—˜ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}")
    print(f"ì„±ê³µ: {success_count}")
    print(f"ì‹¤íŒ¨: {failed_count}")
    print(f"JSON ê²°ê³¼ íŒŒì¼: {results_file}")
    print(f"CSV ê²°ê³¼ íŒŒì¼: {csv_results_file}")
    print(f"{'='*80}\n")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="LaborLab 2 ì¸ê³¼ì¶”ë¡  ë¶„ì„ íŒŒì´í”„ë¼ì¸")
    
    default_config = os.environ.get(
        "EXPERIMENT_CONFIG",
        "config.json"
    )
    
    parser.add_argument(
        "--config",
        type=str,
        default=default_config,
        help="ì„¤ì • JSON íŒŒì¼ ê²½ë¡œ"
    )
    
    args = parser.parse_args()
    
    script_dir = Path(__file__).parent.parent
    
    if os.path.isabs(args.config):
        config_path = Path(args.config)
    else:
        config_path = script_dir / args.config
    
    if not config_path.exists():
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        print(f"   í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
        print(f"   ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {script_dir}")
        return
    
    print(f"ğŸ“„ ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
    config = load_config(config_path)
    
    # ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰
    run_batch_experiments(config, script_dir)


if __name__ == "__main__":
    main()

