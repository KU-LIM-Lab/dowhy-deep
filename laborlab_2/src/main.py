"""
LaborLab 2 - ì¸ê³¼ì¶”ë¡  ë¶„ì„ ë©”ì¸ íŒŒì´í”„ë¼ì¸

ì „ì²´ íŒŒì´í”„ë¼ì¸:
1. ê²½ë¡œë¥¼ í†µí•´ ë°ì´í„° ë¡œë“œ
1-1. (Test mode) ì „ì²˜ë¦¬ê³¼ì •ì´ ì˜ ë˜ëŠ”ì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ë¡œë“œëœ ë°ì´í„°ì˜ ì•ì—ì„œ 5000ê°œë§Œ ì˜ë¼ì„œ ì‚¬ìš©
2. ë°ì´í„° ì „ì²˜ë¦¬
3. ë°ì´í„° ë³‘í•©
4. train test split (1:99)
5. causal graph ë¡œë“œí•´ì„œ ì‹¤í—˜ì •ì˜
6. ê° ì‹¤í—˜ë³„ estimation - refutation - prediction ì§„í–‰ í›„ ê²°ê³¼ì €ì¥
"""
import argparse
import pandas as pd
import warnings
from pathlib import Path
import os
import time
import itertools
from typing import Dict, Any, List, Tuple, Optional

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# DoWhy ë¡œê±° ë ˆë²¨ ì„¤ì •
import logging as dowhy_logging
dowhy_logging.getLogger("dowhy.causal_estimator").setLevel(dowhy_logging.WARNING)
dowhy_logging.getLogger("dowhy.causal_estimators").setLevel(dowhy_logging.WARNING)

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì„í¬íŠ¸
from .utils import (
    load_all_data,
    preprocess_and_merge_data,
    setup_logging,
    load_config
)
from .estimation import (
    run_single_experiment,
    run_inference,
    create_experiment_list,
    prepare_data_for_causal_model
)
from datetime import datetime
import json


def preprocess(
    data_dir_path: Path,
    seis_data_dir: str,
    limit_data: bool = False,
    limit_size: int = 5000,
    job_category_file: str = "KSIC"
) -> pd.DataFrame:
    """
    ì „ì²˜ë¦¬ í•¨ìˆ˜ (limit_data ì˜µì…˜ìœ¼ë¡œ ë°ì´í„° ì œí•œ ê°€ëŠ¥)
    
    Args:
        data_dir_path: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        seis_data_dir: seis_data ë””ë ‰í† ë¦¬ëª…
        limit_data: ë°ì´í„° ì œí•œ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        limit_size: ì œí•œí•  ë°ì´í„° í¬ê¸° (ê¸°ë³¸ê°’: 5000)
        job_category_file: ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ëª… (KECO, KSCO, KSIC ì¤‘ ì„ íƒ, ê¸°ë³¸ê°’: KSIC)
    
    Returns:
        merged_df: ì „ì²˜ë¦¬ ë° ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„
    """
    print("="*80)
    print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ì‹œì‘")
    print("="*80)
    
    file_list, causal_graph = load_all_data(
        str(data_dir_path), 
        seis_data_dir, 
        graph_file=None
    )
    
    print("\n" + "="*80)
    if limit_data:
        print("2ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ ë° 3ï¸âƒ£ ë°ì´í„° ë³‘í•© ì‹œì‘ (ì œí•œ ëª¨ë“œ)")
        print(f"\n(Test mode): ì „ì²˜ë¦¬ ì „ì— ê° íŒŒì¼ì—ì„œ ì• {limit_size}ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        print("2ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ ë° 3ï¸âƒ£ ë°ì´í„° ë³‘í•© ì‹œì‘")
    print("="*80)
    
    print(f"ğŸ“‹ ì‚¬ìš©í•  ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼: job_subcategories_{job_category_file}.csv")
    print("âš¡ JSON íŒŒì¼ 4ê°œ(ì´ë ¥ì„œ, ìê¸°ì†Œê°œì„œ, ì§ì—…í›ˆë ¨, ìê²©ì¦) ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
    preprocessing_start = time.time()
    
    merged_df = preprocess_and_merge_data(
        file_list, 
        str(data_dir_path), 
        limit_data=limit_data, 
        limit_size=limit_size,
        job_category_file=job_category_file
    )
    print(f"âœ… ìµœì¢… ë³‘í•© ë°ì´í„°: {len(merged_df)}ê±´, {len(merged_df.columns)}ê°œ ë³€ìˆ˜")
    
    preprocessing_elapsed = time.time() - preprocessing_start
    print(f"â±ï¸ ì „ì²˜ë¦¬ ë° ë³‘í•© ì™„ë£Œ! ì†Œìš” ì‹œê°„: {preprocessing_elapsed:.2f}ì´ˆ")
    
    return merged_df


def learning(
    merged_df_clean: pd.DataFrame,
    graph_file: str,
    treatment: str,
    outcome: str,
    estimator: str = "tabpfn",
    experiment_id: Optional[str] = None,
    logger: Optional[Any] = None
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ì‹¤í—˜ì— ëŒ€í•œ learning í•¨ìˆ˜
    ë‹¨ì¼ ê·¸ë˜í”„, íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸, estimatorë¡œ í•œ ë²ˆì˜ estimation(fitting) ë° refutationì„ ì§„í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¡œê¹…
    
    Args:
        merged_df_clean: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        graph_file: ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
        treatment: íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸ ë³€ìˆ˜ëª…
        outcome: ê²°ê³¼ ë³€ìˆ˜ëª…
        estimator: ì¶”ì • ë°©ë²• (ê¸°ë³¸ê°’: tabpfn)
        experiment_id: ì‹¤í—˜ ID (ì„ íƒì )
        logger: ë¡œê±° ê°ì²´ (ì„ íƒì )
    
    Returns:
        ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if experiment_id is None:
        experiment_id = f"{Path(graph_file).stem}_{treatment}_{outcome}_{estimator}"
    
    print(f"\n{'='*80}")
    print(f"ğŸ“ Learning ì‹¤í–‰ - {experiment_id}")
    print(f"ê·¸ë˜í”„: {Path(graph_file).name}")
    print(f"Treatment: {treatment}, Outcome: {outcome}, Estimator: {estimator}")
    print(f"{'='*80}\n")
    
    result = run_single_experiment(
        merged_df_clean=merged_df_clean,
        graph_file=graph_file,
        treatment=treatment,
        outcome=outcome,
        estimator=estimator,
        experiment_id=experiment_id,
        logger=logger
    )
    
    if result["status"] == "success":
        print(f"âœ… Learning ì™„ë£Œ: {experiment_id}")
        print(f"   ATE ê°’: {result.get('ate_value', 'N/A')}")
        print(f"   F1 Score: {result.get('f1_score', 'N/A')}")
        print(f"   AUC: {result.get('auc', 'N/A')}")
    else:
        print(f"âŒ Learning ì‹¤íŒ¨: {experiment_id}")
        if result.get("error"):
            print(f"   ì—ëŸ¬: {result['error']}")
    
    return result


def _run_experiments_batch(
    experiment_list: List[Tuple[str, str, str, str]],
    experiment_func,
    experiment_type: str,
    merged_df_clean: pd.DataFrame,
    logger: Optional[Any] = None,
    output_dir: Optional[Path] = None,
    **kwargs
) -> List[Dict[str, Any]]:
    """
    ì‹¤í—˜ ë°°ì¹˜ ì‹¤í–‰ ê³µí†µ í•¨ìˆ˜
    
    Args:
        experiment_list: ì‹¤í—˜ ì¡°í•© ë¦¬ìŠ¤íŠ¸
        experiment_func: ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜
        experiment_type: ì‹¤í—˜ íƒ€ì… ("learning" ë˜ëŠ” "prediction")
        merged_df_clean: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        logger: ë¡œê±° ê°ì²´
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        **kwargs: experiment_funcì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì
    
    Returns:
        ì‹¤í—˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    emoji_map = {"learning": "ğŸ“", "prediction": "ğŸ”®"}
    emoji = emoji_map.get(experiment_type, "ğŸ”¬")
    
    print("="*80)
    print(f"{emoji} {experiment_type.capitalize()} Experiments ì‹¤í–‰")
    print("="*80)
    
    total_experiments = len(experiment_list)
    print(f"\nğŸ“Š ì´ {total_experiments}ê°œì˜ {experiment_type} ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n")
    
    results = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # CSV ì„¤ì •
    csv_file = None
    csv_columns = None
    if output_dir:
        results_file = output_dir / f"{experiment_type}_results_{timestamp}.json"
        csv_file = output_dir / f"{experiment_type}_results_{timestamp}.csv"
        
        if experiment_type == "learning":
            csv_columns = [
                'graph_name', 'treatment', 'estimator', 'ate_value',
                'placebo_passed', 'placebo_pvalue',
                'unobserved_passed', 'unobserved_pvalue',
                'subset_passed', 'subset_pvalue',
                'dummy_passed', 'dummy_pvalue',
                'f1_score', 'auc', 'duration_seconds'
            ]
        else:  # prediction
            csv_columns = [
                'graph_name', 'treatment', 'estimator', 'f1_score', 'auc', 'accuracy'
            ]
        
        pd.DataFrame(columns=csv_columns).to_csv(csv_file, index=False, encoding='utf-8-sig')
    
    # ì‹¤í—˜ ì‹¤í–‰
    for idx, (graph_file, treatment, outcome, estimator) in enumerate(experiment_list, 1):
        experiment_id = f"exp_{idx:04d}_{Path(graph_file).stem}_{treatment}_{outcome}_{estimator}"
        
        print(f"\n[{idx}/{total_experiments}] {experiment_type.capitalize()} ì‹¤í–‰ ì¤‘...")
        
        # ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰
        result = experiment_func(
            merged_df_clean=merged_df_clean,
            graph_file=graph_file,
            treatment=treatment,
            outcome=outcome,
            estimator=estimator,
            experiment_id=experiment_id,
            logger=logger,
            **kwargs
        )
        
        results.append(result)
        
        # CSVì— ê²°ê³¼ ì¶”ê°€
        if output_dir and csv_file and result.get("status") == "success":
            if experiment_type == "learning":
                csv_row = {
                    'graph_name': result.get('graph_name', ''),
                    'treatment': result.get('treatment', ''),
                    'estimator': result.get('estimator', ''),
                    'ate_value': result.get('ate_value'),
                    'placebo_passed': result.get('placebo_passed'),
                    'placebo_pvalue': result.get('placebo_pvalue'),
                    'unobserved_passed': result.get('unobserved_passed'),
                    'unobserved_pvalue': result.get('unobserved_pvalue'),
                    'subset_passed': result.get('subset_passed'),
                    'subset_pvalue': result.get('subset_pvalue'),
                    'dummy_passed': result.get('dummy_passed'),
                    'dummy_pvalue': result.get('dummy_pvalue'),
                    'f1_score': result.get('f1_score'),
                    'auc': result.get('auc'),
                    'duration_seconds': result.get('duration_seconds')
                }
            else:  # prediction
                metrics = result.get("metrics", {})
                csv_row = {
                    'graph_name': result.get('graph_name', ''),
                    'treatment': result.get('treatment', ''),
                    'estimator': result.get('estimator', ''),
                    'f1_score': metrics.get('f1_score'),
                    'auc': metrics.get('auc'),
                    'accuracy': metrics.get('accuracy')
                }
            
            # CSV íŒŒì¼ì— ê²°ê³¼ ì¶”ê°€
            try:
                existing_df = pd.read_csv(csv_file, encoding='utf-8-sig')
            except (FileNotFoundError, pd.errors.EmptyDataError):
                existing_df = pd.DataFrame(columns=csv_columns)
            
            new_row_df = pd.DataFrame([csv_row])
            updated_df = pd.concat([existing_df, new_row_df], ignore_index=True)
            updated_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (JSON)
        if output_dir:
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
        
        success_count = sum(1 for r in results if r.get("status") == "success")
        failed_count = sum(1 for r in results if r.get("status") == "failed")
        print(f"\nâœ… ì„±ê³µ: {success_count}, âŒ ì‹¤íŒ¨: {failed_count}")
    
    # ìµœì¢… ìš”ì•½
    print(f"\n{'='*80}")
    print(f"ğŸ“‹ {experiment_type.capitalize()} Experiments ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}")
    success_count = sum(1 for r in results if r.get("status") == "success")
    failed_count = sum(1 for r in results if r.get("status") == "failed")
    print(f"ì„±ê³µ: {success_count}")
    print(f"ì‹¤íŒ¨: {failed_count}")
    if output_dir:
        print(f"JSON ê²°ê³¼ íŒŒì¼: {results_file}")
        print(f"CSV ê²°ê³¼ íŒŒì¼: {csv_file}")
    print(f"{'='*80}\n")
    
    return results


def learning_experiments(
    merged_df_clean: pd.DataFrame,
    experiment_list: List[Tuple[str, str, str, str]],
    logger: Optional[Any] = None,
    output_dir: Optional[Path] = None
) -> List[Dict[str, Any]]:
    """
    experiment_listì˜ ëª¨ë“  ì¡°í•©ì— ëŒ€í•´ learning ì‹¤í–‰
    
    Args:
        merged_df_clean: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        experiment_list: ì‹¤í—˜ ì¡°í•© ë¦¬ìŠ¤íŠ¸ [(graph_file, treatment, outcome, estimator), ...]
        logger: ë¡œê±° ê°ì²´ (ì„ íƒì )
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„ íƒì )
    
    Returns:
        ì‹¤í—˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    return _run_experiments_batch(
        experiment_list=experiment_list,
        experiment_func=learning,
        experiment_type="learning",
        merged_df_clean=merged_df_clean,
        logger=logger,
        output_dir=output_dir
    )


def prediction(
    merged_df_clean: pd.DataFrame,
    graph_file: str,
    treatment: str,
    outcome: str,
    estimator: str,
    checkpoint_dir: Path,
    experiment_id: Optional[str] = None,
    logger: Optional[Any] = None
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ì‹¤í—˜ì— ëŒ€í•œ prediction í•¨ìˆ˜
    ë‹¨ì¼ ê·¸ë˜í”„, íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸ ë° í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ prediction ì§„í–‰ ë° ê²°ê³¼ ì§€í‘œ ì¶œë ¥
    
    Args:
        merged_df_clean: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        graph_file: ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
        treatment: íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸ ë³€ìˆ˜ëª…
        outcome: ê²°ê³¼ ë³€ìˆ˜ëª…
        estimator: ì¶”ì • ë°©ë²•
        checkpoint_dir: checkpoint ë””ë ‰í† ë¦¬ ê²½ë¡œ
        experiment_id: ì‹¤í—˜ ID (ì„ íƒì )
        logger: ë¡œê±° ê°ì²´ (ì„ íƒì )
    
    Returns:
        ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if experiment_id is None:
        experiment_id = f"{Path(graph_file).stem}_{treatment}_{outcome}_{estimator}"
    
    print(f"\n{'='*80}")
    print(f"ğŸ”® Prediction ì‹¤í–‰ - {experiment_id}")
    print(f"ê·¸ë˜í”„: {Path(graph_file).name}")
    print(f"Treatment: {treatment}, Outcome: {outcome}, Estimator: {estimator}")
    print(f"{'='*80}\n")
    
    try:
        result = run_inference(
            merged_df_clean=merged_df_clean,
            graph_file=graph_file,
            checkpoint_dir=checkpoint_dir,
            treatment=treatment,
            outcome=outcome,
            estimator=estimator,
            logger=logger,
            experiment_id=experiment_id
        )
        
        result["experiment_id"] = experiment_id
        result["graph_name"] = Path(graph_file).stem
        result["treatment"] = treatment
        result["outcome"] = outcome
        result["estimator"] = estimator
        
        if result.get("status") == "success":
            metrics = result.get("metrics", {})
            print(f"âœ… Prediction ì™„ë£Œ: {experiment_id}")
            print(f"   Accuracy: {metrics.get('accuracy', 'N/A')}")
            print(f"   F1 Score: {metrics.get('f1_score', 'N/A')}")
            print(f"   AUC: {metrics.get('auc', 'N/A')}")
        else:
            print(f"âŒ Prediction ì‹¤íŒ¨: {experiment_id}")
            if result.get("error"):
                print(f"   ì—ëŸ¬: {result['error']}")
        
        return result
        
    except Exception as e:
        print(f"âŒ Prediction ì‹¤íŒ¨: {experiment_id}")
        print(f"   ì—ëŸ¬: {e}")
        return {
            "status": "failed",
            "experiment_id": experiment_id,
            "error": str(e)
        }


def prediction_experiments(
    merged_df_clean: pd.DataFrame,
    experiment_list: List[Tuple[str, str, str, str]],
    checkpoint_dir: Path,
    logger: Optional[Any] = None,
    output_dir: Optional[Path] = None
) -> List[Dict[str, Any]]:
    """
    experiment_listì˜ ëª¨ë“  ì¡°í•©ì— ëŒ€í•´ prediction ì‹¤í–‰ ë° ê²°ê³¼ ì§€í‘œë¥¼ CSVë¡œ ì €ì¥
    
    Args:
        merged_df_clean: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        experiment_list: ì‹¤í—˜ ì¡°í•© ë¦¬ìŠ¤íŠ¸ [(graph_file, treatment, outcome, estimator), ...]
        checkpoint_dir: checkpoint ë””ë ‰í† ë¦¬ ê²½ë¡œ
        logger: ë¡œê±° ê°ì²´ (ì„ íƒì )
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„ íƒì )
    
    Returns:
        ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    return _run_experiments_batch(
        experiment_list=experiment_list,
        experiment_func=prediction,
        experiment_type="prediction",
        merged_df_clean=merged_df_clean,
        logger=logger,
        output_dir=output_dir,
        checkpoint_dir=checkpoint_dir
    )




def main():
    parser = argparse.ArgumentParser(description="LaborLab 2 ì¸ê³¼ì¶”ë¡  ë¶„ì„ íŒŒì´í”„ë¼ì¸")
    parser.add_argument("--config", type=str, default="config.json", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    script_dir = Path(__file__).parent.parent
    
    if os.path.isabs(args.config):
        config_path = Path(args.config)
    else:
        config_path = script_dir / args.config
    
    if not config_path.exists():
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        print(f"   í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
        print(f"   ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {script_dir}")
        return
    
    print(f"ğŸ“„ ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
    config = load_config(config_path)
    
    # ì„¤ì • ê°’ ì¶”ì¶œ
    data_dir = config.get("data_dir", "data")
    seis_data_dir = config.get("seis_data_dir", "seis_data")
    graph_data_dir = config.get("graph_data_dir", "graph_data")
    output_dir = config.get("output_dir", "log")
    limit_data = config.get("limit_data", False)
    limit_size = config.get("limit_size", 5000)
    checkpoint_dir = config.get("checkpoint_dir", "data/checkpoint")
    job_category_file = config.get("job_category_file", "KSIC")
    
    # ìƒˆë¡œìš´ ì„¤ì • ë³€ìˆ˜
    do_preprocess = config.get("preprocess", True)
    do_learning = config.get("learning", False)
    do_prediction = config.get("prediction", False)
    do_experiment = config.get("experiment", False)
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    data_dir_path = script_dir / data_dir
    output_dir_path = script_dir / output_dir
    output_dir_path.mkdir(exist_ok=True)
    checkpoint_dir_path = script_dir / checkpoint_dir
    checkpoint_dir_path.mkdir(parents=True, exist_ok=True)
    
    # ë¡œê±° ì„¤ì •
    logger = None
    if not config.get("no_logs", False):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        logger = setup_logging(
            log_dir=output_dir_path,
            log_filename=f"pipeline_{timestamp}.log"
        )
        if logger:
            logger.info(f"íŒŒì´í”„ë¼ì¸ ì‹œì‘ - {timestamp}")
    
    merged_df = None
    merged_df_clean = None
    experiment_list = []
    
    # ========================================================================
    # 1. ì „ì²˜ë¦¬ ì‹¤í–‰
    # ========================================================================
    if do_preprocess:
        merged_df = preprocess(
            data_dir_path=data_dir_path,
            seis_data_dir=seis_data_dir,
            limit_data=limit_data,
            limit_size=limit_size,
            job_category_file=job_category_file
        )
        
        # ì¸ê³¼ ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        merged_df_clean = prepare_data_for_causal_model(
            merged_df=merged_df,
            config=config,
            data_dir_path=data_dir_path,
            graph_data_dir=graph_data_dir
        )
    
    # ========================================================================
    # 2. experiment_list ìƒì„±
    # ========================================================================
    if do_experiment or do_learning or do_prediction:
        experiment_list = create_experiment_list(
            config=config,
            data_dir_path=data_dir_path,
            graph_data_dir=graph_data_dir
        )
        
        if not experiment_list:
            print("âŒ ìœ íš¨í•œ ì‹¤í—˜ ì¡°í•©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“Š ìƒì„±ëœ ì‹¤í—˜ ì¡°í•©: {len(experiment_list)}ê°œ\n")
    
    # ========================================================================
    # 3. Learning ì‹¤í–‰
    # ========================================================================
    if do_learning:
        if merged_df_clean is None:
            print("âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocessë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        if do_experiment:
            # experiment_listì˜ ëª¨ë“  ì¡°í•©ì— ëŒ€í•´ learning ì‹¤í–‰
            learning_experiments(
                merged_df_clean=merged_df_clean,
                experiment_list=experiment_list,
                logger=logger,
                output_dir=output_dir_path
            )
        else:
            # ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (configì—ì„œ ì²« ë²ˆì§¸ ì‹¤í—˜ ì¡°í•© ì‚¬ìš©)
            if experiment_list:
                graph_file, treatment, outcome, estimator = experiment_list[0]
                learning(
                    merged_df_clean=merged_df_clean,
                    graph_file=graph_file,
                    treatment=treatment,
                    outcome=outcome,
                    estimator=estimator,
                    logger=logger
                )
            else:
                print("âŒ ì‹¤í–‰í•  ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ========================================================================
    # 4. Prediction ì‹¤í–‰
    # ========================================================================
    if do_prediction:
        if merged_df_clean is None:
            print("âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. preprocessë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        if do_experiment:
            # experiment_listì˜ ëª¨ë“  ì¡°í•©ì— ëŒ€í•´ prediction ì‹¤í–‰
            prediction_experiments(
                merged_df_clean=merged_df_clean,
                experiment_list=experiment_list,
                checkpoint_dir=checkpoint_dir_path,
                logger=logger,
                output_dir=output_dir_path
            )
        else:
            # ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (configì—ì„œ ì²« ë²ˆì§¸ ì‹¤í—˜ ì¡°í•© ì‚¬ìš©)
            if experiment_list:
                graph_file, treatment, outcome, estimator = experiment_list[0]
                prediction(
                    merged_df_clean=merged_df_clean,
                    graph_file=graph_file,
                    treatment=treatment,
                    outcome=outcome,
                    estimator=estimator,
                    checkpoint_dir=checkpoint_dir_path,
                    logger=logger
                )
            else:
                print("âŒ ì‹¤í–‰í•  ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"\n{'='*80}")
    print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
