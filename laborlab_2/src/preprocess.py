"""
DoWhy ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ
- Basic ì „ì²˜ë¦¬: ê¸°ë³¸ì ì¸ ë°ì´í„° ì •ì œ ë° ë³€í™˜
- NLP ì „ì²˜ë¦¬: í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ

ì‚¬ìš© ì˜ˆì‹œ:
    # JSON íŒŒì¼ë“¤ì— ëŒ€í•´ ê°ê° ë‹¤ë¥¸ ì „ì²˜ë¦¬ ì ìš©
    preprocessor = Preprocessor([])
    
    # ë°©ë²• 1: ê°œë³„ íŒŒì¼ ì²˜ë¦¬
    resume_data = preprocessor.load_and_preprocess_data('resume.json', json_name='ì´ë ¥ì„œ')
    cover_letter_data = preprocessor.load_and_preprocess_data('cover_letter.json', json_name='ìê¸°ì†Œê°œì„œ')
    
    # Excel íŒŒì¼ ì²˜ë¦¬
    excel_data = preprocessor.load_and_preprocess_data('data.xlsx', sheet_name='Sheet1')
    
    # ë°©ë²• 2: ì—¬ëŸ¬ íŒŒì¼ì„ í•œë²ˆì— ì²˜ë¦¬
    file_list = ['resume.json', 'cover_letter.json', 'training.json', 'certification.json']
    json_names = ['ì´ë ¥ì„œ', 'ìê¸°ì†Œê°œì„œ', 'ì§ì—…í›ˆë ¨', 'ìê²©ì¦']
    merged_df = preprocessor.get_merged_df(file_list, json_names=json_names)
"""

import pandas as pd
import numpy as np
import json
import re
import os
from datetime import datetime
from typing import Optional, List, Dict, Any
from collections import Counter
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import partial
import time

from .llm_reference import (
    JSON_NAMES, RESUME_SECTIONS, SUPPORTED_SECTIONS, 
    DEFAULT_MAX_COVER_LEN, DEFAULT_COVER_EXCEED_RATIO, DEFAULT_DATE_FORMAT
)
from .llm_scorer import LLMScorer



class Preprocessor:
    def __init__(self, df_list, job_category_file="KSIC"):
        self.json_names = JSON_NAMES
        self.sheet_name = 'êµ¬ì§ì¸ì¦ ê´€ë ¨ ë°ì´í„°'
        self.df_list = []
        self.variable_mapping = self.load_variable_mapping()
        self.llm_scorer = LLMScorer()
        self.hope_jscd1_map = {}  # JHNT_MBN -> HOPE_JSCD1 ë§¤í•‘ ì €ì¥
        self.job_category_file = job_category_file  # ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ëª… (KECO, KSCO, KSIC)
        self.job_code_to_name = self.load_job_mapping()  # ì†Œë¶„ë¥˜ì½”ë“œ -> ì†Œë¶„ë¥˜ëª… ë§¤í•‘

    def load_variable_mapping(self):
        # variable_mapping.jsonì€ data í´ë”ì— ìˆìŒ
        # __file__ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ê³„ì‚°: src/preprocess.py -> laborlab_2/ -> data/
        preprocess_file = Path(__file__)  # src/preprocess.py
        laborlab_dir = preprocess_file.parent.parent  # laborlab_2/
        variable_mapping_path = laborlab_dir / "data" / "variable_mapping.json"
        
        with open(variable_mapping_path, encoding='utf-8') as f:
            variable_mapping = json.load(f)
        return variable_mapping
    
    def load_job_mapping(self):
        """job_subcategories_XXXX.csvë¥¼ ë¡œë“œí•˜ì—¬ ì†Œë¶„ë¥˜ì½”ë“œ -> ì†Œë¶„ë¥˜ëª… ë§¤í•‘ ìƒì„±"""
        try:
            # __file__ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ê³„ì‚°: src/preprocess.py -> laborlab_2/ -> data/
            preprocess_file = Path(__file__)  # src/preprocess.py
            laborlab_dir = preprocess_file.parent.parent  # laborlab_2/
            
            # job_category_fileì— ë”°ë¼ íŒŒì¼ëª… ê²°ì • (KECO, KSCO, KSIC)
            job_category_file = self.job_category_file.upper()
            if job_category_file not in ["KECO", "KSCO", "KSIC"]:
                print(f"âš ï¸ ì˜ëª»ëœ ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ëª…: {job_category_file}. ê¸°ë³¸ê°’ KSIC ì‚¬ìš©")
                job_category_file = "KSIC"
            
            job_mapping_path = laborlab_dir / "data" / f"job_subcategories_{job_category_file}.csv"
            
            if not job_mapping_path.exists():
                print(f"âš ï¸ ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_mapping_path}")
                print(f"   ê¸°ë³¸ê°’ job_subcategories_KSIC.csv ì‚¬ìš© ì‹œë„")
                job_mapping_path = laborlab_dir / "data" / "job_subcategories_KSIC.csv"
            
            job_df = pd.read_csv(job_mapping_path, encoding='utf-8')
            print(f"âœ… ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {job_mapping_path.name} ({len(job_df)}ê°œ ì§ì¢…)")
            
            # ì†Œë¶„ë¥˜ì½”ë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            job_mapping = dict(zip(job_df['ì†Œë¶„ë¥˜ì½”ë“œ'].astype(str).str.zfill(3), job_df['ì†Œë¶„ë¥˜ëª…']))
            return job_mapping
        except Exception as e:
            print(f"âŒ ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_job_name_from_code(self, code):
        """HOPE_JSCD1 ì½”ë“œë¥¼ ì§ì¢…ëª…ìœ¼ë¡œ ë³€í™˜"""
        if not code:
            return "ë¯¸ìƒ"
        # ì½”ë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì•ì— 0ì„ ì±„ì›Œì„œ 3ìë¦¬ë¡œ ë§Œë“¤ê¸°
        code_str = str(code).zfill(3)
        return self.job_code_to_name.get(code_str, f"ì§ì¢…ì½”ë“œ {code}")

    @staticmethod
    def get_data_info(df):
        """ë°ì´í„° ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        info = {
            'shape': df.shape,
            'columns': list(df.columns),
            'missing_values': df.isnull().sum().to_dict(),
            'dtypes': df.dtypes.to_dict()
        }
        return info

    def basic_preprocessing(self, df):
        """
        ê¸°ë³¸ì ì¸ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        
        Returns:
            pd.DataFrame: ê¸°ë³¸ ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        """
        # ë””ë²„ê¹…: ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼ í™•ì¸
        print(f"[DEBUG] basic_preprocessing ì‹œì‘ - ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        print(f"[DEBUG] ì›ë³¸ ë°ì´í„°ì— JHNT_CTN ì¡´ì¬: {'JHNT_CTN' in df.columns}")
        print(f"[DEBUG] ì›ë³¸ ë°ì´í„°ì— JHNT_MBN ì¡´ì¬: {'JHNT_MBN' in df.columns}")
        

        # ë³‘í•©ì— í•„ìš”í•œ í‚¤ ì»¬ëŸ¼ì€ í•­ìƒ ìœ ì§€
        merge_keys = ["JHNT_CTN", "JHNT_MBN"]
        existing_merge_keys = [key for key in merge_keys if key in df.columns]
        print(f"[DEBUG] ë°œê²¬ëœ ë³‘í•© í‚¤: {existing_merge_keys}")
        
        # variable_mapping.jsonì˜ structured_data í‚¤ë§Œ ì‚¬ìš©
        structured_keys = set(self.variable_mapping.get("structured_data", {}).keys())
        
        # ì›ë³¸ ë°ì´í„°ì—ì„œ í•´ë‹¹ ë³€ìˆ˜ë“¤ë§Œ í•„í„°ë§ (ì¡´ì¬í•˜ëŠ” ë³€ìˆ˜ë§Œ)
        available_vars = list(structured_keys & set(df.columns))
        missing_vars = list(structured_keys - set(df.columns))
        
        if missing_vars:
            print(f"ë‹¤ìŒ ë³€ìˆ˜ë“¤ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤: {missing_vars}")
        
        # ë³‘í•© í‚¤ì™€ í•„í„°ë§ëœ ë³€ìˆ˜ë“¤ì„ í•©ì¹¨ (ì¤‘ë³µ ì œê±°)
        final_vars = list(set(available_vars + existing_merge_keys))
        print(f"[DEBUG] ìµœì¢… ì»¬ëŸ¼ ìˆ˜: {len(final_vars)}, JHNT_MBN í¬í•¨ ì—¬ë¶€: {'JHNT_MBN' in final_vars}")
        df = df[final_vars]

        # BFR_OCTR_YN ì œê±°, BFR_OCTR_CTë§Œ ìœ ì§€
        if "BFR_OCTR_YN" in df.columns and "BFR_OCTR_CT" in df.columns:
            df = df.drop(columns=["BFR_OCTR_YN"])
            print(f"[DEBUG] BFR_OCTR_YN ì œê±° í›„ JHNT_MBN ì¡´ì¬: {'JHNT_MBN' in df.columns}")

        # 9ê°œ ì˜ˆ/ì•„ë‹ˆì˜¤ ë³€ìˆ˜ â†’ í•©ì³ì„œ ìƒˆë¡œìš´ ìˆœì„œí˜• ë²”ì£¼ ë³€ìˆ˜ ìƒì„±
        agree_vars = [
            "EMAIL_RCYN", "SAEIL_CNTC_AGRE_YN", "SHRS_IDIF_AOFR_YN", "SULC_IDIF_AOFR_YN",
            "IDIF_IQRY_AGRE_YN", "SMS_RCYN", "EMAIL_OTPB_YN", "MPNO_OTPB_YN", "EMAIL_RCYN"
        ]

        # ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ì‚¬ìš©
        agree_vars = [col for col in agree_vars if col in df.columns]

        if agree_vars:
            agree_count = (df[agree_vars] == "ì˜ˆ").sum(axis=1)
            df["AGREE_LEVEL"] = agree_count.apply(lambda x: "í•˜" if x <= 3 else ("ì¤‘" if x <= 6 else "ìƒ"))
            df = df.drop(columns=agree_vars)
            print(f"[DEBUG] agree_vars ì œê±° í›„ JHNT_MBN ì¡´ì¬: {'JHNT_MBN' in df.columns}")

        # HOPE_JSCD1_NAME ë³€ìˆ˜ ì¶”ê°€ (HOPE_JSCD1 ì½”ë“œë¥¼ ì†Œë¶„ë¥˜ëª…ìœ¼ë¡œ ë³€í™˜)
        if "HOPE_JSCD1" in df.columns:
            df["HOPE_JSCD1_NAME"] = df["HOPE_JSCD1"].apply(lambda code: self.get_job_name_from_code(code))
            print(f"[DEBUG] HOPE_JSCD1_NAME ë³€ìˆ˜ ì¶”ê°€ ì™„ë£Œ: {df['HOPE_JSCD1_NAME'].nunique()}ê°œ ê³ ìœ ê°’")

        print(f"[DEBUG] basic_preprocessing ì™„ë£Œ - ìµœì¢… ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}, JHNT_MBN ì¡´ì¬: {'JHNT_MBN' in df.columns}")
        if 'JHNT_MBN' in df.columns:
            print(f"[DEBUG] JHNT_MBN ìƒ˜í”Œ ê°’: {df['JHNT_MBN'].head(3).tolist()}")
        
        return df

    def nlp_preprocessing(self, data, json_name=None, limit_data=False, limit_size=5000):
        """
        NLP ê¸°ë°˜ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            data: json íŒŒì¼ (ìê¸°ì†Œê°œì„œ, ì´ë ¥ì„œ, ì§ì—…í›ˆë ¨, ìê²©ì¦)
            json_name (str): JSON ë°ì´í„° íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ì „ì²˜ë¦¬ ì ìš©
            limit_data (bool): í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ë°ì´í„° ì œí•œ ì—¬ë¶€
            limit_size (int): ì œí•œí•  ë°ì´í„° í¬ê¸°
        Returns:
            pd.DataFrame: NLP ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        """
        # JSON íŒŒì¼ì€ ë°°ì—´ í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë¡œë“œë¨
        if isinstance(data, list):
            if limit_data and len(data) > limit_size:
                original_count = len(data)
                data = data[:limit_size]
                print(f"ğŸ“Š {json_name} ë°ì´í„° ì œí•œ: {len(data)}ê°œ ë ˆì½”ë“œ ì‚¬ìš© (ì „ì²´ {original_count}ê°œ ì¤‘ ì• {limit_size}ê°œ)")
            else:
                # limit_dataê°€ Falseì´ë©´ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬
                print(f"ğŸ“Š {json_name} ì „ì²´ ë°ì´í„° ì²˜ë¦¬: {len(data)}ê°œ ë ˆì½”ë“œ")
        else:
            # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° (ë‹¨ì¼ ê°ì²´)ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë‚˜ì¤‘ì— _preprocess_* í•¨ìˆ˜ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ë¨)
            print(f"âš ï¸ {json_name} ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤. ë‹¨ì¼ ê°ì²´ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        
        # JSON ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ íŠ¹í™”ëœ ì „ì²˜ë¦¬
        if json_name == 'ì´ë ¥ì„œ':
            df_processed = self._preprocess_resume(data)
        elif json_name == 'ìê¸°ì†Œê°œì„œ':
            df_processed = self._preprocess_cover_letter(data)
        elif json_name == 'ì§ì—…í›ˆë ¨':
            df_processed = self._preprocess_training(data)
        elif json_name == 'ìê²©ì¦':
            df_processed = self._preprocess_certification(data)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” json íŒŒì¼ì…ë‹ˆë‹¤. {json_name}")
        
        return df_processed


    def _process_single_resume(self, item):
        """ë‹¨ì¼ ì´ë ¥ì„œ ë ˆì½”ë“œ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        # SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ ë³€í™˜
        seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "")
        if not seek_id:
            return None
        
        # BASIC_RESUME_YN == "Y"ì¸ resume ì°¾ê¸°
        resumes = item.get("RESUMES", [])
        basic_resume = None
        for resume in resumes:
            if str(resume.get("BASIC_RESUME_YN", "")).upper() == "Y":
                basic_resume = resume
                break
        
        # ê¸°ë³¸ ì´ë ¥ì„œê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if basic_resume is None:
            return {
                "JHNT_MBN": seek_id,
                "resume_score": None,
                "items_num": 0
            }
        
        # ITEMS ê°€ì ¸ì˜¤ê¸°
        items = basic_resume.get("ITEMS", [])
        items_num = len(items)
        
        # variable_mappingì—ì„œ resume ì„¹ì…˜ ê°€ì ¸ì˜¤ê¸°
        resume_mapping = self.variable_mapping.get("resume", {})
        
        # ITEMSë¥¼ í¬ë§¤íŒ…
        formatting_sentence = ""
        for item_data in items:
            for key, value in item_data.items():
                # variable_mappingì—ì„œ í•œê¸€ ë³€ìˆ˜ëª… ì°¾ê¸°
                if key in resume_mapping:
                    korean_key = resume_mapping[key].get("ë³€ìˆ˜ëª…", key)
                else:
                    korean_key = key
                
                # valueê°€ Noneì´ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                value_str = str(value) if value is not None else ""
                formatting_sentence += f"{korean_key}: {value_str}\n"
            formatting_sentence += "\n"
        
        # í¬ë§¤íŒ…ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not formatting_sentence.strip():
            formatting_sentence = "ì •ë³´ ì—†ìŒ"
        
        # HOPE_JSCD1 ì •ë³´ ê°€ì ¸ì™€ì„œ ì§ì¢…ëª…ìœ¼ë¡œ ë³€í™˜
        hope_jscd1 = self.hope_jscd1_map.get(seek_id, "")
        job_name = self.get_job_name_from_code(hope_jscd1)
        job_examples = []  # í•„ìš”ì‹œ HOPE_JSCD1ë¡œë¶€í„° ì§ì¢… ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥
        
        # LLM scorerì— ì „ë‹¬í•˜ì—¬ ì ìˆ˜ ê³„ì‚°
        score, _ = self.llm_scorer.score("ì´ë ¥ì„œ", job_name, job_examples, formatting_sentence)
        
        return {
            "JHNT_MBN": seek_id,
            "resume_score": score,
            "items_num": items_num
        }
    
    def _preprocess_resume(self, data):
        """ì´ë ¥ì„œ íŠ¹í™” ì „ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬)"""
        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬ (JSON íŒŒì¼ì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¼ ìˆ˜ ìˆìŒ)
        if not isinstance(data, list):
            data = [data]
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° ë ˆì½”ë“œ ì²˜ë¦¬
        max_workers = min(len(data), 10)  # ìµœëŒ€ 10ê°œ ìŠ¤ë ˆë“œ
        rows = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self._process_single_resume, item): item for item in data}
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result is not None:
                        rows.append(result)
                except Exception as e:
                    item = futures[future]
                    seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "unknown")
                    print(f"âš ï¸ ì´ë ¥ì„œ ì²˜ë¦¬ ì˜¤ë¥˜ (JHNT_MBN: {seek_id}): {e}")
                    rows.append({
                        "JHNT_MBN": seek_id,
                        "resume_score": None,
                        "items_num": 0
                    })
        
        # DataFrame ìƒì„± ì „ì— Logger ê°ì²´ í™•ì¸ ë° ì œê±°
        import logging
        cleaned_rows = []
        for row_idx, row in enumerate(rows):
            cleaned_row = {}
            for key, value in row.items():
                # Logger ê°ì²´ì¸ì§€ í™•ì¸
                if isinstance(value, logging.Logger) or 'Logger' in str(type(value)):
                    print(f"âš ï¸ [ì´ë ¥ì„œ ì „ì²˜ë¦¬] {row_idx}ë²ˆì§¸ í–‰ì˜ ë”•ì…”ë„ˆë¦¬ í‚¤ '{key}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(value).__name__})")
                    cleaned_row[key] = np.nan
                else:
                    cleaned_row[key] = value
            cleaned_rows.append(cleaned_row)
        
        df = pd.DataFrame(cleaned_rows)
        
        # SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ rename (ìˆëŠ” ê²½ìš°)
        if 'SEEK_CUST_NO' in df.columns and 'JHNT_MBN' not in df.columns:
            df = df.rename(columns={'SEEK_CUST_NO': 'JHNT_MBN'})
            print(f"âœ… ì´ë ¥ì„œ ë°ì´í„°: SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ ë³€ê²½")
        elif 'SEEK_CUST_NO' in df.columns and 'JHNT_MBN' in df.columns:
            # ë‘˜ ë‹¤ ìˆìœ¼ë©´ SEEK_CUST_NOì˜ ê°’ìœ¼ë¡œ JHNT_MBNì„ ì±„ìš°ê³  SEEK_CUST_NO ì œê±°
            df['JHNT_MBN'] = df['JHNT_MBN'].fillna(df['SEEK_CUST_NO'])
            df = df.drop(columns=['SEEK_CUST_NO'])
            print(f"âœ… ì´ë ¥ì„œ ë°ì´í„°: SEEK_CUST_NO ê°’ì„ JHNT_MBNì— ë³‘í•© í›„ SEEK_CUST_NO ì œê±°")
        
        return df


    def _process_single_cover_letter(self, item):
        """ë‹¨ì¼ ìê¸°ì†Œê°œì„œ ë ˆì½”ë“œ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        # SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ ë³€í™˜
        seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "")
        if not seek_id:
            return None
                
        # ìê¸°ì†Œê°œì„œ ë°ì´í„° ì¶”ì¶œ (BASS_SFID_YN == "Y"ì¸ í•­ëª©ë§Œ)
        texts = []
        items = []
        for c in item.get("COVERLETTERS", []):
            if str(c.get("BASS_SFID_YN", "")).upper() == "Y":
                items = c.get("ITEMS", []) or []
                for it in items:
                    t = it.get("SELF_INTRO_CONT", "")
                    if t:
                        texts.append(t.strip())
                break
        
        full_text = "\n\n".join(texts) if texts else "ì •ë³´ ì—†ìŒ"
        
        # HOPE_JSCD1 ì •ë³´ ê°€ì ¸ì™€ì„œ ì§ì¢…ëª…ìœ¼ë¡œ ë³€í™˜
        hope_jscd1 = self.hope_jscd1_map.get(seek_id, "")
        job_name = self.get_job_name_from_code(hope_jscd1)
        job_examples = []  # í•„ìš”ì‹œ HOPE_JSCD1ë¡œë¶€í„° ì§ì¢… ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥
        
        # ì ìˆ˜ ê³„ì‚°ê³¼ ì˜¤íƒˆì ìˆ˜ ê³„ì‚°ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=2) as executor:
            score_future = executor.submit(self.llm_scorer.score, "ìê¸°ì†Œê°œì„œ", job_name, job_examples, full_text)
            typo_future = executor.submit(self.llm_scorer.count_typos, full_text)
            
            score, _ = score_future.result()
            typo_count = typo_future.result()
        
        # scoreì™€ ì˜¤íƒˆì ìˆ˜ë§Œ ë°˜í™˜ (ê·¸ë˜í”„ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜)
        return {
            "JHNT_MBN": seek_id,
            "cover_letter_score": score,  # ê·¸ë˜í”„: cover_letter_score
            "cover_letter_typo_count": typo_count  # ê·¸ë˜í”„: cover_letter_typo_count
        }
    
    def _preprocess_cover_letter(self, data):
        """ìê¸°ì†Œê°œì„œ íŠ¹í™” ì „ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬)"""
        if not isinstance(data, list):
            data = [data]
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° ë ˆì½”ë“œ ì²˜ë¦¬
        max_workers = min(len(data), 10)  # ìµœëŒ€ 10ê°œ ìŠ¤ë ˆë“œ
        rows = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self._process_single_cover_letter, item): item for item in data}
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result is not None:
                        rows.append(result)
                except Exception as e:
                    item = futures[future]
                    seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "unknown")
                    print(f"âš ï¸ ìê¸°ì†Œê°œì„œ ì²˜ë¦¬ ì˜¤ë¥˜ (JHNT_MBN: {seek_id}): {e}")
                    rows.append({
                        "JHNT_MBN": seek_id,
                        "cove_letter_score": None,
                        "cover_letter_typo_count": 0
                    })
        
        # DataFrame ìƒì„± ì „ì— Logger ê°ì²´ í™•ì¸ ë° ì œê±°
        import logging
        cleaned_rows = []
        for row_idx, row in enumerate(rows):
            cleaned_row = {}
            for key, value in row.items():
                # Logger ê°ì²´ì¸ì§€ í™•ì¸
                if isinstance(value, logging.Logger) or 'Logger' in str(type(value)):
                    print(f"âš ï¸ [ìê¸°ì†Œê°œì„œ ì „ì²˜ë¦¬] {row_idx}ë²ˆì§¸ í–‰ì˜ ë”•ì…”ë„ˆë¦¬ í‚¤ '{key}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(value).__name__})")
                    cleaned_row[key] = np.nan
                else:
                    cleaned_row[key] = value
            cleaned_rows.append(cleaned_row)
        
        df = pd.DataFrame(cleaned_rows)
        
        # SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ rename (ìˆëŠ” ê²½ìš°)
        if 'SEEK_CUST_NO' in df.columns and 'JHNT_MBN' not in df.columns:
            df = df.rename(columns={'SEEK_CUST_NO': 'JHNT_MBN'})
            print(f"âœ… ìê¸°ì†Œê°œì„œ ë°ì´í„°: SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ ë³€ê²½")
        elif 'SEEK_CUST_NO' in df.columns and 'JHNT_MBN' in df.columns:
            # ë‘˜ ë‹¤ ìˆìœ¼ë©´ SEEK_CUST_NOì˜ ê°’ìœ¼ë¡œ JHNT_MBNì„ ì±„ìš°ê³  SEEK_CUST_NO ì œê±°
            df['JHNT_MBN'] = df['JHNT_MBN'].fillna(df['SEEK_CUST_NO'])
            df = df.drop(columns=['SEEK_CUST_NO'])
            print(f"âœ… ìê¸°ì†Œê°œì„œ ë°ì´í„°: SEEK_CUST_NO ê°’ì„ JHNT_MBNì— ë³‘í•© í›„ SEEK_CUST_NO ì œê±°")
        
        return df


    def _process_single_training(self, item):
        """ë‹¨ì¼ ì§ì—…í›ˆë ¨ ë ˆì½”ë“œ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        # JHNT_CTNì„ í‚¤ë¡œ ì‚¬ìš©
        jhnt_ctn = item.get("JHNT_CTN", "")
        if not jhnt_ctn:
            return None
        
        # êµ¬ì§ì¸ì¦ ì¼ì ê°€ì ¸ì˜¤ê¸°
        jhcr_de = item.get("JHCR_DE", "")  # êµ¬ì§ì¸ì¦ ì¼ì
        
        # TRAININGSì—ì„œ í›ˆë ¨ ë°ì´í„° ì¶”ì¶œ
        trainings = item.get("TRAININGS", [])
        
        # TRAININGSì—ì„œ ëª¨ë“  TRNG_ENDE ê°€ì ¸ì™€ì„œ datetime ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        training_end_dates = []
        for tr in trainings:
            trng_ende = tr.get("TRNG_ENDE", "").strip()
            if trng_ende:
                try:
                    # ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
                    date_obj = datetime.strptime(trng_ende, DEFAULT_DATE_FORMAT)
                    training_end_dates.append(date_obj)
                except:
                    pass
        
        # ê²½ê³¼ì¼ ê³„ì‚°: JHCR_DE - ìµœê·¼ TRNG_ENDE (ì¼ìˆ˜ ì°¨ì´)
        elapsed_days = None
        if jhcr_de and training_end_dates:
            try:
                # êµ¬ì§ì¸ì¦ ì¼ìë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
                jhcr_date = datetime.strptime(jhcr_de, DEFAULT_DATE_FORMAT)
                # ê°€ì¥ ìµœê·¼ í›ˆë ¨ ì¢…ë£Œì¼ (ìµœëŒ€ê°’)
                latest_end_date = max(training_end_dates)
                # ì¼ìˆ˜ ì°¨ì´ ê³„ì‚°
                elapsed_days = (jhcr_date - latest_end_date).days
                elapsed_days = elapsed_days if elapsed_days >= 0 else None
            except:
                elapsed_days = None
        
        # í…ìŠ¤íŠ¸ í¬ë§·íŒ…: {TRNG_CRSN}: ({TRNG_BGDE} ~ {TRNG_ENDE})
        training_texts = []
        for tr in trainings:
            trng_crsn = tr.get("TRNG_CRSN", "").strip()  # í›ˆë ¨ ê³¼ì •ëª…
            trng_bgde = tr.get("TRNG_BGDE", "").strip()  # í›ˆë ¨ ì‹œì‘ì¼
            trng_ende = tr.get("TRNG_ENDE", "").strip()  # í›ˆë ¨ ì¢…ë£Œì¼
            if trng_crsn and trng_bgde and trng_ende:
                training_texts.append(f"{trng_crsn}: ({trng_bgde} ~ {trng_ende})")
        
        text = "\n".join(training_texts) if training_texts else "ì •ë³´ ì—†ìŒ"
        
        # seek_idëŠ” HOPE_JSCD1 ë§¤í•‘ì„ ìœ„í•´ ì‚¬ìš© (JHNT_MBNì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None)
        seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "")
        
        # HOPE_JSCD1 ì •ë³´ ê°€ì ¸ì™€ì„œ ì§ì¢…ëª…ìœ¼ë¡œ ë³€í™˜
        hope_jscd1 = self.hope_jscd1_map.get(seek_id, "")
        job_name = self.get_job_name_from_code(hope_jscd1)
        job_examples = []  # í•„ìš”ì‹œ HOPE_JSCD1ë¡œë¶€í„° ì§ì¢… ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥
        
        # ì ìˆ˜ ê³„ì‚°
        score, why = self.llm_scorer.score("ì§ì—…í›ˆë ¨", job_name, job_examples, text)
        
        return {
            "JHNT_CTN": jhnt_ctn,
            "training_score": score,
            "days_last_training_to_jobseek": elapsed_days if elapsed_days is not None else None  # ê·¸ë˜í”„: days_last_training_to_jobseek
        }
    
    def _preprocess_training(self, data):
        """ì§ì—…í›ˆë ¨ íŠ¹í™” ì „ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬)"""
        if not isinstance(data, list):
            data = [data]
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° ë ˆì½”ë“œ ì²˜ë¦¬
        max_workers = min(len(data), 10)  # ìµœëŒ€ 10ê°œ ìŠ¤ë ˆë“œ
        rows = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self._process_single_training, item): item for item in data}
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result is not None:
                        rows.append(result)
                except Exception as e:
                    item = futures[future]
                    jhnt_ctn = item.get("JHNT_CTN", "unknown")
                    print(f"âš ï¸ ì§ì—…í›ˆë ¨ ì²˜ë¦¬ ì˜¤ë¥˜ (JHNT_CTN: {jhnt_ctn}): {e}")
                    rows.append({
                        "JHNT_CTN": jhnt_ctn,
                        "training_score": None,
                        "days_last_training_to_jobseek": None
                    })
        
        # DataFrame ìƒì„± ì „ì— Logger ê°ì²´ í™•ì¸ ë° ì œê±°
        import logging
        cleaned_rows = []
        for row_idx, row in enumerate(rows):
            cleaned_row = {}
            for key, value in row.items():
                # Logger ê°ì²´ì¸ì§€ í™•ì¸
                if isinstance(value, logging.Logger) or 'Logger' in str(type(value)):
                    print(f"âš ï¸ [ì§ì—…í›ˆë ¨ ì „ì²˜ë¦¬] {row_idx}ë²ˆì§¸ í–‰ì˜ ë”•ì…”ë„ˆë¦¬ í‚¤ '{key}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(value).__name__})")
                    cleaned_row[key] = np.nan
                else:
                    cleaned_row[key] = value
            cleaned_rows.append(cleaned_row)
        
        return pd.DataFrame(cleaned_rows)


    def _process_single_certification(self, item):
        """ë‹¨ì¼ ìê²©ì¦ ë ˆì½”ë“œ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        # JHNT_CTNì„ í‚¤ë¡œ ì‚¬ìš©
        jhnt_ctn = item.get("JHNT_CTN", "")
        if not jhnt_ctn:
            return None
        
        # JSONì—ì„œ ìê²©ì¦ ë°ì´í„° ì¶”ì¶œ
        licenses = item.get("LICENSES", [])
        
        # ìê²©ì¦ í¬ë§·íŒ…: ìê²©ì¦1: ì „ê¸°ê¸°ëŠ¥ì‚¬/êµ­ê°€ê¸°ìˆ ìê²© í˜•ì‹
        formatted_texts = []
        for idx, lic in enumerate(licenses, start=1):
            qulf_itnm = lic.get("QULF_ITNM", "").strip()  # ìê²©ì¦ëª…
            qulf_lcns_lcfn = lic.get("QULF_LCNS_LCFN", "").strip()  # ìê²©ì¦ ë¶„ë¥˜
            
            if qulf_itnm and qulf_lcns_lcfn:
                formatted_texts.append(f"ìê²©ì¦{idx}: {qulf_itnm}/{qulf_lcns_lcfn}")
            elif qulf_itnm:
                formatted_texts.append(f"ìê²©ì¦{idx}: {qulf_itnm}")
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        text = "\n".join(formatted_texts) if formatted_texts else "ì •ë³´ ì—†ìŒ"
        
        # seek_idëŠ” HOPE_JSCD1 ë§¤í•‘ì„ ìœ„í•´ ì‚¬ìš© (JHNT_MBNì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None)
        seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "")
        
        # HOPE_JSCD1 ì •ë³´ ê°€ì ¸ì™€ì„œ ì§ì¢…ëª…ìœ¼ë¡œ ë³€í™˜
        hope_jscd1 = self.hope_jscd1_map.get(seek_id, "")
        job_name = self.get_job_name_from_code(hope_jscd1)
        job_examples = []  # í•„ìš”ì‹œ HOPE_JSCD1ë¡œë¶€í„° ì§ì¢… ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥
        
        # ì ìˆ˜ ê³„ì‚°
        score, _ = self.llm_scorer.score("ìê²©ì¦", job_name, job_examples, text)
        
        # scoreë§Œ ë°˜í™˜ (ê·¸ë˜í”„ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜)
        return {
            "JHNT_CTN": jhnt_ctn,
            "certification_score": score  # ê·¸ë˜í”„: certification_score
        }
    
    def _preprocess_certification(self, data):
        """ìê²©ì¦ íŠ¹í™” ì „ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬)"""
        if not isinstance(data, list):
            data = [data]
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° ë ˆì½”ë“œ ì²˜ë¦¬
        max_workers = min(len(data), 10)  # ìµœëŒ€ 10ê°œ ìŠ¤ë ˆë“œ
        rows = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self._process_single_certification, item): item for item in data}
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result is not None:
                        rows.append(result)
                except Exception as e:
                    item = futures[future]
                    jhnt_ctn = item.get("JHNT_CTN", "unknown")
                    print(f"âš ï¸ ìê²©ì¦ ì²˜ë¦¬ ì˜¤ë¥˜ (JHNT_CTN: {jhnt_ctn}): {e}")
                    rows.append({
                        "JHNT_CTN": jhnt_ctn,
                        "certification_score": None
                    })
        
        # DataFrame ìƒì„± ì „ì— Logger ê°ì²´ í™•ì¸ ë° ì œê±°
        import logging
        cleaned_rows = []
        for row_idx, row in enumerate(rows):
            cleaned_row = {}
            for key, value in row.items():
                # Logger ê°ì²´ì¸ì§€ í™•ì¸
                if isinstance(value, logging.Logger) or 'Logger' in str(type(value)):
                    print(f"âš ï¸ [ìê²©ì¦ ì „ì²˜ë¦¬] {row_idx}ë²ˆì§¸ í–‰ì˜ ë”•ì…”ë„ˆë¦¬ í‚¤ '{key}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(value).__name__})")
                    cleaned_row[key] = np.nan
                else:
                    cleaned_row[key] = value
            cleaned_rows.append(cleaned_row)
        
        return pd.DataFrame(cleaned_rows)


    def load_and_preprocess_data(self, data_file, json_name=None, limit_data=False, limit_size=5000):
        """
        ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            data_file (str): ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            sheet_name (str): ì—‘ì…€ ì‹œíŠ¸ëª… (Excel íŒŒì¼ìš©)
            json_name (str): JSON ë°ì´í„° íƒ€ì… ('ì´ë ¥ì„œ', 'ìê¸°ì†Œê°œì„œ', 'ì§ì—…í›ˆë ¨', 'ìê²©ì¦')
            limit_data (bool): í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ë°ì´í„° ì œí•œ ì—¬ë¶€
            limit_size (int): ì œí•œí•  ë°ì´í„° í¬ê¸°
        
        Returns:
            pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        """
        # ë°ì´í„° ë¡œë“œ
        if data_file.endswith('.csv'):
            data = pd.read_csv(data_file)
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ê²½ìš° CSV íŒŒì¼ë„ ì œí•œ
            if limit_data and len(data) > limit_size:
                original_count = len(data)
                data = data.head(limit_size)
                print(f"ğŸ“Š CSV ë°ì´í„° ì œí•œ: {len(data)}ê°œ í–‰ ì‚¬ìš© (ì „ì²´ {original_count}ê°œ ì¤‘ ì• {limit_size}ê°œ)")
            data_processed = self.basic_preprocessing(data)
        elif data_file.endswith(('.xlsx', '.xls')):
            data = pd.read_excel(data_file, sheet_name=self.sheet_name)
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ê²½ìš° Excel íŒŒì¼ë„ ì œí•œ
            if limit_data and len(data) > limit_size:
                original_count = len(data)
                data = data.head(limit_size)
                print(f"ğŸ“Š Excel ë°ì´í„° ì œí•œ: {len(data)}ê°œ í–‰ ì‚¬ìš© (ì „ì²´ {original_count}ê°œ ì¤‘ ì• {limit_size}ê°œ)")
            data_processed = self.basic_preprocessing(data)
        elif data_file.endswith('.json'):
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # JSON íŒŒì¼ì˜ ê²½ìš° json_nameì„ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì‚¬ìš©
                data_processed = self.nlp_preprocessing(data, json_name=json_name, limit_data=limit_data, limit_size=limit_size)
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV, Excel ë˜ëŠ” JSON íŒŒì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        return data_processed


    def get_merged_df(self, file_list, limit_data=False, limit_size=5000):
        """
        íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ê° íŒŒì¼ì„ load_and_preprocess_dataë¡œ ì½ê³  self.df_listì— append,
        ì´í›„ JHNT_MBN ë˜ëŠ” JHNT_CTN ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì¡°ì¸í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
        
        ì²« ë²ˆì§¸ íŒŒì¼(CSV)ì€ ìˆœì°¨ ì²˜ë¦¬í•˜ê³ , ë‚˜ë¨¸ì§€ 4ê°œ JSON íŒŒì¼ì€ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        Args:
            file_list (list): íŒŒì¼ëª…(str) ë¦¬ìŠ¤íŠ¸
            limit_data (bool): í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ë°ì´í„° ì œí•œ ì—¬ë¶€
            limit_size (int): ì œí•œí•  ë°ì´í„° í¬ê¸°
 
        Returns:
            pd.DataFrame: JHNT_MBN ë˜ëŠ” JHNT_CTN ê¸°ì¤€ìœ¼ë¡œ ì¡°ì¸ëœ ë°ì´í„°í”„ë ˆì„ -> repeat ì²˜ë¦¬ í•„ìš”
        """
        self.df_list = []
        result = None
        
        # ì²« ë²ˆì§¸ íŒŒì¼(ì •í˜• ë°ì´í„° CSV) ë¨¼ì € ì²˜ë¦¬ - HOPE_JSCD1(í¬ë§ ì§ì¢… ì½”ë“œ) ì •ë³´ ì €ì¥
        if file_list:
            # ì²« ë²ˆì§¸ íŒŒì¼ì€ ì •í˜• ë°ì´í„°ì´ë¯€ë¡œ json_name=None
            csv_start_time = time.time()
            print(f"[DEBUG] ì²« ë²ˆì§¸ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_list[0]}, íƒ€ì…: ì •í˜• ë°ì´í„° (CSV)")
            df = self.load_and_preprocess_data(file_list[0], json_name=None, limit_data=limit_data, limit_size=limit_size)
            
            csv_elapsed = time.time() - csv_start_time
            print(f"â±ï¸ ì •í˜• ë°ì´í„°(CSV) ì²˜ë¦¬ ì†Œìš” ì‹œê°„: {csv_elapsed:.2f}ì´ˆ")
            self.df_list.append(df)
            result = df
            
            print(f"[DEBUG] ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {result.shape}")
            print(f"[DEBUG] ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼: {list(result.columns)}")
            print(f"[DEBUG] ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì— JHNT_MBN ì¡´ì¬: {'JHNT_MBN' in result.columns}")
            print(f"[DEBUG] ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì— JHNT_CTN ì¡´ì¬: {'JHNT_CTN' in result.columns}")
            
            # HOPE_JSCD1 ì •ë³´ë¥¼ JHNT_MBN ê¸°ì¤€ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ì €ì¥
            if 'HOPE_JSCD1' in df.columns and 'JHNT_MBN' in df.columns:
                self.hope_jscd1_map = df.set_index('JHNT_MBN')['HOPE_JSCD1'].to_dict()
                print(f"[DEBUG] HOPE_JSCD1 ë§¤í•‘ ìƒì„± ì™„ë£Œ: {len(self.hope_jscd1_map)}ê°œ")
            else:
                print(f"[DEBUG] ê²½ê³ : HOPE_JSCD1 ë˜ëŠ” JHNT_MBNì´ ì—†ì–´ ë§¤í•‘ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë‚˜ë¨¸ì§€ 4ê°œ íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        json_files = []
        for idx, file in enumerate(file_list[1:], start=0):
            if idx >= len(self.json_names):
                raise IndexError(f"JSON íŒŒì¼ ìˆ˜({len(file_list)-1})ê°€ json_names ê¸¸ì´({len(self.json_names)})ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. file: {file}")
            current_json_name = self.json_names[idx]
            json_files.append((file, current_json_name, idx))
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ 4ê°œ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬
        processed_dfs = {}
        max_workers = min(len(json_files), 4)  # ìµœëŒ€ 4ê°œ ìŠ¤ë ˆë“œ (4ê°œ íŒŒì¼)
        
        def process_json_file(file_info):
            """ë‹¨ì¼ JSON íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
            file, json_name, idx = file_info
            try:
                file_start_time = time.time()
                print(f"[DEBUG] {idx+1}ë²ˆì§¸ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file}, íƒ€ì…: {json_name}")
                df = self.load_and_preprocess_data(file, json_name=json_name, limit_data=limit_data, limit_size=limit_size)
                file_elapsed = time.time() - file_start_time
                print(f"[DEBUG] {json_name} ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {df.shape}")
                print(f"[DEBUG] {json_name} ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼: {list(df.columns)}")
                print(f"â±ï¸ {json_name} ì²˜ë¦¬ ì†Œìš” ì‹œê°„: {file_elapsed:.2f}ì´ˆ")
                return (json_name, df, idx, file_elapsed)
            except Exception as e:
                print(f"âš ï¸ {json_name} íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                raise
        
        json_file_times = {}
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(process_json_file, file_info): file_info for file_info in json_files}
            
            for future in as_completed(futures):
                try:
                    json_name, df, idx, file_elapsed = future.result()
                    processed_dfs[idx] = (json_name, df)
                    json_file_times[json_name] = file_elapsed
                except Exception as e:
                    file_info = futures[future]
                    print(f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_info[0]}, ì˜¤ë¥˜: {e}")
                    raise
        
        # JSON íŒŒì¼ ì²˜ë¦¬ ì‹œê°„ ìš”ì•½ ì¶œë ¥
        if json_file_times:
            print("\n" + "="*60)
            print("â±ï¸ JSON íŒŒì¼ë³„ ì²˜ë¦¬ ì‹œê°„ ìš”ì•½")
            print("="*60)
            total_json_time = sum(json_file_times.values())
            for json_name, elapsed in sorted(json_file_times.items(), key=lambda x: x[1], reverse=True):
                percentage = (elapsed / total_json_time * 100) if total_json_time > 0 else 0
                print(f"  {json_name:15s}: {elapsed:7.2f}ì´ˆ ({percentage:5.1f}%)")
            print(f"  {'ì „ì²´':15s}: {total_json_time:7.2f}ì´ˆ (100.0%)")
            print("="*60)
        
        # ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ë“¤ì„ ìˆœì„œëŒ€ë¡œ ë³‘í•©
        merge_start_time = time.time()
        for idx in sorted(processed_dfs.keys()):
            json_name, df = processed_dfs[idx]
            self.df_list.append(df)
            
            # ì§ì—…í›ˆë ¨ê³¼ ìê²©ì¦ì€ JHNT_CTN ê¸°ì¤€ìœ¼ë¡œ merge
            if json_name in ['ì§ì—…í›ˆë ¨', 'ìê²©ì¦']:
                merge_key = "JHNT_CTN"
            else:
                merge_key = "JHNT_MBN"
            
            print(f"[DEBUG] ë³‘í•© í‚¤: {merge_key}")
            print(f"[DEBUG] resultì— {merge_key} ì¡´ì¬: {merge_key in result.columns}")
            print(f"[DEBUG] {json_name}ì— {merge_key} ì¡´ì¬: {merge_key in df.columns}")
            
            # ë³‘í•© í‚¤ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if merge_key not in result.columns:
                print(f"[DEBUG] ERROR: result ì»¬ëŸ¼ ëª©ë¡: {list(result.columns)}")
                raise KeyError(f"ë³‘í•© í‚¤ '{merge_key}'ê°€ ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(result.columns)}")
            if merge_key not in df.columns:
                print(f"[DEBUG] ERROR: {json_name} ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
                raise KeyError(f"ë³‘í•© í‚¤ '{merge_key}'ê°€ {json_name} ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤. íŒŒì¼: {file_list[idx+1]}, ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
            
            print(f"[DEBUG] ë³‘í•© ì „ result í¬ê¸°: {result.shape}, {json_name} í¬ê¸°: {df.shape}")
            
            # ë³‘í•© ì „ì— Logger ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸
            import logging
            for col in df.columns:
                if df[col].dtype == 'object' and len(df) > 0:
                    non_null_values = df[col].dropna()
                    if len(non_null_values) > 0:
                        first_val = non_null_values.iloc[0]
                        if isinstance(first_val, logging.Logger) or 'Logger' in str(type(first_val)):
                            print(f"âš ï¸ [ë³‘í•© ì „] {json_name}ì˜ ì»¬ëŸ¼ '{col}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(first_val).__name__})")
                            # Logger ê°ì²´ë¥¼ NaNìœ¼ë¡œ ëŒ€ì²´
                            df[col] = df[col].apply(lambda x: np.nan if (isinstance(x, logging.Logger) or 'Logger' in str(type(x))) else x)
            
            # í…Œì´ë¸”ì„ ê¸°ì¤€ìœ¼ë¡œ inner join
            result = result.merge(df, on=merge_key, how="inner", suffixes=('', f'_df{idx+1}'))
            print(f"[DEBUG] ë³‘í•© í›„ result í¬ê¸°: {result.shape}")
            
            # ë³‘í•© í›„ì— Logger ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸
            for col in result.columns:
                if result[col].dtype == 'object' and len(result) > 0:
                    non_null_values = result[col].dropna()
                    if len(non_null_values) > 0:
                        first_val = non_null_values.iloc[0]
                        if isinstance(first_val, logging.Logger) or 'Logger' in str(type(first_val)):
                            print(f"âš ï¸ [ë³‘í•© í›„] resultì˜ ì»¬ëŸ¼ '{col}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(first_val).__name__})")
        
        merge_elapsed = time.time() - merge_start_time
        print(f"â±ï¸ ë°ì´í„° ë³‘í•© ì†Œìš” ì‹œê°„: {merge_elapsed:.2f}ì´ˆ")
        
        # ë³‘í•© í›„ ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” rowì˜ ë¹„ìœ¨ í™•ì¸ (inner joinìœ¼ë¡œ ì¸í•œ ê²°ì¸¡ì¹˜ í™•ì¸)
        total_rows = len(result)
        rows_with_missing = result.isnull().any(axis=1).sum()
        missing_ratio = (rows_with_missing / total_rows * 100) if total_rows > 0 else 0
        print(f"\nğŸ“Š ë³‘í•© í›„ ê²°ì¸¡ì¹˜ ë¶„ì„:")
        print(f"   ì „ì²´ í–‰ ìˆ˜: {total_rows}ê°œ")
        print(f"   ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ìˆ˜: {rows_with_missing}ê°œ")
        print(f"   ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ë¹„ìœ¨: {missing_ratio:.2f}%")
        
        # ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ë„ ì¶œë ¥
        missing_by_column = result.isnull().sum()
        columns_with_missing = missing_by_column[missing_by_column > 0]
        if len(columns_with_missing) > 0:
            print(f"\nğŸ“Š ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ í˜„í™©:")
            for col, missing_count in columns_with_missing.items():
                missing_pct = (missing_count / total_rows * 100) if total_rows > 0 else 0
                print(f"   {col}: {missing_count}ê°œ ({missing_pct:.2f}%)")
        
        # ê²°ì¸¡ì¹˜ ë³´ê°„ (í‰ê· ê°’ ë˜ëŠ” ìµœë¹ˆê°’ìœ¼ë¡œ)
        from . import utils
        print(f"\nğŸ“Š ê²°ì¸¡ì¹˜ ë³´ê°„ ì‹œì‘...")
        result = utils.impute_missing_values(result)
        print(f"âœ… ê²°ì¸¡ì¹˜ ë³´ê°„ ì™„ë£Œ")
        
        # Logger ê°ì²´ê°€ ë°ì´í„°í”„ë ˆì„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬
        import logging
        logger_columns = []
        for col in result.columns:
            if result[col].dtype == 'object' and len(result) > 0:
                non_null_values = result[col].dropna()
                if len(non_null_values) > 0:
                    first_val = non_null_values.iloc[0]
                    # Logger ê°ì²´ì¸ì§€ í™•ì¸
                    if isinstance(first_val, logging.Logger) or 'Logger' in str(type(first_val)):
                        logger_columns.append((col, type(first_val).__name__))
                        print(f"âš ï¸ [ì „ì²˜ë¦¬] ê²½ê³ : ì»¬ëŸ¼ '{col}'ì— Logger ê°ì²´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! (íƒ€ì…: {type(first_val).__name__})")
        
        if logger_columns:
            print(f"\nâŒ [ì „ì²˜ë¦¬] ì˜¤ë¥˜: ë‹¤ìŒ ì»¬ëŸ¼ì— Logger ê°ì²´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
            for col, col_type in logger_columns:
                print(f"   - {col} (íƒ€ì…: {col_type})")
            print(f"ì´ ì»¬ëŸ¼ë“¤ì€ ë°ì´í„° ì •ë¦¬ ê³¼ì •ì—ì„œ ì œê±°ë©ë‹ˆë‹¤.")
        
        return result