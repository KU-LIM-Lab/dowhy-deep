"""
DoWhy ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ
- Basic ì „ì²˜ë¦¬: ê¸°ë³¸ì ì¸ ë°ì´í„° ì •ì œ ë° ë³€í™˜
- NLP ì „ì²˜ë¦¬: í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ

ì‚¬ìš© ì˜ˆì‹œ:
    # JSON íŒŒì¼ë“¤ì— ëŒ€í•´ ê°ê° ë‹¤ë¥¸ ì „ì²˜ë¦¬ ì ìš©
    preprocessor = Preprocessor([])
    
    # ë°©ë²• 1: ê°œë³„ íŒŒì¼ ì²˜ë¦¬
    resume_data = preprocessor.load_and_preprocess_data('resume.json', json_name='ì´ë ¥ì„œ')
    cover_letter_data = preprocessor.load_and_preprocess_data('cover_letter.json', json_name='ìê¸°ì†Œê°œì„œ')
    
    # Excel íŒŒì¼ ì²˜ë¦¬
    excel_data = preprocessor.load_and_preprocess_data('data.xlsx', sheet_name='Sheet1')
    
    # ë°©ë²• 2: ì—¬ëŸ¬ íŒŒì¼ì„ í•œë²ˆì— ì²˜ë¦¬
    file_list = ['resume.json', 'cover_letter.json', 'training.json', 'certification.json']
    json_names = ['ì´ë ¥ì„œ', 'ìê¸°ì†Œê°œì„œ', 'ì§ì—…í›ˆë ¨', 'ìê²©ì¦']
    merged_df = preprocessor.get_merged_df(file_list, json_names=json_names)
"""

import pandas as pd
import numpy as np
import json
import re
import os
from datetime import datetime
from typing import Optional, List, Dict, Any
from collections import Counter
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import partial
import time
import asyncio
import aiohttp
from tqdm import tqdm
from tqdm.asyncio import tqdm as atqdm

from .llm_reference import (
    JSON_NAMES, RESUME_SECTIONS, SUPPORTED_SECTIONS, 
    DEFAULT_MAX_COVER_LEN, DEFAULT_COVER_EXCEED_RATIO, DEFAULT_DATE_FORMAT
)
from .llm_scorer import LLMScorer



class Preprocessor:
    def __init__(self, df_list, job_category_file="KSIC", max_concurrent_requests=None, top_job_categories=5):
        self.json_names = JSON_NAMES
        self.sheet_name = 'êµ¬ì§ì¸ì¦ ê´€ë ¨ ë°ì´í„°'
        self.df_list = []
        self.variable_mapping = self.load_variable_mapping()
        self.llm_scorer = LLMScorer()
        self.hope_jscd1_map = {}  # JHNT_MBN -> HOPE_JSCD1 ë§¤í•‘ ì €ì¥
        self.job_category_file = job_category_file  # ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ëª… (KECO, KSCO, KSIC)
        self.job_code_to_name = self.load_job_mapping()  # ì†Œë¶„ë¥˜ì½”ë“œ -> ì†Œë¶„ë¥˜ëª… ë§¤í•‘
        self.top_job_categories = top_job_categories  # ìƒìœ„ ì§ì¢… ì†Œë¶„ë¥˜ ê°œìˆ˜ (-1ì´ë©´ ì „ì²´ ì‚¬ìš©)
        
        # ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ ì„¤ì • (OLLAMA_NUM_PARALLEL í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
        if max_concurrent_requests is None:
            max_concurrent_requests = int(os.getenv("OLLAMA_NUM_PARALLEL", "32"))
        self.max_concurrent_requests = max_concurrent_requests
        self.semaphore = asyncio.Semaphore(max_concurrent_requests)
        print(f"ğŸ”§ Ollama ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ: {max_concurrent_requests}ê°œ")

    def load_variable_mapping(self):
        # variable_mapping.jsonì€ data í´ë”ì— ìˆìŒ
        # __file__ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ê³„ì‚°: src/preprocess.py -> laborlab_2/ -> data/
        preprocess_file = Path(__file__)  # src/preprocess.py
        laborlab_dir = preprocess_file.parent.parent  # laborlab_2/
        variable_mapping_path = laborlab_dir / "data" / "variable_mapping.json"
        
        with open(variable_mapping_path, encoding='utf-8') as f:
            variable_mapping = json.load(f)
        return variable_mapping
    
    def load_job_mapping(self):
        """job_subcategories_XXXX.csvë¥¼ ë¡œë“œí•˜ì—¬ ì†Œë¶„ë¥˜ì½”ë“œ -> ì†Œë¶„ë¥˜ëª… ë§¤í•‘ ìƒì„±"""
        try:
            # __file__ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ê³„ì‚°: src/preprocess.py -> laborlab_2/ -> data/
            preprocess_file = Path(__file__)  # src/preprocess.py
            laborlab_dir = preprocess_file.parent.parent  # laborlab_2/
            
            # job_category_fileì— ë”°ë¼ íŒŒì¼ëª… ê²°ì • (KECO, KSCO, KSIC)
            job_category_file = self.job_category_file.upper()
            if job_category_file not in ["KECO", "KSCO", "KSIC"]:
                print(f"âš ï¸ ì˜ëª»ëœ ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ëª…: {job_category_file}. ê¸°ë³¸ê°’ KSIC ì‚¬ìš©")
                job_category_file = "KSIC"
            
            job_mapping_path = laborlab_dir / "data" / f"job_subcategories_{job_category_file}.csv"
            
            if not job_mapping_path.exists():
                print(f"âš ï¸ ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_mapping_path}")
                print(f"   ê¸°ë³¸ê°’ job_subcategories_KSIC.csv ì‚¬ìš© ì‹œë„")
                job_mapping_path = laborlab_dir / "data" / "job_subcategories_KSIC.csv"
            
            job_df = pd.read_csv(job_mapping_path, encoding='utf-8')
            print(f"âœ… ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {job_mapping_path.name} ({len(job_df)}ê°œ ì§ì¢…)")
            
            # ì†Œë¶„ë¥˜ì½”ë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            job_mapping = dict(zip(job_df['ì†Œë¶„ë¥˜ì½”ë“œ'].astype(str).str.zfill(3), job_df['ì†Œë¶„ë¥˜ëª…']))
            return job_mapping
        except Exception as e:
            print(f"âŒ ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_job_name_from_code(self, code):
        """HOPE_JSCD1 ì½”ë“œë¥¼ ì§ì¢…ëª…ìœ¼ë¡œ ë³€í™˜"""
        if not code:
            return "ë¯¸ìƒ"
        # ì½”ë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì•ì— 0ì„ ì±„ì›Œì„œ 3ìë¦¬ë¡œ ë§Œë“¤ê¸°
        code_str = str(code).zfill(3)
        return self.job_code_to_name.get(code_str, f"ì§ì¢…ì½”ë“œ {code}")

    @staticmethod
    def get_data_info(df):
        """ë°ì´í„° ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        info = {
            'shape': df.shape,
            'columns': list(df.columns),
            'missing_values': df.isnull().sum().to_dict(),
            'dtypes': df.dtypes.to_dict()
        }
        return info

    def basic_preprocessing(self, df):
        """
        ê¸°ë³¸ì ì¸ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        
        Returns:
            pd.DataFrame: ê¸°ë³¸ ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        """
        # ë””ë²„ê¹…: ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼ í™•ì¸
        print(f"[DEBUG] basic_preprocessing ì‹œì‘ - ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
        print(f"[DEBUG] ì›ë³¸ ë°ì´í„°ì— JHNT_CTN ì¡´ì¬: {'JHNT_CTN' in df.columns}")
        print(f"[DEBUG] ì›ë³¸ ë°ì´í„°ì— JHNT_MBN ì¡´ì¬: {'JHNT_MBN' in df.columns}")
        

        # ë³‘í•©ì— í•„ìš”í•œ í‚¤ ì»¬ëŸ¼ì€ í•­ìƒ ìœ ì§€
        merge_keys = ["JHNT_CTN", "JHNT_MBN"]
        existing_merge_keys = [key for key in merge_keys if key in df.columns]
        print(f"[DEBUG] ë°œê²¬ëœ ë³‘í•© í‚¤: {existing_merge_keys}")
        
        # variable_mapping.jsonì˜ structured_data í‚¤ë§Œ ì‚¬ìš©
        structured_keys = set(self.variable_mapping.get("structured_data", {}).keys())
        
        # ì›ë³¸ ë°ì´í„°ì—ì„œ í•´ë‹¹ ë³€ìˆ˜ë“¤ë§Œ í•„í„°ë§ (ì¡´ì¬í•˜ëŠ” ë³€ìˆ˜ë§Œ)
        available_vars = list(structured_keys & set(df.columns))
        missing_vars = list(structured_keys - set(df.columns))
        
        if missing_vars:
            print(f"ë‹¤ìŒ ë³€ìˆ˜ë“¤ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤: {missing_vars}")
        
        # ë³‘í•© í‚¤ì™€ í•„í„°ë§ëœ ë³€ìˆ˜ë“¤ì„ í•©ì¹¨ (ì¤‘ë³µ ì œê±°)
        final_vars = list(set(available_vars + existing_merge_keys))
        print(f"[DEBUG] ìµœì¢… ì»¬ëŸ¼ ìˆ˜: {len(final_vars)}, JHNT_MBN í¬í•¨ ì—¬ë¶€: {'JHNT_MBN' in final_vars}")
        df = df[final_vars]

        # BFR_OCTR_YN ì œê±°, BFR_OCTR_CTë§Œ ìœ ì§€
        if "BFR_OCTR_YN" in df.columns and "BFR_OCTR_CT" in df.columns:
            df = df.drop(columns=["BFR_OCTR_YN"])
            print(f"[DEBUG] BFR_OCTR_YN ì œê±° í›„ JHNT_MBN ì¡´ì¬: {'JHNT_MBN' in df.columns}")

        # 9ê°œ ì˜ˆ/ì•„ë‹ˆì˜¤ ë³€ìˆ˜ â†’ í•©ì³ì„œ ìƒˆë¡œìš´ ìˆœì„œí˜• ë²”ì£¼ ë³€ìˆ˜ ìƒì„±
        agree_vars = [
            "EMAIL_RCYN", "SAEIL_CNTC_AGRE_YN", "SHRS_IDIF_AOFR_YN", "SULC_IDIF_AOFR_YN",
            "IDIF_IQRY_AGRE_YN", "SMS_RCYN", "EMAIL_OTPB_YN", "MPNO_OTPB_YN", "EMAIL_RCYN"
        ]

        # ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ì‚¬ìš©
        agree_vars = [col for col in agree_vars if col in df.columns]

        if agree_vars:
            agree_count = (df[agree_vars] == "ì˜ˆ").sum(axis=1)
            df["AGREE_LEVEL"] = agree_count.apply(lambda x: "í•˜" if x <= 3 else ("ì¤‘" if x <= 6 else "ìƒ"))
            df = df.drop(columns=agree_vars)
            print(f"[DEBUG] agree_vars ì œê±° í›„ JHNT_MBN ì¡´ì¬: {'JHNT_MBN' in df.columns}")

        # HOPE_JSCD1_NAME ë³€ìˆ˜ ì¶”ê°€ (HOPE_JSCD1 ì½”ë“œë¥¼ ì†Œë¶„ë¥˜ëª…ìœ¼ë¡œ ë³€í™˜)
        if "HOPE_JSCD1" in df.columns:
            df["HOPE_JSCD1_NAME"] = df["HOPE_JSCD1"].apply(lambda code: self.get_job_name_from_code(code))
            print(f"[DEBUG] HOPE_JSCD1_NAME ë³€ìˆ˜ ì¶”ê°€ ì™„ë£Œ: {df['HOPE_JSCD1_NAME'].nunique()}ê°œ ê³ ìœ ê°’")

        print(f"[DEBUG] basic_preprocessing ì™„ë£Œ - ìµœì¢… ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}, JHNT_MBN ì¡´ì¬: {'JHNT_MBN' in df.columns}")
        
        # JHNT_MBNê³¼ JHNT_CTNì„ ë¬¸ìì—´ë¡œ í†µì¼ (13ìë¦¬ 0íŒ¨ë”©)
        if 'JHNT_MBN' in df.columns:
            df['JHNT_MBN'] = df['JHNT_MBN'].astype(str).str.zfill(13)
        if 'JHNT_CTN' in df.columns:
            df['JHNT_CTN'] = df['JHNT_CTN'].astype(str).str.zfill(13)
        
        return df

    async def nlp_preprocessing(self, data, json_name=None, limit_data=False, limit_size=5000):
        """
        NLP ê¸°ë°˜ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ (ë¹„ë™ê¸°)
        
        Args:
            data: json íŒŒì¼ (ìê¸°ì†Œê°œì„œ, ì´ë ¥ì„œ, ì§ì—…í›ˆë ¨, ìê²©ì¦)
            json_name (str): JSON ë°ì´í„° íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ì „ì²˜ë¦¬ ì ìš©
            limit_data (bool): í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ë°ì´í„° ì œí•œ ì—¬ë¶€
            limit_size (int): ì œí•œí•  ë°ì´í„° í¬ê¸°
        Returns:
            pd.DataFrame: NLP ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        """
        # JSON íŒŒì¼ì€ ë°°ì—´ í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë¡œë“œë¨
        if isinstance(data, list):
            if limit_data and len(data) > limit_size:
                original_count = len(data)
                data = data[:limit_size]
                print(f"ğŸ“Š {json_name} ë°ì´í„° ì œí•œ: {len(data)}ê°œ ë ˆì½”ë“œ ì‚¬ìš© (ì „ì²´ {original_count}ê°œ ì¤‘ ì• {limit_size}ê°œ)")
            else:
                # limit_dataê°€ Falseì´ë©´ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬
                print(f"ğŸ“Š {json_name} ì „ì²´ ë°ì´í„° ì²˜ë¦¬: {len(data)}ê°œ ë ˆì½”ë“œ")
        else:
            # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° (ë‹¨ì¼ ê°ì²´)ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë‚˜ì¤‘ì— _preprocess_* í•¨ìˆ˜ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ë¨)
            print(f"âš ï¸ {json_name} ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤. ë‹¨ì¼ ê°ì²´ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        
        # JSON ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ íŠ¹í™”ëœ ì „ì²˜ë¦¬ (ë¹„ë™ê¸°)
        if json_name == 'ì´ë ¥ì„œ':
            df_processed = await self._preprocess_resume(data)
        elif json_name == 'ìê¸°ì†Œê°œì„œ':
            df_processed = await self._preprocess_cover_letter(data)
        elif json_name == 'ì§ì—…í›ˆë ¨':
            df_processed = await self._preprocess_training(data)
        elif json_name == 'ìê²©ì¦':
            df_processed = await self._preprocess_certification(data)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” json íŒŒì¼ì…ë‹ˆë‹¤. {json_name}")
        
        return df_processed


    async def _process_single_resume(self, item, session: aiohttp.ClientSession):
        """ë‹¨ì¼ ì´ë ¥ì„œ ë ˆì½”ë“œ ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
        # SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ ë³€í™˜
        seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "")
        if not seek_id:
            return None
        
        # BASIC_RESUME_YN == "Y"ì¸ resume ì°¾ê¸° (CONTENTS ë°°ì—´ì—ì„œ)
        contents = item.get("CONTENTS", [])
        basic_resume = None
        for content in contents:
            if str(content.get("BASIC_RESUME_YN", "")).upper() == "Y":
                basic_resume = content
                break
        
        # ê¸°ë³¸ ì´ë ¥ì„œê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if basic_resume is None:
            return {
                "JHNT_MBN": seek_id,
                "resume_score": None,
                "items_num": 0
            }
        
        # RESUME_CONTENTS ê°€ì ¸ì˜¤ê¸°
        items = basic_resume.get("RESUME_CONTENTS", [])
        items_num = len(items)
        
        # variable_mappingì—ì„œ resume ì„¹ì…˜ ê°€ì ¸ì˜¤ê¸°
        resume_mapping = self.variable_mapping.get("resume", {})
        
        # ITEMSë¥¼ í¬ë§¤íŒ…
        formatting_sentence = ""
        for item_data in items:
            for key, value in item_data.items():
                # variable_mappingì—ì„œ í•œê¸€ ë³€ìˆ˜ëª… ì°¾ê¸°
                if key in resume_mapping:
                    korean_key = resume_mapping[key].get("ë³€ìˆ˜ëª…", key)
                else:
                    korean_key = key
                
                # valueê°€ Noneì´ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                value_str = str(value) if value is not None else ""
                formatting_sentence += f"{korean_key}: {value_str}\n"
            formatting_sentence += "\n"
        
        # í¬ë§¤íŒ…ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not formatting_sentence.strip():
            formatting_sentence = "ì •ë³´ ì—†ìŒ"
        
        # HOPE_JSCD1 ì •ë³´ ê°€ì ¸ì™€ì„œ ì§ì¢…ëª…ìœ¼ë¡œ ë³€í™˜
        hope_jscd1 = self.hope_jscd1_map.get(seek_id, "")
        job_name = self.get_job_name_from_code(hope_jscd1)
        job_examples = []  # í•„ìš”ì‹œ HOPE_JSCD1ë¡œë¶€í„° ì§ì¢… ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥
        
        # LLM scorerì— ì „ë‹¬í•˜ì—¬ ì ìˆ˜ ê³„ì‚° (ë¹„ë™ê¸°)
        score, _ = await self.llm_scorer.score_async("ì´ë ¥ì„œ", job_name, job_examples, formatting_sentence, session)
        
        return {
            "JHNT_MBN": str(seek_id),  # ë¬¸ìì—´ë¡œ ë³€í™˜
            "resume_score": score,
            "items_num": items_num
        }
    
    async def _preprocess_resume(self, data):
        """ì´ë ¥ì„œ íŠ¹í™” ì „ì²˜ë¦¬ (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬)"""
        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬ (JSON íŒŒì¼ì´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¼ ìˆ˜ ìˆìŒ)
        if not isinstance(data, list):
            data = [data]
        
        # ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° ë ˆì½”ë“œ ì²˜ë¦¬
        rows = []
        import logging
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for item in data:
                task = self._process_single_resume(item, session)
                tasks.append(task)
            
            results = await atqdm.gather(*tasks, desc="ì´ë ¥ì„œ ì „ì²˜ë¦¬", unit="ê±´")
            
            for idx, result in enumerate(results):
                try:
                    if result is not None:
                        rows.append(result)
                except Exception as e:
                    item = data[idx]
                    seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "unknown")
                    print(f"âš ï¸ ì´ë ¥ì„œ ì²˜ë¦¬ ì˜¤ë¥˜ (JHNT_MBN: {seek_id}): {e}")
                    rows.append({
                        "JHNT_MBN": str(seek_id),  # ë¬¸ìì—´ë¡œ ë³€í™˜
                        "resume_score": None,
                        "items_num": 0
                    })
        
        # DataFrame ìƒì„± ì „ì— Logger ê°ì²´ í™•ì¸ ë° ì œê±°
        cleaned_rows = []
        for row_idx, row in enumerate(rows):
            cleaned_row = {}
            for key, value in row.items():
                # Logger ê°ì²´ì¸ì§€ í™•ì¸
                if isinstance(value, logging.Logger) or 'Logger' in str(type(value)):
                    print(f"âš ï¸ [ì´ë ¥ì„œ ì „ì²˜ë¦¬] {row_idx}ë²ˆì§¸ í–‰ì˜ ë”•ì…”ë„ˆë¦¬ í‚¤ '{key}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(value).__name__})")
                    cleaned_row[key] = np.nan
                else:
                    cleaned_row[key] = value
            cleaned_rows.append(cleaned_row)
        
        df = pd.DataFrame(cleaned_rows)
        
        # SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ rename (ìˆëŠ” ê²½ìš°)
        if 'SEEK_CUST_NO' in df.columns and 'JHNT_MBN' not in df.columns:
            df = df.rename(columns={'SEEK_CUST_NO': 'JHNT_MBN'})
            print(f"âœ… ì´ë ¥ì„œ ë°ì´í„°: SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ ë³€ê²½")
        elif 'SEEK_CUST_NO' in df.columns and 'JHNT_MBN' in df.columns:
            # ë‘˜ ë‹¤ ìˆìœ¼ë©´ SEEK_CUST_NOì˜ ê°’ìœ¼ë¡œ JHNT_MBNì„ ì±„ìš°ê³  SEEK_CUST_NO ì œê±°
            df['JHNT_MBN'] = df['JHNT_MBN'].fillna(df['SEEK_CUST_NO'])
            df = df.drop(columns=['SEEK_CUST_NO'])
            print(f"âœ… ì´ë ¥ì„œ ë°ì´í„°: SEEK_CUST_NO ê°’ì„ JHNT_MBNì— ë³‘í•© í›„ SEEK_CUST_NO ì œê±°")
        
        # JHNT_MBNì„ ë¬¸ìì—´ë¡œ í†µì¼
        if 'JHNT_MBN' in df.columns:
            df['JHNT_MBN'] = df['JHNT_MBN'].astype(str)
        
        return df


    async def _process_single_cover_letter(self, item, session: aiohttp.ClientSession):
        """ë‹¨ì¼ ìê¸°ì†Œê°œì„œ ë ˆì½”ë“œ ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
        # SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ ë³€í™˜
        seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "")
        if not seek_id:
            return None
                
        # ìê¸°ì†Œê°œì„œ ë°ì´í„° ì¶”ì¶œ (BASS_SFID_YN == "Y"ì¸ í•­ëª©ë§Œ)
        texts = []
        items = []
        for c in item.get("COVERLETTERS", []):
            if str(c.get("BASS_SFID_YN", "")).upper() == "Y":
                items = c.get("ITEMS", []) or []
                for it in items:
                    t = it.get("SELF_INTRO_CONT", "")
                    if t:
                        texts.append(t.strip())
                break
        
        full_text = "\n\n".join(texts) if texts else "ì •ë³´ ì—†ìŒ"
        
        # HOPE_JSCD1 ì •ë³´ ê°€ì ¸ì™€ì„œ ì§ì¢…ëª…ìœ¼ë¡œ ë³€í™˜
        hope_jscd1 = self.hope_jscd1_map.get(seek_id, "")
        job_name = self.get_job_name_from_code(hope_jscd1)
        job_examples = []  # í•„ìš”ì‹œ HOPE_JSCD1ë¡œë¶€í„° ì§ì¢… ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥
        
        # ì ìˆ˜ ê³„ì‚°ê³¼ ì˜¤íƒˆì ìˆ˜ ê³„ì‚°ì„ ë¹„ë™ê¸°ë¡œ ë³‘ë ¬ ì‹¤í–‰
        score_task = self.llm_scorer.score_async("ìê¸°ì†Œê°œì„œ", job_name, job_examples, full_text, session)
        typo_task = self.llm_scorer.count_typos_async(full_text, session)
        score, _ = await score_task
        typo_count = await typo_task
        
        # scoreì™€ ì˜¤íƒˆì ìˆ˜ë§Œ ë°˜í™˜ (ê·¸ë˜í”„ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜)
        return {
            "JHNT_MBN": str(seek_id),  # ë¬¸ìì—´ë¡œ ë³€í™˜
            "cover_letter_score": score,  # ê·¸ë˜í”„: cover_letter_score
            "cover_letter_typo_count": typo_count  # ê·¸ë˜í”„: cover_letter_typo_count
        }
    
    async def _preprocess_cover_letter(self, data):
        """ìê¸°ì†Œê°œì„œ íŠ¹í™” ì „ì²˜ë¦¬ (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬)"""
        if not isinstance(data, list):
            data = [data]
        
        # ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° ë ˆì½”ë“œ ì²˜ë¦¬
        rows = []
        import logging
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for item in data:
                task = self._process_single_cover_letter(item, session)
                tasks.append(task)
            
            results = await atqdm.gather(*tasks, desc="ìê¸°ì†Œê°œì„œ ì „ì²˜ë¦¬", unit="ê±´")
            
            for idx, result in enumerate(results):
                try:
                    if result is not None:
                        rows.append(result)
                except Exception as e:
                    item = data[idx]
                    seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "unknown")
                    print(f"âš ï¸ ìê¸°ì†Œê°œì„œ ì²˜ë¦¬ ì˜¤ë¥˜ (JHNT_MBN: {seek_id}): {e}")
                    rows.append({
                        "JHNT_MBN": str(seek_id),  # ë¬¸ìì—´ë¡œ ë³€í™˜
                        "cove_letter_score": None,
                        "cover_letter_typo_count": 0
                    })
        
        # DataFrame ìƒì„± ì „ì— Logger ê°ì²´ í™•ì¸ ë° ì œê±°
        cleaned_rows = []
        for row_idx, row in enumerate(rows):
            cleaned_row = {}
            for key, value in row.items():
                # Logger ê°ì²´ì¸ì§€ í™•ì¸
                if isinstance(value, logging.Logger) or 'Logger' in str(type(value)):
                    print(f"âš ï¸ [ìê¸°ì†Œê°œì„œ ì „ì²˜ë¦¬] {row_idx}ë²ˆì§¸ í–‰ì˜ ë”•ì…”ë„ˆë¦¬ í‚¤ '{key}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(value).__name__})")
                    cleaned_row[key] = np.nan
                else:
                    cleaned_row[key] = value
            cleaned_rows.append(cleaned_row)
        
        df = pd.DataFrame(cleaned_rows)
        
        # SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ rename (ìˆëŠ” ê²½ìš°)
        if 'SEEK_CUST_NO' in df.columns and 'JHNT_MBN' not in df.columns:
            df = df.rename(columns={'SEEK_CUST_NO': 'JHNT_MBN'})
            print(f"âœ… ìê¸°ì†Œê°œì„œ ë°ì´í„°: SEEK_CUST_NOë¥¼ JHNT_MBNìœ¼ë¡œ ë³€ê²½")
        elif 'SEEK_CUST_NO' in df.columns and 'JHNT_MBN' in df.columns:
            # ë‘˜ ë‹¤ ìˆìœ¼ë©´ SEEK_CUST_NOì˜ ê°’ìœ¼ë¡œ JHNT_MBNì„ ì±„ìš°ê³  SEEK_CUST_NO ì œê±°
            df['JHNT_MBN'] = df['JHNT_MBN'].fillna(df['SEEK_CUST_NO'])
            df = df.drop(columns=['SEEK_CUST_NO'])
            print(f"âœ… ìê¸°ì†Œê°œì„œ ë°ì´í„°: SEEK_CUST_NO ê°’ì„ JHNT_MBNì— ë³‘í•© í›„ SEEK_CUST_NO ì œê±°")
        
        # JHNT_MBNì„ ë¬¸ìì—´ë¡œ í†µì¼
        if 'JHNT_MBN' in df.columns:
            df['JHNT_MBN'] = df['JHNT_MBN'].astype(str)
        
        return df


    async def _process_single_training(self, item, session: aiohttp.ClientSession):
        """ë‹¨ì¼ ì§ì—…í›ˆë ¨ ë ˆì½”ë“œ ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
        # JHNT_CTNì„ í‚¤ë¡œ ì‚¬ìš©
        jhnt_ctn = item.get("JHNT_CTN", "")
        if not jhnt_ctn:
            return None
        
        # êµ¬ì§ì¸ì¦ ì¼ì ê°€ì ¸ì˜¤ê¸°
        jhcr_de = item.get("JHCR_DE", "")  # êµ¬ì§ì¸ì¦ ì¼ì
        
        # CONTENTSì—ì„œ í›ˆë ¨ ë°ì´í„° ì¶”ì¶œ
        trainings = item.get("CONTENTS", [])
        
        # ë‚ ì§œ íŒŒì‹± í—¬í¼ í•¨ìˆ˜ (ì—¬ëŸ¬ í˜•ì‹ ì§€ì›)
        def parse_date(date_str):
            if not date_str:
                return None
            date_str = date_str.strip()
            for fmt in ["%Y-%m-%d", "%Y%m%d"]:
                try:
                    return datetime.strptime(date_str, fmt)
                except:
                    continue
            return None
        
        # CONTENTSì—ì„œ ëª¨ë“  TRNG_ENDE ê°€ì ¸ì™€ì„œ datetime ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        training_end_dates = []
        for tr in trainings:
            trng_ende = tr.get("TRNG_ENDE", "")
            date_obj = parse_date(trng_ende)
            if date_obj:
                training_end_dates.append(date_obj)
        
        # ê²½ê³¼ì¼ ê³„ì‚°: JHCR_DE - ìµœê·¼ TRNG_ENDE (ì¼ìˆ˜ ì°¨ì´)
        elapsed_days = None
        if jhcr_de and training_end_dates:
            try:
                # êµ¬ì§ì¸ì¦ ì¼ìë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
                jhcr_date = parse_date(jhcr_de)
                # ê°€ì¥ ìµœê·¼ í›ˆë ¨ ì¢…ë£Œì¼ (ìµœëŒ€ê°’)    
                latest_end_date = max(training_end_dates)
                # ì¼ìˆ˜ ì°¨ì´ ê³„ì‚° (ë‘˜ ë‹¤ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ)
                if jhcr_date and latest_end_date:
                    elapsed_days = (jhcr_date - latest_end_date).days
                    elapsed_days = elapsed_days if elapsed_days >= 0 else None
            except:
                elapsed_days = None
        
        # í…ìŠ¤íŠ¸ í¬ë§·íŒ…: {TRNG_CRSN}: ({TRNG_BGDE} ~ {TRNG_ENDE})
        training_texts = []
        for tr in trainings:
            trng_crsn = tr.get("TRNG_CRSN", "").strip()  # í›ˆë ¨ ê³¼ì •ëª…
            trng_bgde = tr.get("TRNG_BGDE", "").strip()  # í›ˆë ¨ ì‹œì‘ì¼
            trng_ende = tr.get("TRNG_ENDE", "").strip()  # í›ˆë ¨ ì¢…ë£Œì¼
            if trng_crsn and trng_bgde and trng_ende:
                training_texts.append(f"{trng_crsn}: ({trng_bgde} ~ {trng_ende})")
        
        text = "\n".join(training_texts) if training_texts else "ì •ë³´ ì—†ìŒ"
        
        # seek_idëŠ” HOPE_JSCD1 ë§¤í•‘ì„ ìœ„í•´ ì‚¬ìš© (JHNT_MBNì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None)
        seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "")
        
        # HOPE_JSCD1 ì •ë³´ ê°€ì ¸ì™€ì„œ ì§ì¢…ëª…ìœ¼ë¡œ ë³€í™˜
        hope_jscd1 = self.hope_jscd1_map.get(seek_id, "")
        job_name = self.get_job_name_from_code(hope_jscd1)
        job_examples = []  # í•„ìš”ì‹œ HOPE_JSCD1ë¡œë¶€í„° ì§ì¢… ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥
        
        # ì ìˆ˜ ê³„ì‚° (ë¹„ë™ê¸°)
        score, why = await self.llm_scorer.score_async("ì§ì—…í›ˆë ¨", job_name, job_examples, text, session)
        
        return {
            "JHNT_CTN": str(jhnt_ctn),  # ë¬¸ìì—´ë¡œ ë³€í™˜
            "training_score": score,
            "days_last_training_to_jobseek": elapsed_days if elapsed_days is not None else None  # ê·¸ë˜í”„: days_last_training_to_jobseek
        }
    
    async def _preprocess_training(self, data):
        """ì§ì—…í›ˆë ¨ íŠ¹í™” ì „ì²˜ë¦¬ (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬)"""
        if not isinstance(data, list):
            data = [data]
        
        # ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° ë ˆì½”ë“œ ì²˜ë¦¬
        rows = []
        import logging
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for item in data:
                task = self._process_single_training(item, session)
                tasks.append(task)
            
            results = await atqdm.gather(*tasks, desc="ì§ì—…í›ˆë ¨ ì „ì²˜ë¦¬", unit="ê±´")
            
            for idx, result in enumerate(results):
                try:
                    if result is not None:
                        rows.append(result)
                except Exception as e:
                    item = data[idx]
                    jhnt_ctn = item.get("JHNT_CTN", "unknown")
                    print(f"âš ï¸ ì§ì—…í›ˆë ¨ ì²˜ë¦¬ ì˜¤ë¥˜ (JHNT_CTN: {jhnt_ctn}): {e}")
                    rows.append({
                        "JHNT_CTN": str(jhnt_ctn),  # ë¬¸ìì—´ë¡œ ë³€í™˜
                        "training_score": None,
                        "days_last_training_to_jobseek": None
                    })
        
        # DataFrame ìƒì„± ì „ì— Logger ê°ì²´ í™•ì¸ ë° ì œê±°
        cleaned_rows = []
        for row_idx, row in enumerate(rows):
            cleaned_row = {}
            for key, value in row.items():
                # Logger ê°ì²´ì¸ì§€ í™•ì¸
                if isinstance(value, logging.Logger) or 'Logger' in str(type(value)):
                    print(f"âš ï¸ [ì§ì—…í›ˆë ¨ ì „ì²˜ë¦¬] {row_idx}ë²ˆì§¸ í–‰ì˜ ë”•ì…”ë„ˆë¦¬ í‚¤ '{key}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(value).__name__})")
                    cleaned_row[key] = np.nan
                else:
                    cleaned_row[key] = value
            cleaned_rows.append(cleaned_row)
        
        return pd.DataFrame(cleaned_rows)


    async def _process_single_certification(self, item, session: aiohttp.ClientSession):
        """ë‹¨ì¼ ìê²©ì¦ ë ˆì½”ë“œ ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
        # JHNT_CTNì„ í‚¤ë¡œ ì‚¬ìš©
        jhnt_ctn = item.get("JHNT_CTN", "")
        if not jhnt_ctn:
            return None
        
        # JSONì—ì„œ ìê²©ì¦ ë°ì´í„° ì¶”ì¶œ
        licenses = item.get("LICENSES", [])
        
        # ìê²©ì¦ í¬ë§·íŒ…: ìê²©ì¦1: ì „ê¸°ê¸°ëŠ¥ì‚¬/êµ­ê°€ê¸°ìˆ ìê²© í˜•ì‹
        formatted_texts = []
        for idx, lic in enumerate(licenses, start=1):
            qulf_itnm = lic.get("QULF_ITNM", "").strip()  # ìê²©ì¦ëª…
            qulf_lcns_lcfn = lic.get("QULF_LCNS_LCFN", "").strip()  # ìê²©ì¦ ë¶„ë¥˜
            
            if qulf_itnm and qulf_lcns_lcfn:
                formatted_texts.append(f"ìê²©ì¦{idx}: {qulf_itnm}/{qulf_lcns_lcfn}")
            elif qulf_itnm:
                formatted_texts.append(f"ìê²©ì¦{idx}: {qulf_itnm}")
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        text = "\n".join(formatted_texts) if formatted_texts else "ì •ë³´ ì—†ìŒ"
        
        # seek_idëŠ” HOPE_JSCD1 ë§¤í•‘ì„ ìœ„í•´ ì‚¬ìš© (JHNT_MBNì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None)
        seek_id = item.get("JHNT_MBN", "") or item.get("SEEK_CUST_NO", "")
        
        # HOPE_JSCD1 ì •ë³´ ê°€ì ¸ì™€ì„œ ì§ì¢…ëª…ìœ¼ë¡œ ë³€í™˜
        hope_jscd1 = self.hope_jscd1_map.get(seek_id, "")
        job_name = self.get_job_name_from_code(hope_jscd1)
        job_examples = []  # í•„ìš”ì‹œ HOPE_JSCD1ë¡œë¶€í„° ì§ì¢… ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥
        
        # ì ìˆ˜ ê³„ì‚° (ë¹„ë™ê¸°)
        score, _ = await self.llm_scorer.score_async("ìê²©ì¦", job_name, job_examples, text, session)
        
        # scoreë§Œ ë°˜í™˜ (ê·¸ë˜í”„ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜)
        return {
            "JHNT_CTN": str(jhnt_ctn),  # ë¬¸ìì—´ë¡œ ë³€í™˜
            "certification_score": score  # ê·¸ë˜í”„: certification_score
        }
    
    async def _preprocess_certification(self, data):
        """ìê²©ì¦ íŠ¹í™” ì „ì²˜ë¦¬ (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬)"""
        if not isinstance(data, list):
            data = [data]
        
        # ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° ë ˆì½”ë“œ ì²˜ë¦¬
        rows = []
        import logging
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for item in data:
                task = self._process_single_certification(item, session)
                tasks.append(task)
            
            results = await atqdm.gather(*tasks, desc="ìê²©ì¦ ì „ì²˜ë¦¬", unit="ê±´")
            
            for idx, result in enumerate(results):
                try:
                    if result is not None:
                        rows.append(result)
                except Exception as e:
                    item = data[idx]
                    jhnt_ctn = item.get("JHNT_CTN", "unknown")
                    print(f"âš ï¸ ìê²©ì¦ ì²˜ë¦¬ ì˜¤ë¥˜ (JHNT_CTN: {jhnt_ctn}): {e}")
                    rows.append({
                        "JHNT_CTN": str(jhnt_ctn),  # ë¬¸ìì—´ë¡œ ë³€í™˜
                        "certification_score": None
                    })
        
        # DataFrame ìƒì„± ì „ì— Logger ê°ì²´ í™•ì¸ ë° ì œê±°
        cleaned_rows = []
        for row_idx, row in enumerate(rows):
            cleaned_row = {}
            for key, value in row.items():
                # Logger ê°ì²´ì¸ì§€ í™•ì¸
                if isinstance(value, logging.Logger) or 'Logger' in str(type(value)):
                    print(f"âš ï¸ [ìê²©ì¦ ì „ì²˜ë¦¬] {row_idx}ë²ˆì§¸ í–‰ì˜ ë”•ì…”ë„ˆë¦¬ í‚¤ '{key}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(value).__name__})")
                    cleaned_row[key] = np.nan
                else:
                    cleaned_row[key] = value
            cleaned_rows.append(cleaned_row)
        
        return pd.DataFrame(cleaned_rows)


    def load_and_preprocess_data(self, data_file, json_name=None, limit_data=False, limit_size=5000):
        """
        ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            data_file (str): ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            sheet_name (str): ì—‘ì…€ ì‹œíŠ¸ëª… (Excel íŒŒì¼ìš©)
            json_name (str): JSON ë°ì´í„° íƒ€ì… ('ì´ë ¥ì„œ', 'ìê¸°ì†Œê°œì„œ', 'ì§ì—…í›ˆë ¨', 'ìê²©ì¦')
            limit_data (bool): í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ë°ì´í„° ì œí•œ ì—¬ë¶€
            limit_size (int): ì œí•œí•  ë°ì´í„° í¬ê¸°
        
        Returns:
            pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        """
        # ë°ì´í„° ë¡œë“œ
        if data_file.endswith('.csv'):
            data = pd.read_csv(data_file)
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ê²½ìš° CSV íŒŒì¼ë„ ì œí•œ
            if limit_data and len(data) > limit_size:
                original_count = len(data)
                data = data.head(limit_size)
                print(f"ğŸ“Š CSV ë°ì´í„° ì œí•œ: {len(data)}ê°œ í–‰ ì‚¬ìš© (ì „ì²´ {original_count}ê°œ ì¤‘ ì• {limit_size}ê°œ)")
            data_processed = self.basic_preprocessing(data)
        elif data_file.endswith(('.xlsx', '.xls')):
            data = pd.read_excel(data_file, sheet_name=self.sheet_name)
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ê²½ìš° Excel íŒŒì¼ë„ ì œí•œ
            if limit_data and len(data) > limit_size:
                original_count = len(data)
                data = data.head(limit_size)
                print(f"ğŸ“Š Excel ë°ì´í„° ì œí•œ: {len(data)}ê°œ í–‰ ì‚¬ìš© (ì „ì²´ {original_count}ê°œ ì¤‘ ì• {limit_size}ê°œ)")
            data_processed = self.basic_preprocessing(data)
        elif data_file.endswith('.json'):
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # JSON íŒŒì¼ì˜ ê²½ìš° json_nameì„ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì‚¬ìš© (ë¹„ë™ê¸°)
                data_processed = asyncio.run(self.nlp_preprocessing(data, json_name=json_name, limit_data=limit_data, limit_size=limit_size))
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV, Excel ë˜ëŠ” JSON íŒŒì¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        return data_processed


    async def get_merged_df(self, file_list, limit_data=False, limit_size=5000):
        """
        íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ê° íŒŒì¼ì„ load_and_preprocess_dataë¡œ ì½ê³  self.df_listì— append,
        ì´í›„ JHNT_MBN ë˜ëŠ” JHNT_CTN ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì¡°ì¸í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
        
        ì²« ë²ˆì§¸ íŒŒì¼(CSV)ì€ ìˆœì°¨ ì²˜ë¦¬í•˜ê³ , ë‚˜ë¨¸ì§€ 4ê°œ JSON íŒŒì¼ì€ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        CSVë¥¼ ë¨¼ì € ë¡œë“œí•˜ì—¬ ìƒìœ„ ì§ì¢… ì†Œë¶„ë¥˜ë¥¼ í•„í„°ë§í•˜ê³ , í•´ë‹¹í•˜ëŠ” JHNT_MBN/JHNT_CTNë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            file_list (list): íŒŒì¼ëª…(str) ë¦¬ìŠ¤íŠ¸
            limit_data (bool): í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ë°ì´í„° ì œí•œ ì—¬ë¶€
            limit_size (int): ì œí•œí•  ë°ì´í„° í¬ê¸°
 
        Returns:
            pd.DataFrame: JHNT_MBN ë˜ëŠ” JHNT_CTN ê¸°ì¤€ìœ¼ë¡œ ì¡°ì¸ëœ ë°ì´í„°í”„ë ˆì„ -> repeat ì²˜ë¦¬ í•„ìš”
        """
        self.df_list = []
        result = None
        
        # í•„í„°ë§ëœ JHNT_MBN, JHNT_CTN ì§‘í•© (ì´ˆê¸°ê°’ì€ None - í•„í„°ë§ ì•ˆ í•¨)
        filtered_jhnt_mbn_set = None
        filtered_jhnt_ctn_set = None
        
        # ì²« ë²ˆì§¸ íŒŒì¼(ì •í˜• ë°ì´í„° CSV) ë¨¼ì € ì²˜ë¦¬ - HOPE_JSCD1(í¬ë§ ì§ì¢… ì½”ë“œ) ì •ë³´ ì €ì¥ ë° í•„í„°ë§
        if file_list:
            # ì²« ë²ˆì§¸ íŒŒì¼ì€ ì •í˜• ë°ì´í„°ì´ë¯€ë¡œ json_name=None
            csv_start_time = time.time()
            print(f"[DEBUG] ì²« ë²ˆì§¸ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_list[0]}, íƒ€ì…: ì •í˜• ë°ì´í„° (CSV)")
            df = self.load_and_preprocess_data(file_list[0], json_name=None, limit_data=limit_data, limit_size=limit_size)
            
            # ìƒìœ„ ì§ì¢… ì†Œë¶„ë¥˜ í•„í„°ë§ (top_job_categoriesê°€ -1ì´ ì•„ë‹ˆê³  HOPE_JSCD1ì´ ìˆëŠ” ê²½ìš°)
            if self.top_job_categories != -1 and 'HOPE_JSCD1' in df.columns:
                print(f"\nğŸ“Š ì§ì¢… ì†Œë¶„ë¥˜ í•„í„°ë§ ì‹œì‘ (ìƒìœ„ {self.top_job_categories}ê°œ)")
                print("="*60)
                
                # HOPE_JSCD1 ë¹ˆë„ìˆ˜ ê³„ì‚° (ê²°ì¸¡ì¹˜ ì œì™¸)
                job_counts = df['HOPE_JSCD1'].value_counts()
                print(f"ì „ì²´ ì§ì¢… ì†Œë¶„ë¥˜ ìˆ˜: {len(job_counts)}ê°œ")
                
                # ìƒìœ„ Nê°œ ì„ íƒ
                top_jobs = job_counts.head(self.top_job_categories)
                top_job_codes = set(top_jobs.index.tolist())
                
                print(f"\nìƒìœ„ {self.top_job_categories}ê°œ ì§ì¢… ì†Œë¶„ë¥˜:")
                for idx, (job_code, count) in enumerate(top_jobs.items(), 1):
                    job_name = self.get_job_name_from_code(job_code)
                    print(f"  {idx}. {job_code} ({job_name}): {count}ê±´")
                
                # í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±
                original_count = len(df)
                df = df[df['HOPE_JSCD1'].isin(top_job_codes)].copy()
                filtered_count = len(df)
                
                print(f"\ní•„í„°ë§ ê²°ê³¼: {original_count}ê±´ â†’ {filtered_count}ê±´ ({filtered_count/original_count*100:.1f}%)")
                print("="*60)
                
                # í•„í„°ë§ëœ JHNT_MBN, JHNT_CTN ì¶”ì¶œ
                if 'JHNT_MBN' in df.columns:
                    filtered_jhnt_mbn_set = set(df['JHNT_MBN'].dropna().unique())
                    print(f"í•„í„°ë§ëœ JHNT_MBN ìˆ˜: {len(filtered_jhnt_mbn_set)}ê°œ")
                if 'JHNT_CTN' in df.columns:
                    filtered_jhnt_ctn_set = set(df['JHNT_CTN'].dropna().unique())
                    print(f"í•„í„°ë§ëœ JHNT_CTN ìˆ˜: {len(filtered_jhnt_ctn_set)}ê°œ")
            else:
                if self.top_job_categories == -1:
                    print("ğŸ“Š ì§ì¢… ì†Œë¶„ë¥˜ í•„í„°ë§ ë¹„í™œì„±í™” (ì „ì²´ ì‚¬ìš©)")
                else:
                    print("âš ï¸ HOPE_JSCD1 ì»¬ëŸ¼ì´ ì—†ì–´ ì§ì¢… ì†Œë¶„ë¥˜ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            
            csv_elapsed = time.time() - csv_start_time
            print(f"â±ï¸ ì •í˜• ë°ì´í„°(CSV) ì²˜ë¦¬ ì†Œìš” ì‹œê°„: {csv_elapsed:.2f}ì´ˆ")
            self.df_list.append(df)
            result = df
            
            print(f"[DEBUG] ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {result.shape}")
            print(f"[DEBUG] ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼: {list(result.columns)}")
            print(f"[DEBUG] ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì— JHNT_MBN ì¡´ì¬: {'JHNT_MBN' in result.columns}")
            print(f"[DEBUG] ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì— JHNT_CTN ì¡´ì¬: {'JHNT_CTN' in result.columns}")
            
            # HOPE_JSCD1 ì •ë³´ë¥¼ JHNT_MBN ê¸°ì¤€ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ì €ì¥
            if 'HOPE_JSCD1' in df.columns and 'JHNT_MBN' in df.columns:
                self.hope_jscd1_map = df.set_index('JHNT_MBN')['HOPE_JSCD1'].to_dict()
                print(f"[DEBUG] HOPE_JSCD1 ë§¤í•‘ ìƒì„± ì™„ë£Œ: {len(self.hope_jscd1_map)}ê°œ")
            else:
                print(f"[DEBUG] ê²½ê³ : HOPE_JSCD1 ë˜ëŠ” JHNT_MBNì´ ì—†ì–´ ë§¤í•‘ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë‚˜ë¨¸ì§€ 4ê°œ íŒŒì¼ì„ ë¹„ë™ê¸° ë³‘ë ¬ë¡œ ì²˜ë¦¬
        async def process_json_file_async(file_info):
            """ë‹¨ì¼ JSON íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ (ë¹„ë™ê¸°)"""
            file, json_name, idx = file_info
            try:
                file_start_time = time.time()
                print(f"[DEBUG] {idx+1}ë²ˆì§¸ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file}, íƒ€ì…: {json_name}")
                # JSON íŒŒì¼ ë¡œë“œ
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # í•„í„°ë§ëœ í‚¤ê°’ë§Œ ì‚¬ìš© (í•„í„°ë§ì´ í™œì„±í™”ëœ ê²½ìš°)
                if isinstance(data, list):
                    # ë¨¼ì € í‚¤ê°’ë“¤ì„ ë¬¸ìì—´ë¡œ í†µì¼
                    for item in data:
                        if "JHNT_MBN" in item:
                            item["JHNT_MBN"] = str(item["JHNT_MBN"])
                        if "JHNT_CTN" in item:
                            item["JHNT_CTN"] = str(item["JHNT_CTN"])
                        if "SEEK_CUST_NO" in item:
                            item["SEEK_CUST_NO"] = str(item["SEEK_CUST_NO"])
                    
                    original_count = len(data)
                    filtered_data = data
                    
                    # ì´ë ¥ì„œì™€ ìê¸°ì†Œê°œì„œëŠ” JHNT_MBN ë˜ëŠ” SEEK_CUST_NOë¡œ í•„í„°ë§
                    if json_name in ['ì´ë ¥ì„œ', 'ìê¸°ì†Œê°œì„œ']:
                        if filtered_jhnt_mbn_set is not None:
                            # ë””ë²„ê¹… ë¡œê·¸
                            print(f"  [{json_name}] filtered_jhnt_mbn_set ê¸¸ì´: {len(filtered_jhnt_mbn_set)}, ì²˜ìŒ 5ê°œ: {list(filtered_jhnt_mbn_set)[:5]}")
                            print(f"  [{json_name}] data ì²˜ìŒ 5ê°œ JHNT_MBN: {[item.get('JHNT_MBN', '') for item in data[:5]]}")
                            print(f"  [{json_name}] data ì²˜ìŒ 5ê°œ SEEK_CUST_NO: {[item.get('SEEK_CUST_NO', '') for item in data[:5]]}")
                            
                            filtered_data = [
                                item for item in data
                                if item.get('JHNT_MBN', '') in filtered_jhnt_mbn_set or 
                                   item.get('SEEK_CUST_NO', '') in filtered_jhnt_mbn_set
                            ]
                            print(f"  [{json_name}] í•„í„°ë§: {original_count}ê±´ â†’ {len(filtered_data)}ê±´ (JHNT_MBN/SEEK_CUST_NO ê¸°ì¤€)")
                    
                    # ì§ì—…í›ˆë ¨ê³¼ ìê²©ì¦ì€ JHNT_CTNìœ¼ë¡œ í•„í„°ë§
                    elif json_name in ['ì§ì—…í›ˆë ¨', 'ìê²©ì¦']:
                        if filtered_jhnt_ctn_set is not None:
                            filtered_data = [
                                item for item in data
                                if item.get('JHNT_CTN', '') in filtered_jhnt_ctn_set
                            ]
                            print(f"  [{json_name}] í•„í„°ë§: {original_count}ê±´ â†’ {len(filtered_data)}ê±´ (JHNT_CTN ê¸°ì¤€)")
                    
                    data = filtered_data
                
                # ë¹„ë™ê¸° ì „ì²˜ë¦¬
                df = await self.nlp_preprocessing(data, json_name=json_name, limit_data=limit_data, limit_size=limit_size)
                file_elapsed = time.time() - file_start_time
                print(f"[DEBUG] {json_name} ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {df.shape}")
                print(f"[DEBUG] {json_name} ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼: {list(df.columns)}")
                print(f"â±ï¸ {json_name} ì²˜ë¦¬ ì†Œìš” ì‹œê°„: {file_elapsed:.2f}ì´ˆ")
                return (json_name, df, idx, file_elapsed)
            except Exception as e:
                print(f"âš ï¸ {json_name} íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                raise
        
        json_files = []
        for idx, file in enumerate(file_list[1:], start=0):
            if idx >= len(self.json_names):
                raise IndexError(f"JSON íŒŒì¼ ìˆ˜({len(file_list)-1})ê°€ json_names ê¸¸ì´({len(self.json_names)})ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. file: {file}")
            current_json_name = self.json_names[idx]
            json_files.append((file, current_json_name, idx))
        
        # ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¡œ 4ê°œ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬
        tasks = [process_json_file_async(file_info) for file_info in json_files]
        results = await asyncio.gather(*tasks)
        
        processed_dfs = {}
        json_file_times = {}
        for json_name, df, idx, file_elapsed in results:
            processed_dfs[idx] = (json_name, df)
            json_file_times[json_name] = file_elapsed
        
        # JSON íŒŒì¼ ì²˜ë¦¬ ì‹œê°„ ìš”ì•½ ì¶œë ¥
        if json_file_times:
            print("\n" + "="*60)
            print("â±ï¸ JSON íŒŒì¼ë³„ ì²˜ë¦¬ ì‹œê°„ ìš”ì•½")
            print("="*60)
            total_json_time = sum(json_file_times.values())
            for json_name, elapsed in sorted(json_file_times.items(), key=lambda x: x[1], reverse=True):
                percentage = (elapsed / total_json_time * 100) if total_json_time > 0 else 0
                print(f"  {json_name:15s}: {elapsed:7.2f}ì´ˆ ({percentage:5.1f}%)")
            print(f"  {'ì „ì²´':15s}: {total_json_time:7.2f}ì´ˆ (100.0%)")
            print("="*60)
        
        # ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ë“¤ì„ ìˆœì„œëŒ€ë¡œ ë³‘í•©
        merge_start_time = time.time()
        import logging
        
        for idx in tqdm(sorted(processed_dfs.keys()), desc="ë°ì´í„° ë³‘í•©", unit="íŒŒì¼"):
            json_name, df = processed_dfs[idx]
            self.df_list.append(df)
            
            # ì§ì—…í›ˆë ¨ê³¼ ìê²©ì¦ì€ JHNT_CTN ê¸°ì¤€ìœ¼ë¡œ merge
            if json_name in ['ì§ì—…í›ˆë ¨', 'ìê²©ì¦']:
                merge_key = "JHNT_CTN"
            else:
                merge_key = "JHNT_MBN"
            
            print(f"[DEBUG] ë³‘í•© í‚¤: {merge_key}")
            print(f"[DEBUG] resultì— {merge_key} ì¡´ì¬: {merge_key in result.columns}")
            print(f"[DEBUG] {json_name}ì— {merge_key} ì¡´ì¬: {merge_key in df.columns}")
            
            # ë³‘í•© í‚¤ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if merge_key not in result.columns:
                print(f"[DEBUG] ERROR: result ì»¬ëŸ¼ ëª©ë¡: {list(result.columns)}")
                raise KeyError(f"ë³‘í•© í‚¤ '{merge_key}'ê°€ ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(result.columns)}")
            if merge_key not in df.columns:
                print(f"[DEBUG] ERROR: {json_name} ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
                raise KeyError(f"ë³‘í•© í‚¤ '{merge_key}'ê°€ {json_name} ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤. íŒŒì¼: {file_list[idx+1]}, ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
            
            print(f"[DEBUG] ë³‘í•© ì „ result í¬ê¸°: {result.shape}, {json_name} í¬ê¸°: {df.shape}")
            
            # ë³‘í•© í‚¤ë¥¼ ë¬¸ìì—´ë¡œ í†µì¼ (íƒ€ì… ë¶ˆì¼ì¹˜ ë°©ì§€)
            if merge_key in result.columns:
                result[merge_key] = result[merge_key].astype(str)
            if merge_key in df.columns:
                df[merge_key] = df[merge_key].astype(str)
            
            # ë³‘í•© ì „ì— Logger ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸
            for col in df.columns:
                if df[col].dtype == 'object' and len(df) > 0:
                    non_null_values = df[col].dropna()
                    if len(non_null_values) > 0:
                        first_val = non_null_values.iloc[0]
                        if isinstance(first_val, logging.Logger) or 'Logger' in str(type(first_val)):
                            print(f"âš ï¸ [ë³‘í•© ì „] {json_name}ì˜ ì»¬ëŸ¼ '{col}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(first_val).__name__})")
                            # Logger ê°ì²´ë¥¼ NaNìœ¼ë¡œ ëŒ€ì²´
                            df[col] = df[col].apply(lambda x: np.nan if (isinstance(x, logging.Logger) or 'Logger' in str(type(x))) else x)
            
            # í…Œì´ë¸”ì„ ê¸°ì¤€ìœ¼ë¡œ inner join
            result = result.merge(df, on=merge_key, how="inner", suffixes=('', f'_df{idx+1}'))
            print(f"[DEBUG] ë³‘í•© í›„ result í¬ê¸°: {result.shape}")
            
            # ë³‘í•© í›„ì— Logger ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸
            for col in result.columns:
                if result[col].dtype == 'object' and len(result) > 0:
                    non_null_values = result[col].dropna()
                    if len(non_null_values) > 0:
                        first_val = non_null_values.iloc[0]
                        if isinstance(first_val, logging.Logger) or 'Logger' in str(type(first_val)):
                            print(f"âš ï¸ [ë³‘í•© í›„] resultì˜ ì»¬ëŸ¼ '{col}'ì— Logger ê°ì²´ ë°œê²¬! (íƒ€ì…: {type(first_val).__name__})")
        
        merge_elapsed = time.time() - merge_start_time
        print(f"â±ï¸ ë°ì´í„° ë³‘í•© ì†Œìš” ì‹œê°„: {merge_elapsed:.2f}ì´ˆ")
        
        # ë³‘í•© í›„ ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” rowì˜ ë¹„ìœ¨ í™•ì¸ (inner joinìœ¼ë¡œ ì¸í•œ ê²°ì¸¡ì¹˜ í™•ì¸)
        total_rows = len(result)
        rows_with_missing = result.isnull().any(axis=1).sum()
        missing_ratio = (rows_with_missing / total_rows * 100) if total_rows > 0 else 0
        print(f"\nğŸ“Š ë³‘í•© í›„ ê²°ì¸¡ì¹˜ ë¶„ì„:")
        print(f"   ì „ì²´ í–‰ ìˆ˜: {total_rows}ê°œ")
        print(f"   ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ìˆ˜: {rows_with_missing}ê°œ")
        print(f"   ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ë¹„ìœ¨: {missing_ratio:.2f}%")
        
        # ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ë„ ì¶œë ¥
        missing_by_column = result.isnull().sum()
        columns_with_missing = missing_by_column[missing_by_column > 0]
        if len(columns_with_missing) > 0:
            print(f"\nğŸ“Š ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ í˜„í™©:")
            for col, missing_count in columns_with_missing.items():
                missing_pct = (missing_count / total_rows * 100) if total_rows > 0 else 0
                print(f"   {col}: {missing_count}ê°œ ({missing_pct:.2f}%)")
        
        # ë²”ì£¼í˜•ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•  ì»¬ëŸ¼ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ìµœë¹ˆê°’ ë³´ê°„ì„ ìœ„í•´)
        categorical_cols = ['HOPE_JSCD1', 'HOPE_JSCD2', 'HOPE_JSCD3']
        for col in categorical_cols:
            if col in result.columns:
                result[col] = result[col].astype(str).replace('nan', np.nan)
        
        # ê²°ì¸¡ì¹˜ ë³´ê°„ (í‰ê· ê°’ ë˜ëŠ” ìµœë¹ˆê°’ìœ¼ë¡œ)
        from . import utils
        print(f"\nğŸ“Š ê²°ì¸¡ì¹˜ ë³´ê°„ ì‹œì‘...")
        result = utils.impute_missing_values(result)
        print(f"âœ… ê²°ì¸¡ì¹˜ ë³´ê°„ ì™„ë£Œ")
        
        # Logger ê°ì²´ê°€ ë°ì´í„°í”„ë ˆì„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬
        import logging
        logger_columns = []
        for col in result.columns:
            if result[col].dtype == 'object' and len(result) > 0:
                non_null_values = result[col].dropna()
                if len(non_null_values) > 0:
                    first_val = non_null_values.iloc[0]
                    # Logger ê°ì²´ì¸ì§€ í™•ì¸
                    if isinstance(first_val, logging.Logger) or 'Logger' in str(type(first_val)):
                        logger_columns.append((col, type(first_val).__name__))
                        print(f"âš ï¸ [ì „ì²˜ë¦¬] ê²½ê³ : ì»¬ëŸ¼ '{col}'ì— Logger ê°ì²´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! (íƒ€ì…: {type(first_val).__name__})")
        
        if logger_columns:
            print(f"\nâŒ [ì „ì²˜ë¦¬] ì˜¤ë¥˜: ë‹¤ìŒ ì»¬ëŸ¼ì— Logger ê°ì²´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
            for col, col_type in logger_columns:
                print(f"   - {col} (íƒ€ì…: {col_type})")
            print(f"ì´ ì»¬ëŸ¼ë“¤ì€ ë°ì´í„° ì •ë¦¬ ê³¼ì •ì—ì„œ ì œê±°ë©ë‹ˆë‹¤.")
        
        return result