"""
ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ - ê·¸ë˜í”„ íŒŒì‹±, ë°ì´í„° ë¡œë“œ, ì „ì²˜ë¦¬, ë¡œê¹… ê¸°ëŠ¥
"""
import re
import logging
import os
import json
import time
import itertools
import pandas as pd
import numpy as np
import networkx as nx
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score


# ============================================================================
# ê·¸ë˜í”„ íŒŒì‹± í•¨ìˆ˜
# ============================================================================

def extract_treatments_from_dot(dot_file_path: Path) -> List[Dict[str, str]]:
    """
    .dot íŒŒì¼ì—ì„œ treatment ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Input:
        dot_file_path (Path): .dot íŒŒì¼ ê²½ë¡œ
    
    Output:
        List[Dict[str, str]]: treatment ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ê° treatmentëŠ” dict í˜•íƒœ)
            - ê° dictëŠ” ë‹¤ìŒ í‚¤ë¥¼ í¬í•¨: node, treatment_var, treatment_name, 
              treatment_def, treatment_question, label, outcome
    """
    with open(dot_file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    treatments = []
    
    # subgraph cluster_treatments ë¸”ë¡ ì°¾ê¸°
    subgraph_pattern = r'subgraph\s+cluster_treatments\s*\{[^}]*\}'
    subgraph_match = re.search(subgraph_pattern, content, re.DOTALL)
    
    if not subgraph_match:
        return treatments
    
    subgraph_content = subgraph_match.group(0)
    
    # T1, T2, ... í˜•íƒœì˜ treatment ë…¸ë“œ ì°¾ê¸°
    treatment_pattern = r'(T\d+)\s*\[([^\]]+)\]'
    treatment_matches = re.finditer(treatment_pattern, subgraph_content, re.DOTALL)
    
    for match in treatment_matches:
        node_name = match.group(1)  # T1, T2, etc.
        node_attrs = match.group(2)
        
        # ì†ì„± ì¶”ì¶œ
        treatment_var = re.search(r'treatment_var\s*=\s*"([^"]+)"', node_attrs)
        treatment_name = re.search(r'treatment_name\s*=\s*"([^"]+)"', node_attrs)
        treatment_def = re.search(r'treatment_def\s*=\s*"([^"]+)"', node_attrs)
        treatment_question = re.search(r'treatment_question\s*=\s*"([^"]+)"', node_attrs)
        label = re.search(r'label\s*=\s*"([^"]+)"', node_attrs)
        
        treatment_info = {
            "node": node_name,
            "treatment_var": treatment_var.group(1) if treatment_var else "",
            "treatment_name": treatment_name.group(1) if treatment_name else "",
            "treatment_def": treatment_def.group(1) if treatment_def else "",
            "treatment_question": treatment_question.group(1) if treatment_question else "",
            "label": label.group(1) if label else node_name,
        }
        
        # outcome ì •ë³´ë„ ì¶”ì¶œ (subgraphì˜ labelì—ì„œ)
        outcome_match = re.search(r'label\s*=\s*"Treatments\s*\(outcome:\s*([^)]+)\)"', subgraph_content)
        if outcome_match:
            treatment_info["outcome"] = outcome_match.group(1).strip()
        
        treatments.append(treatment_info)
    
    return treatments


def extract_treatments_from_graph(graph_file_path: Path) -> List[Dict[str, str]]:
    """
    DOT ê·¸ë˜í”„ íŒŒì¼ì—ì„œ treatment ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Input:
        graph_file_path (Path): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (.dot íŒŒì¼)
    
    Output:
        List[Dict[str, str]]: treatment ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    graph_path = Path(graph_file_path)
    
    if graph_path.suffix == '.dot':
        return extract_treatments_from_dot(graph_path)
    else:
        return []


def find_all_graph_files(data_dir: Path, graph_data_dir: Optional[str] = None) -> List[Path]:
    """
    ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    
    Input:
        data_dir (Path): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        graph_data_dir (Optional[str]): ê·¸ë˜í”„ ë°ì´í„° ë””ë ‰í† ë¦¬ ì´ë¦„ (ê¸°ë³¸ê°’: "graph_data")
    
    Output:
        List[Path]: ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ì •ë ¬ëœ ìˆœì„œ)
    """
    if graph_data_dir is None:
        graph_data_dir = "graph_data"
    
    graph_data_path = Path(data_dir) / graph_data_dir
    
    if not graph_data_path.exists():
        return []
    
    # .dot íŒŒì¼ë§Œ ì°¾ê¸°
    graph_files = list(graph_data_path.glob("graph_*.dot"))
    
    # ì •ë ¬í•˜ì—¬ ë°˜í™˜
    return sorted(graph_files)


def get_treatments_from_all_graphs(data_dir: Path, graph_data_dir: Optional[str] = None) -> Dict[str, List[Dict[str, str]]]:
    """
    ëª¨ë“  ê·¸ë˜í”„ íŒŒì¼ì—ì„œ treatment ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Input:
        data_dir (Path): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        graph_data_dir (Optional[str]): ê·¸ë˜í”„ ë°ì´í„° ë””ë ‰í† ë¦¬ ì´ë¦„
    
    Output:
        Dict[str, List[Dict[str, str]]]: {graph_file_name: [treatment_info, ...]} ë”•ì…”ë„ˆë¦¬
    """
    graph_files = find_all_graph_files(data_dir, graph_data_dir)
    
    result = {}
    
    for graph_file in graph_files:
        treatments = extract_treatments_from_graph(graph_file)
        if treatments:
            result[str(graph_file)] = treatments
    
    return result


# ============================================================================
# ë¡œê¹… í•¨ìˆ˜
# ============================================================================

def setup_logging(
    log_dir: Optional[Path] = None,
    log_filename: Optional[str] = None,
    level: int = logging.INFO,
    no_logs: bool = False
) -> Optional[logging.Logger]:
    """
    ë¡œê¹…ì„ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜
    
    Input:
        log_dir (Optional[Path]): ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        log_filename (Optional[str]): ë¡œê·¸ íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
        level (int): ë¡œê¹… ë ˆë²¨ (ê¸°ë³¸ê°’: logging.INFO)
        no_logs (bool): ë¡œê·¸ ì €ì¥ ë¹„í™œì„±í™” ì—¬ë¶€
    
    Output:
        Optional[logging.Logger]: Logger ê°ì²´ ë˜ëŠ” None (no_logs=Trueì¸ ê²½ìš°)
    """
    if no_logs:
        return None
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
    if log_dir is None:
        script_dir = Path(__file__).parent.parent
        log_dir = script_dir / "log"
    else:
        log_dir = Path(log_dir)
    
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # ë¡œê·¸ íŒŒì¼ëª… ì„¤ì •
    if log_filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"experiment_{timestamp}.log"
    
    log_filepath = log_dir / log_filename
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filepath, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¡œê¹… ì‹œì‘ - ë¡œê·¸ íŒŒì¼: {log_filepath}")
    
    return logger


# ============================================================================
# ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜
# ============================================================================

def create_causal_graph(graph_file: str) -> nx.DiGraph:
    """
    DOT í˜•ì‹ ê·¸ë˜í”„ íŒŒì¼ì„ ì½ì–´ì„œ NetworkX ì¸ê³¼ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Input:
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (DOT í˜•ì‹)
    
    Output:
        nx.DiGraph: ì¸ê³¼ ê·¸ë˜í”„ ê°ì²´ (NetworkX ë°©í–¥ì„± ê·¸ë˜í”„)
    """
    return _parse_dot_graph(graph_file)


def _parse_dot_graph(graph_file: str) -> nx.DiGraph:
    """
    DOT í˜•ì‹ ê·¸ë˜í”„ íŒŒì¼ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    
    Input:
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
    
    Output:
        nx.DiGraph: íŒŒì‹±ëœ NetworkX ë°©í–¥ì„± ê·¸ë˜í”„
    """
    try:
        # pydotì„ ì‚¬ìš©í•˜ì—¬ DOT íŒŒì¼ ì½ê¸°
        import pydot
        graphs = pydot.graph_from_dot_file(graph_file)
        if not graphs:
            raise ValueError(f"DOT íŒŒì¼ì—ì„œ ê·¸ë˜í”„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_file}")
        
        # ì²« ë²ˆì§¸ ê·¸ë˜í”„ ì‚¬ìš©
        dot_graph = graphs[0]
        
        # NetworkX ê·¸ë˜í”„ë¡œ ë³€í™˜
        G = nx.drawing.nx_pydot.from_pydot(dot_graph)
        
        # ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ë³€í™˜ (digraphì¸ ê²½ìš°)
        if not G.is_directed():
            with open(graph_file, 'r', encoding='utf-8') as f:
                content = f.read()
            if content.strip().startswith('digraph'):
                G = G.to_directed()
        
        return G
    except ImportError:
        # pydotì´ ì—†ìœ¼ë©´ ìˆ˜ë™ íŒŒì‹±
        return _parse_dot_manual(graph_file)
    except Exception as e:
        # pydot íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ íŒŒì‹± ì‹œë„
        try:
            return _parse_dot_manual(graph_file)
        except Exception as e2:
            raise ValueError(f"DOT íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}. ìˆ˜ë™ íŒŒì‹±ë„ ì‹¤íŒ¨: {e2}")


def _parse_dot_manual(graph_file: str) -> nx.DiGraph:
    """
    DOT í˜•ì‹ì„ ìˆ˜ë™ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤ (pydot ì—†ì´).
    
    Input:
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
    
    Output:
        nx.DiGraph: íŒŒì‹±ëœ NetworkX ë°©í–¥ì„± ê·¸ë˜í”„
    """
    with open(graph_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    G = nx.DiGraph()
    
    # digraphì¸ì§€ í™•ì¸
    is_digraph = content.strip().startswith('digraph')
    
    # subgraph cluster_treatments ë¸”ë¡ ì œê±°
    content_without_subgraph = re.sub(
        r'subgraph\s+cluster_treatments\s*\{[^}]*\}',
        '',
        content,
        flags=re.DOTALL
    )
    
    # ë…¸ë“œ ì •ì˜ ì°¾ê¸°
    node_pattern = r'([A-Za-z_][A-Za-z0-9_]*)\s*\[[^\]]*label\s*=\s*"([^"]+)"'
    for match in re.finditer(node_pattern, content_without_subgraph):
        node_id = match.group(1)
        label = match.group(2)
        if not re.match(r'^T\d+$', node_id):
            G.add_node(node_id, label=label)
    
    # ì—£ì§€ ì°¾ê¸°
    edge_pattern = r'([A-Za-z_][A-Za-z0-9_]*)\s*->\s*([A-Za-z_][A-Za-z0-9_]*)'
    for match in re.finditer(edge_pattern, content_without_subgraph):
        source = match.group(1)
        target = match.group(2)
        if not re.match(r'^T\d+$', source) and not re.match(r'^T\d+$', target):
            G.add_edge(source, target)
    
    # ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ë³€í™˜
    if is_digraph and not G.is_directed():
        G = G.to_directed()
    
    return G


# ============================================================================
# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ============================================================================

def load_all_data(data_dir: str, seis_data_dir: str, graph_file: Optional[str] = None) -> Tuple[List[str], nx.DiGraph]:
    """
    ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°(JSON)ë¥¼ ëª¨ë‘ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Input:
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        seis_data_dir (str): seis_data ë””ë ‰í† ë¦¬ ì´ë¦„
        graph_file (Optional[str]): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ìœ¼ë¡œ ì°¾ìŒ)
    
    Output:
        Tuple[List[str], nx.DiGraph]: (íŒŒì¼ê²½ë¡œ_ë¦¬ìŠ¤íŠ¸, ì¸ê³¼ê·¸ë˜í”„)
            - íŒŒì¼ê²½ë¡œ_ë¦¬ìŠ¤íŠ¸: [ì •í˜•ë°ì´í„°ê²½ë¡œ, ì´ë ¥ì„œê²½ë¡œ, ìê¸°ì†Œê°œì„œê²½ë¡œ, ì§ì—…í›ˆë ¨ê²½ë¡œ, ìê²©ì¦ê²½ë¡œ]
            - ì¸ê³¼ê·¸ë˜í”„: NetworkX ë°©í–¥ì„± ê·¸ë˜í”„ ê°ì²´
    """
    data_path = Path(data_dir)
    
    # 1. ì •í˜• ë°ì´í„° íŒŒì¼ ê²½ë¡œ í™•ì¸ (seis_data í´ë”ì—ì„œ)
    structured_data_path = data_path / seis_data_dir / "seis_data.csv"
    
    if not structured_data_path.exists():
        raise FileNotFoundError(f"ì •í˜• ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {structured_data_path}")
    
    print(f"âœ… ì •í˜• ë°ì´í„° íŒŒì¼ ê²½ë¡œ: {structured_data_path}")
    
    # 2. ë¹„ì •í˜• ë°ì´í„°(JSON) íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    seis_data_path = data_path / seis_data_dir
    file_list = []
    
    json_files = [
        ("resume.json", "ì´ë ¥ì„œ"),
        ("coverletters.json", "ìê¸°ì†Œê°œì„œ"),
        ("trainings.json", "ì§ì—…í›ˆë ¨"),
        ("licenses.json", "ìê²©ì¦")
    ]
    
    # ì •í˜• ë°ì´í„° íŒŒì¼ì„ ë¨¼ì € ì¶”ê°€
    file_list.append(str(structured_data_path))
    
    # JSON íŒŒì¼ ê²½ë¡œ ì¶”ê°€
    for filename, json_type in json_files:
        json_path = seis_data_path / filename
        if json_path.exists():
            file_list.append(str(json_path))
            print(f"âœ… {json_type} íŒŒì¼ ê²½ë¡œ ì¶”ê°€: {json_path}")
        else:
            print(f"âš ï¸ {json_type} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    # 3. ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ (graph_fileì´ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ì²« ë²ˆì§¸ ê·¸ë˜í”„ ì‚¬ìš©)
    if graph_file is None:
        graph_data_path = data_path / "graph_data"
        if graph_data_path.exists():
            graph_files = list(graph_data_path.glob("graph_*.dot"))
            if graph_files:
                graph_file = sorted(graph_files)[0]
                print(f"âš ï¸ ê·¸ë˜í”„ íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•„ {graph_file.name}ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_data_path}")
        else:
            raise FileNotFoundError(f"ê·¸ë˜í”„ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_data_path}")
    else:
        graph_file = Path(graph_file)
    
    if not graph_file.exists():
        raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_file}")
    
    causal_graph = create_causal_graph(str(graph_file))
    print(f"âœ… ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {causal_graph.number_of_nodes()}ê°œ ë…¸ë“œ, {causal_graph.number_of_edges()}ê°œ ì—£ì§€")
    
    return file_list, causal_graph


# ============================================================================
# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================

def impute_missing_values(df: pd.DataFrame, logger: Optional[logging.Logger] = None) -> pd.DataFrame:
    """
    ê²°ì¸¡ì¹˜ë¥¼ ì „ë¶€ 0ìœ¼ë¡œ ì±„ìš°ëŠ” ë‹¨ìˆœ ë³´ê°„ í•¨ìˆ˜
    
    Input:
        df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        logger (Optional[logging.Logger]): ë¡œê±° ê°ì²´
    
    Output:
        pd.DataFrame: ê²°ì¸¡ì¹˜ê°€ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë°ì´í„°í”„ë ˆì„
    """
    df_imputed = df.copy()
    
    for col in df_imputed.columns:
        missing_count = df_imputed[col].isnull().sum()
        
        if missing_count == 0:
            continue
        
        df_imputed[col] = df_imputed[col].fillna(0)
        if logger:
            logger.info(f"ì»¬ëŸ¼ '{col}': {missing_count}ê°œ ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ë³´ê°„")
        else:
            print(f"ğŸ“Š ì»¬ëŸ¼ '{col}': {missing_count}ê°œ ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ë³´ê°„")
    
    return df_imputed


def clean_dataframe_for_causal_model(df: pd.DataFrame, required_vars: Optional[List[str]] = None, logger: Optional[logging.Logger] = None) -> pd.DataFrame:
    """
    CausalModel ìƒì„± ì „ì— ë°ì´í„°í”„ë ˆì„ì„ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜
    Logger ê°ì²´ë‚˜ ë‹¤ë¥¸ ë¹„ë°ì´í„° íƒ€ì… ì»¬ëŸ¼ ì œê±°
    
    Input:
        df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        required_vars (Optional[List[str]]): ë°˜ë“œì‹œ ìœ ì§€í•´ì•¼ í•  ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ (treatment, outcome ë“±)
        logger (Optional[logging.Logger]): ë¡œê±° ê°ì²´
    
    Output:
        pd.DataFrame: ì •ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ (Logger ê°ì²´ ë“±ì´ ì œê±°ë¨)
    """
    df_clean = df.copy()
    cols_to_drop = []
    
    if required_vars is None:
        required_vars = []
    
    for col in df_clean.columns:
        if df_clean[col].dtype == 'object':
            if len(df_clean) > 0:
                non_null_values = df_clean[col].dropna()
                if len(non_null_values) > 0:
                    first_val = non_null_values.iloc[0]
                    is_logger_object = isinstance(first_val, logging.Logger) or 'Logger' in str(type(first_val))
                    is_invalid_type = not isinstance(first_val, (str, int, float, bool, type(None)))
                    
                    if is_logger_object or is_invalid_type:
                        if col in required_vars:
                            if logger:
                                logger.warning(f"í•„ìˆ˜ ë³€ìˆ˜ '{col}'ì˜ ê°’ì´ ê°ì²´ íƒ€ì…({type(first_val).__name__})ì´ì–´ì„œ NaNìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                            else:
                                print(f"âš ï¸ í•„ìˆ˜ ë³€ìˆ˜ '{col}'ì˜ ê°’ì´ ê°ì²´ íƒ€ì…({type(first_val).__name__})ì´ì–´ì„œ NaNìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                            df_clean[col] = np.nan
                        else:
                            cols_to_drop.append(col)
    
    if cols_to_drop:
        df_clean = df_clean.drop(columns=cols_to_drop)
    
    return df_clean


def preprocess_and_merge_data(file_list: List[str], data_dir: str, limit_data: bool = False, limit_size: int = 5000, job_category_file: str = "KSIC", top_job_categories: int = 5) -> pd.DataFrame:
    """
    Preprocessor í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    
    Input:
        file_list (List[str]): íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ [ì •í˜•ë°ì´í„°, ì´ë ¥ì„œ, ìê¸°ì†Œê°œì„œ, ì§ì—…í›ˆë ¨, ìê²©ì¦]
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        limit_data (bool): í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ë°ì´í„° ì œí•œ ì—¬ë¶€
        limit_size (int): ì œí•œí•  ë°ì´í„° í¬ê¸°
        job_category_file (str): ì§ì¢… ì†Œë¶„ë¥˜ íŒŒì¼ëª… (KECO, KSCO, KSIC ì¤‘ ì„ íƒ, ê¸°ë³¸ê°’: KSIC)
        top_job_categories (int): ìƒìœ„ ì§ì¢… ì†Œë¶„ë¥˜ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5, -1ì´ë©´ ì „ì²´ ì‚¬ìš©)
    
    Output:
        pd.DataFrame: ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„
    """
    import asyncio
    from . import preprocess
    preprocessor = preprocess.Preprocessor([], job_category_file=job_category_file, top_job_categories=top_job_categories)
    absolute_file_list = [str(Path(f).resolve()) for f in file_list]
    merged_df = asyncio.run(preprocessor.get_merged_df(absolute_file_list, limit_data=limit_data, limit_size=limit_size))
    print(f"âœ… ëª¨ë“  ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ì™„ë£Œ")
    return merged_df


# ============================================================================
# ê²°ê³¼ ì €ì¥ í•¨ìˆ˜
# ============================================================================

def save_predictions_to_excel(df_with_predictions: pd.DataFrame, output_dir: Optional[Path] = None, filename: Optional[str] = None, logger: Optional[logging.Logger] = None) -> str:
    """
    ì˜ˆì¸¡ê°’ì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ì„ Excel íŒŒì¼ë¡œ ì €ì¥
    
    Input:
        df_with_predictions (pd.DataFrame): ì˜ˆì¸¡ê°’ì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„
        output_dir (Optional[Path]): ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ log í´ë” ì‚¬ìš©)
        filename (Optional[str]): íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
        logger (Optional[logging.Logger]): ë¡œê±° ê°ì²´
    
    Output:
        str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    if output_dir is None:
        script_dir = Path(__file__).parent.parent
        output_dir = script_dir / "log"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"predictions_{timestamp}.xlsx"
    
    filepath = output_dir / filename
    
    df_with_predictions.to_excel(filepath, index=False, engine='openpyxl')
    
    if logger:
        logger.info(f"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
    
    return str(filepath)


# ============================================================================
# ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜ëŠ” estimation.pyë¡œ ì´ë™ë¨
# ============================================================================


# ============================================================================
# ì„¤ì • íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
# ============================================================================

def load_config(config_path: Path) -> Dict[str, Any]:
    """
    ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤
    
    Input:
        config_path (Path): ì„¤ì • íŒŒì¼ ê²½ë¡œ (JSON í˜•ì‹)
    
    Output:
        Dict[str, Any]: ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config


# ============================================================================
# í‰ê°€ì§€í‘œ ê³„ì‚° í•¨ìˆ˜
# ============================================================================

def calculate_metrics(
    actual_y: pd.Series, 
    predicted_y: pd.Series, 
    prob_y: Optional[pd.Series] = None, 
    logger: Optional[logging.Logger] = None
) -> Dict[str, Optional[float]]:
    """
    ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë¹„êµí•˜ì—¬ í‰ê°€ì§€í‘œ(Accuracy, F1, AUC)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        actual_y: ì‹¤ì œ ì •ë‹µê°’ (Series)
        predicted_y: ì˜ˆì¸¡ê°’ (Series, í™•ë¥  ë˜ëŠ” í´ë˜ìŠ¤)
        prob_y: ì˜ˆì¸¡ í™•ë¥ ê°’ (Series, Optional. AUC ê³„ì‚°ìš©)
        logger: ë¡œê±° ê°ì²´ (Optional)
        
    Returns:
        Dict: {'accuracy', 'f1_score', 'auc'}ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    metrics = {'accuracy': None, 'f1_score': None, 'auc': None}
    
    # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜
    if not pd.api.types.is_numeric_dtype(actual_y):
        actual_y = pd.to_numeric(actual_y, errors='coerce')
    
    # NaN ì œê±°
    valid_mask = ~(pd.isna(actual_y) | pd.isna(predicted_y))
    if valid_mask.sum() == 0:
        if logger:
            logger.warning("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return metrics
        
    actual_y_clean = actual_y[valid_mask]
    predicted_y_clean = predicted_y[valid_mask]
    
    # ì´ì§„ ë¶„ë¥˜ ì—¬ë¶€ í™•ì¸
    unique_values = set(actual_y_clean.dropna().unique())
    is_binary = len(unique_values) <= 2 and all(v in [0, 1] for v in unique_values if not pd.isna(v))
    
    if is_binary:
        # í´ë˜ìŠ¤ ì˜ˆì¸¡ê°’ ìƒì„± (0.5 ê¸°ì¤€)
        # predicted_yê°€ ì´ë¯¸ í´ë˜ìŠ¤ì¼ ìˆ˜ë„ ìˆê³  í™•ë¥ ì¼ ìˆ˜ë„ ìˆìŒ
        if predicted_y_clean.min() >= 0 and predicted_y_clean.max() <= 1:
            predicted_classes = (predicted_y_clean >= 0.5).astype(int)
        else:
            # í™•ë¥ ì´ ì•„ë‹Œ ê²½ìš° (ì˜ˆ: Regression ê²°ê³¼) ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ì‹œê·¸ëª¨ì´ë“œ ë³€í™˜ ê²€í†  í•„ìš”
            # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 0.5 ê¸°ì¤€ìœ¼ë¡œ í´ë˜ìŠ¤ ë¶„ë¦¬
            predicted_classes = (predicted_y_clean >= 0.5).astype(int)
            
        metrics['accuracy'] = float(accuracy_score(actual_y_clean, predicted_classes))
        metrics['f1_score'] = float(f1_score(actual_y_clean, predicted_classes, zero_division=0))
        
        # AUC ê³„ì‚°
        # ì „ìš© í™•ë¥ ê°’(prob_y) ë˜ëŠ” í™•ë¥  í˜•íƒœì˜ ì˜ˆì¸¡ê°’(predicted_y)ì´ ìˆë‹¤ê³  ê°€ì •
        auc_input = prob_y[valid_mask] if prob_y is not None else predicted_y_clean
        metrics['auc'] = float(roc_auc_score(actual_y_clean, auc_input))
            
    return metrics

