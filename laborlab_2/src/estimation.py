"""
DoWhy ì¸ê³¼íš¨ê³¼ ì¶”ì • ë° ê²€ì¦ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ì¸ê³¼íš¨ê³¼ ì¶”ì •, ê²€ì¦ í…ŒìŠ¤íŠ¸, ë¯¼ê°ë„ ë¶„ì„ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import logging
from pathlib import Path
from datetime import datetime
import os
import sys
import pickle
import json
import time
import itertools
import gc
from typing import Dict, Any, Optional, List, Tuple
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from tqdm import tqdm
from . import utils

# CUDA 0ë²ˆ GPU ì‚¬ìš© (Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œëŠ” í• ë‹¹ëœ GPUê°€ 0ë²ˆìœ¼ë¡œ ë³´ì„)
import torch
if torch.cuda.is_available():
    torch.cuda.set_device(0)

from dowhy.causal_estimators.regression_estimator import RegressionEstimator
from dowhy import CausalModel

# ë¡œì»¬ DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

# DoWhy ë‚´ë¶€ í•¨ìˆ˜ ì„í¬íŠ¸
from dowhy.causal_estimator import estimate_effect as dowhy_estimate_effect

# DoWhy/TabPFN ë‚´ë¶€ ë¡œê¹… í™œì„±í™” (INFO ë ˆë²¨)
logging.getLogger("dowhy").setLevel(logging.INFO)
logging.getLogger("dowhy.causal_model").setLevel(logging.INFO)
logging.getLogger("dowhy.causal_estimator").setLevel(logging.INFO)
logging.getLogger("dowhy.causal_estimators.tabpfn_estimator").setLevel(logging.INFO)
logging.getLogger("tabpfn").setLevel(logging.INFO)

def predict_conditional_expectation(estimate, data_df, treatment_value=None, logger=None):
    """
    E(Y|A, X) ì¡°ê±´ë¶€ ê¸°ëŒ€ê°’ ì˜ˆì¸¡
    
    Args:
        estimate: CausalEstimate ê°ì²´
        data_df: ì˜ˆì¸¡í•  ë°ì´í„°í”„ë ˆì„
        treatment_value: ì²˜ì¹˜ ê°’ (Noneì´ë©´ ì‹¤ì œ ê°’ ì‚¬ìš©)
        logger: ë¡œê±° ê°ì²´
    
    Returns:
        tuple: (metrics_dict, data_df_with_predictions)
            - metrics_dict: {'accuracy': float, 'f1_score': float, 'auc': float} ë˜ëŠ” None
            - data_df_with_predictions: outcome ì—´ì— ì˜ˆì¸¡ê°’ì´ ì±„ì›Œì§„ ë°ì´í„°í”„ë ˆì„
    """
    if not hasattr(estimate, 'estimator'):
        raise ValueError("estimate.estimatorê°€ ì—†ìŠµë‹ˆë‹¤. estimate_causal_effectë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    
    estimator = estimate.estimator
    if not isinstance(estimator, RegressionEstimator):
        raise ValueError(f"{type(estimator).__name__}ëŠ” ì˜ˆì¸¡ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    total_samples = len(data_df)
    if logger:
        logger.info(f"E(Y|A, X) ì˜ˆì¸¡ ì‹œì‘: {total_samples}ê°œ")
        if treatment_value is not None:
            logger.info(f"ì²˜ì¹˜ ê°’: {treatment_value}")
    
    print(f"ğŸ”® ì˜ˆì¸¡ ì‹œì‘: {total_samples}ê°œ ìƒ˜í”Œ")
    
    try:
        # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ (ì›ë³¸ ë³´í˜¸)
        data_df_clean = data_df.copy()
        
        # treatmentì™€ outcome ë³€ìˆ˜ëŠ” ë°˜ë“œì‹œ ìœ ì§€í•´ì•¼ í•¨
        # _treatment_nameê³¼ _outcome_nameì€ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ (private attribute)
        treatment_var = estimate._treatment_name[0] if isinstance(estimate._treatment_name, list) else estimate._treatment_name
        outcome_var = estimate._outcome_name[0] if isinstance(estimate._outcome_name, list) else estimate._outcome_name

        # treatment ë³€ìˆ˜ê°€ ìˆëŠ”ì§€ í™•ì¸
        if treatment_var not in data_df_clean.columns:
            raise ValueError(f"Treatment ë³€ìˆ˜ '{treatment_var}'ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(data_df_clean.columns)}")
        
        # OrdinalEncoderê°€ ì €ì¥ë˜ì–´ ìˆìœ¼ë©´ ì˜ˆì¸¡ ë°ì´í„°ì—ë„ ì ìš©
        if hasattr(estimate, '_ordinal_encoder') and hasattr(estimate, '_categorical_columns'):
            ordinal_encoder = estimate._ordinal_encoder
            categorical_columns = estimate._categorical_columns
            
            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ ì¤‘ ì¸ì½”ë”©ì´ í•„ìš”í•œ ê²ƒë§Œ í•„í„°ë§
            cols_to_encode = [
                col for col in categorical_columns 
                if col in data_df_clean.columns and not pd.api.types.is_integer_dtype(data_df_clean[col])
            ]
            
            if cols_to_encode:
                print(f"ğŸ”¢ ì˜ˆì¸¡ ë°ì´í„°ì— OrdinalEncoder ì ìš©: {len(cols_to_encode)}ê°œ ë³€ìˆ˜")
                
                # ì „ì²˜ë¦¬: NaN ì²˜ë¦¬ ë° ë¬¸ìì—´ ë³€í™˜
                for col in cols_to_encode:
                    data_df_clean[col] = data_df_clean[col].fillna('__nan__').astype(str)
                
                # ì¸ì½”ë”© ì ìš©
                try:
                    data_df_clean[cols_to_encode] = ordinal_encoder.transform(data_df_clean[cols_to_encode])
                    
                    # Unknown categories ë¡œê¹…
                    for col in cols_to_encode:
                        unknown_count = (data_df_clean[col] == -1).sum()
                        if unknown_count > 0:
                            print(f"   âš ï¸ '{col}': {unknown_count}ê°œ unknown categories â†’ -1ë¡œ ì¸ì½”ë”©ë¨")
                except Exception as e:
                    print(f"   âš ï¸ OrdinalEncoder ì˜¤ë¥˜: {e}")
                    print(f"   ì»¬ëŸ¼ íƒ€ì…: {[(col, str(data_df_clean[col].dtype)) for col in cols_to_encode]}")
                    raise
        
        # Unknown Categories ì•ˆì „ì¥ì¹˜: ì˜ˆì¸¡ ì‹œë„ ì‹œ ì˜¤ë¥˜ ë°œìƒí•˜ë©´ í•´ë‹¹ í–‰ ì œì™¸
        def safe_predict(df_to_predict, treatment_val=None):
            """Unknown categories ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ í–‰ì„ ì œì™¸í•˜ê³  ì¬ì‹œë„"""
            try:
                if treatment_val is not None:
                    return estimator.interventional_outcomes(df_to_predict, treatment_val)
                else:
                    return estimator.predict(df_to_predict)
            except ValueError as e:
                if "unknown categories" in str(e).lower() or "found unknown" in str(e).lower():
                    error_msg = str(e)
                    
                    # ì˜¤ë¥˜ ë©”ì‹œì§€ì—ì„œ ì»¬ëŸ¼ ì •ë³´ ì¶”ì¶œ
                    column_info = "ì•Œ ìˆ˜ ì—†ìŒ"
                    if "column" in error_msg.lower():
                        # "in column 0" ë˜ëŠ” "in column 'col_name'" í˜•ì‹ íŒŒì‹±
                        import re
                        col_match = re.search(r"column\s+(\d+|\w+)", error_msg, re.IGNORECASE)
                        if col_match:
                            col_ref = col_match.group(1)
                            # ìˆ«ìì¸ ê²½ìš° ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                            try:
                                col_idx = int(col_ref)
                                categorical_cols = df_to_predict.select_dtypes(include=['object', 'string', 'category']).columns
                                if col_idx < len(categorical_cols):
                                    column_info = categorical_cols[col_idx]
                            except:
                                column_info = col_ref
                    
                    # ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì‹ë³„ ë° í•„í„°ë§
                    categorical_cols = df_to_predict.select_dtypes(include=['object', 'string', 'category']).columns
                    problematic_cols = []
                    
                    if len(categorical_cols) > 0 and hasattr(estimator, '_data'):
                        # Train ë°ì´í„°ì˜ ì¹´í…Œê³ ë¦¬ ê°’ë§Œ ìœ ì§€
                        train_data = estimator._data
                        rows_before = len(df_to_predict)
                        
                        for col in categorical_cols:
                            if col in train_data.columns:
                                train_categories = set(train_data[col].dropna().unique())
                                test_categories = set(df_to_predict[col].dropna().unique())
                                unknown_categories = test_categories - train_categories
                                
                                if unknown_categories:
                                    problematic_cols.append({
                                        'column': col,
                                        'unknown_count': len(unknown_categories),
                                        'unknown_values': list(unknown_categories)[:10]  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                                    })
                                    mask = df_to_predict[col].isin(train_categories) | df_to_predict[col].isna()
                                    df_to_predict = df_to_predict[mask].copy()
                        
                        rows_after = len(df_to_predict)
                        rows_removed = rows_before - rows_after
                        
                        # ë¡œê¹… ë° í”„ë¦°íŠ¸
                        if problematic_cols:
                            for prob_col in problematic_cols:
                                col_name = prob_col['column']
                                unknown_vals = prob_col['unknown_values']
                                unknown_count = prob_col['unknown_count']
                                
                                msg = (
                                    f"âš ï¸ Unknown Categories ê°ì§€ - ì»¬ëŸ¼: '{col_name}', "
                                    f"ì•Œ ìˆ˜ ì—†ëŠ” ê°’: {unknown_count}ê°œ "
                                    f"({unknown_vals[:5]}{'...' if len(unknown_vals) > 5 else ''})"
                                )
                                print(msg)
                                if logger:
                                    logger.warning(msg)
                        
                        if rows_removed > 0:
                            msg = f"ğŸ“Š í•„í„°ë§ ê²°ê³¼: {rows_before}ê±´ â†’ {rows_after}ê±´ ({rows_removed}ê±´ ì œê±°)"
                            print(msg)
                            if logger:
                                logger.info(msg)
                        
                        if len(df_to_predict) == 0:
                            error_msg = "í•„í„°ë§ í›„ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                            print(f"âŒ {error_msg}")
                            if logger:
                                logger.error(error_msg)
                            raise ValueError(error_msg)
                        
                        # ì¬ì‹œë„
                        if treatment_val is not None:
                            return estimator.interventional_outcomes(df_to_predict, treatment_val)
                        else:
                            return estimator.predict(df_to_predict)
                raise
        
        # ì˜ˆì¸¡ ìˆ˜í–‰ (ì§„í–‰ë¥  í‘œì‹œ)
        estimator_type = type(estimator).__name__
        is_tabpfn = 'tabpfn' in estimator_type.lower() or 'TabPFN' in estimator_type
        
        # TabPFNì˜ ê²½ìš° ë°°ì¹˜ í¬ê¸° í™•ì¸
        batch_size = None
        if is_tabpfn:
            # TabPFNì˜ ê²½ìš° ë°°ì¹˜ í¬ê¸° í™•ì¸
            if hasattr(estimate, 'estimator') and hasattr(estimate.estimator, '_method_params'):
                method_params = estimate.estimator._method_params
                if method_params and 'prediction_batch_size' in method_params:
                    batch_size = method_params['prediction_batch_size']
            if batch_size is None:
                batch_size = 512  # ê¸°ë³¸ ë°°ì¹˜ í¬ê¸°
        
        prediction_start_time = time.time()
        
        # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ ê°€ëŠ¥í•œ ê²½ìš° progress bar í‘œì‹œ
        if batch_size and total_samples > batch_size:
            num_batches = (total_samples + batch_size - 1) // batch_size
            print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}, ì´ ë°°ì¹˜ ìˆ˜: {num_batches}")
            
            predictions_list = []
            with tqdm(total=total_samples, desc="ì˜ˆì¸¡ ì§„í–‰", unit="ìƒ˜í”Œ", ncols=100, leave=True) as pbar:
                for i in range(0, total_samples, batch_size):
                    batch_end = min(i + batch_size, total_samples)
                    batch_df = data_df_clean.iloc[i:batch_end]
                    
                    try:
                        # ë°°ì¹˜ ì˜ˆì¸¡ ìˆ˜í–‰ (ì•ˆì „ì¥ì¹˜ í¬í•¨)
                        batch_predictions = safe_predict(batch_df, treatment_value)
                        
                        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                        if isinstance(batch_predictions, np.ndarray):
                            predictions_list.append(batch_predictions)
                        elif isinstance(batch_predictions, (list, tuple)):
                            predictions_list.extend(batch_predictions)
                        elif isinstance(batch_predictions, pd.Series):
                            predictions_list.append(batch_predictions.values)
                        else:
                            predictions_list.append([batch_predictions])
                        
                        pbar.update(len(batch_df))
                    except Exception as e:
                        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì „ì²´ ë°ì´í„°ë¡œ fallback
                        if logger:
                            logger.warning(f"ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤íŒ¨, ì „ì²´ ë°ì´í„°ë¡œ ì²˜ë¦¬: {e}")
                        pbar.close()
                        print("âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨, ì „ì²´ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
                        predictions = safe_predict(data_df_clean, treatment_value)
                        prediction_elapsed = time.time() - prediction_start_time
                        print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ì˜ˆì¸¡ê°’ ìƒì„± (ì†Œìš” ì‹œê°„: {prediction_elapsed:.2f}ì´ˆ)")
                        break
                else:
                    # ëª¨ë“  ë°°ì¹˜ê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê²½ìš°
                    if predictions_list:
                        # ë°°ì¹˜ ê²°ê³¼ í•©ì¹˜ê¸°
                        if isinstance(predictions_list[0], np.ndarray):
                            predictions = np.concatenate(predictions_list)
                        else:
                            predictions = np.array([item for sublist in predictions_list for item in (sublist if isinstance(sublist, (list, tuple)) else [sublist])])
                        prediction_elapsed = time.time() - prediction_start_time
                        print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ì˜ˆì¸¡ê°’ ìƒì„± (ì†Œìš” ì‹œê°„: {prediction_elapsed:.2f}ì´ˆ)")
                    else:
                        raise ValueError("ì˜ˆì¸¡ ê²°ê³¼ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            # ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ê±°ë‚˜ ë°°ì¹˜ í¬ê¸°ê°€ ì¶©ë¶„íˆ í° ê²½ìš°
            with tqdm(total=1, desc="ì˜ˆì¸¡ ìˆ˜í–‰", unit="ë°°ì¹˜", ncols=100, leave=True) as pbar:
                predictions = safe_predict(data_df_clean, treatment_value)
                pbar.update(1)
            
            prediction_elapsed = time.time() - prediction_start_time
            print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ì˜ˆì¸¡ê°’ ìƒì„± (ì†Œìš” ì‹œê°„: {prediction_elapsed:.2f}ì´ˆ)")
        
        if logger:
            logger.info(f"ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ì˜ˆì¸¡ê°’ ìƒì„±")
        
        predictions_series = pd.Series(predictions, index=data_df_clean.index)
        
        # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ í›„ ì˜ˆì¸¡ê°’ ì±„ìš°ê¸°
        result_df = data_df_clean.copy()
        # _outcome_nameì€ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ
        outcome_name = estimate._outcome_name[0] if isinstance(estimate._outcome_name, list) else estimate._outcome_name
        # ì˜ˆì¸¡ê°’ì´ 0~1 ì‚¬ì´ì˜ í™•ë¥ ì¸ ê²½ìš° 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°”ì´ë„ˆë¦¬í•˜ê²Œ ë³€í™˜
        # (ì‚¬ìš©ì ìš”ì²­: 0~1 ì‚¬ì´ì˜ í™•ë¥ ê°’ì„ 0.5 ê¸°ì¤€ìœ¼ë¡œ ë°”ì´ë„ˆë¦¬í•˜ê²Œ ë³€ê²½)
        if predictions_series.min() >= 0 and predictions_series.max() <= 1:
            if logger:
                logger.info("ì˜ˆì¸¡ í™•ë¥ ê°’ì„ 0.5 ê¸°ì¤€ìœ¼ë¡œ ë°”ì´ë„ˆë¦¬(0/1) ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
            print("â„¹ï¸ ì˜ˆì¸¡ í™•ë¥ ê°’ì„ 0.5 ê¸°ì¤€ìœ¼ë¡œ ë°”ì´ë„ˆë¦¬(0/1) ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
            # ì›ë³¸ í™•ë¥ ê°’ì€ _prob ì ‘ë¯¸ì‚¬ë¥¼ ë¶™ì—¬ ì €ì¥ (ë‚´ë¶€ ë©”íŠ¸ë¦­ ê³„ì‚° ìš©ë„)
            result_df[f"{outcome_name}_prob"] = predictions_series
            result_df[outcome_name] = (predictions_series >= 0.5).astype(int)
        else:
            result_df[outcome_name] = predictions_series
        
        # ì‹¤ì œ Y ê°’ê³¼ ë¹„êµí•˜ì—¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {'accuracy': None, 'f1_score': None, 'auc': None}
        if outcome_name in data_df_clean.columns:
            actual_y = data_df_clean[outcome_name]
            metrics = utils.calculate_metrics(actual_y, predictions_series, logger=logger)
            
            if metrics.get('accuracy') is not None:
                if logger:
                    logger.info(f"ì˜ˆì¸¡ ì™„ë£Œ: Accuracy={metrics['accuracy']:.4f}, F1={metrics['f1_score']:.4f}, AUC={metrics.get('auc', 'N/A')}")
            else:
                if logger:
                    # NaN ì œê±° í›„ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ í‰ê·  ê³„ì‚°
                    valid_mask = ~(pd.isna(actual_y) | pd.isna(predictions_series))
                    if valid_mask.sum() > 0:
                        logger.info(f"ì˜ˆì¸¡ ì™„ë£Œ: í‰ê· ={predictions_series[valid_mask].mean():.6f} (ì—°ì†í˜• ë³€ìˆ˜)")
        else:
            if logger:
                logger.warning(f"ì‹¤ì œ Y ê°’({outcome_name})ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return metrics, result_df
        
    except Exception as e:
        if logger:
            logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        raise

def cleanup_tabpfn_memory(estimate, device_id=0, logger=None, force_release=False):
    """
    TabPFN ëª¨ë¸ì˜ GPU ë©”ëª¨ë¦¬ë¥¼ ì™„ì „íˆ í•´ì œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        estimate: CausalEstimate ê°ì²´
        device_id: CUDA device ID (ê¸°ë³¸ê°’: 0, Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê¸°ì¤€)
        logger: ë¡œê±° ê°ì²´
        force_release: ê°•ì œ ë©”ëª¨ë¦¬ í•´ì œ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
    """
    try:
        import torch
        if not torch.cuda.is_available():
            return
        
        torch.cuda.set_device(device_id)
        
        # TabPFN ëª¨ë¸ ê°ì²´ì—ì„œ ë©”ëª¨ë¦¬ í•´ì œ
        if hasattr(estimate, 'estimator') and hasattr(estimate.estimator, 'tabpfn_model'):
            tabpfn_model = estimate.estimator.tabpfn_model
            if tabpfn_model is not None:
                # _single_model í•´ì œ
                if hasattr(tabpfn_model, '_single_model') and tabpfn_model._single_model is not None:
                    try:
                        del tabpfn_model._single_model
                    except:
                        pass
                    tabpfn_model._single_model = None
                
                # train_X, train_y ë©”ëª¨ë¦¬ í•´ì œ
                if hasattr(tabpfn_model, 'train_X'):
                    try:
                        del tabpfn_model.train_X
                    except:
                        pass
                if hasattr(tabpfn_model, 'train_y'):
                    try:
                        del tabpfn_model.train_y
                    except:
                        pass
                
                # ëª¨ë¸ ê°ì²´ ì‚­ì œ
                try:
                    del tabpfn_model
                except:
                    pass
                estimate.estimator.tabpfn_model = None
        
        # Python garbage collection ê°•ì œ ì‹¤í–‰
        gc.collect()
        
        # GPU ìºì‹œ ì •ë¦¬ (PyTorch ë©”ëª¨ë¦¬ í’€ ì •ë¦¬)
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # ë©”ëª¨ë¦¬ í†µê³„ ë¦¬ì…‹ (ë‹¤ë¥¸ ì„œë¹„ìŠ¤ì™€ ê³µìœ  ì‹œ ìœ ìš©)
        try:
            torch.cuda.reset_peak_memory_stats(device_id)
        except:
            pass  # ì¼ë¶€ PyTorch ë²„ì „ì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        
        # force_release ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš° ì¶”ê°€ ì •ë¦¬ ì‹œë„
        if force_release:
            # ì—¬ëŸ¬ ë²ˆ empty_cache í˜¸ì¶œë¡œ ë©”ëª¨ë¦¬ í’€ ê°•ì œ ì •ë¦¬ ì‹œë„
            for _ in range(3):
                gc.collect()
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        
        if logger:
            allocated = torch.cuda.memory_allocated(device_id) / 1024**3  # GB
            reserved = torch.cuda.memory_reserved(device_id) / 1024**3  # GB
            logger.debug(f"TabPFN ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ (CUDA {device_id}) - í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB")
            if reserved > 0.1:  # ì˜ˆì•½ ë©”ëª¨ë¦¬ê°€ 100MB ì´ìƒì´ë©´ ê²½ê³ 
                logger.warning(
                    f"âš ï¸ GPU ë©”ëª¨ë¦¬ ì˜ˆì•½ëŸ‰ì´ {reserved:.2f}GBì…ë‹ˆë‹¤. "
                    f"PyTorchëŠ” ë©”ëª¨ë¦¬ í’€ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ëŠ” ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ì¦‰ì‹œ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                    f"ë‹¤ë¥¸ ì„œë¹„ìŠ¤ì™€ ê°™ì€ GPUë¥¼ ê³µìœ í•˜ëŠ” ê²½ìš° ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
    except Exception as e:
        if logger:
            logger.warning(f"TabPFN ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


def estimate_causal_effect(model, identified_estimand, estimator, logger=None, tabpfn_config=None):
    """ì¸ê³¼íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        model: CausalModel ê°ì²´
        identified_estimand: IdentifiedEstimand ê°ì²´
        estimator: ì¶”ì • ë°©ë²• ì´ë¦„
        logger: ë¡œê±° ê°ì²´ (ì„ íƒì )
        tabpfn_config: TabPFN ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ì„ íƒì )
    """
    if logger:
        logger.info("="*60)
        logger.info("ì¸ê³¼íš¨ê³¼ ì¶”ì • ì‹œì‘")
        logger.info("="*60)
    
    method_map = {
        'linear_regression': 'backdoor.linear_regression',
        'tabpfn': 'backdoor.tabpfn',
        'propensity_score': 'backdoor.propensity_score_stratification',
        'instrumental_variable': 'iv.instrumental_variable'
    }
    
    method = method_map.get(estimator, 'backdoor.linear_regression')
    
    if logger:
        logger.info(f"ì‚¬ìš©í•  ì¶”ì • ë°©ë²•: {method}")
        logger.info(f"ìš”ì²­ëœ ì¶”ì •ê¸°: {estimator}")
    
    print(f"ğŸ“Š ì¸ê³¼íš¨ê³¼ ì¶”ì • ì‹œì‘: {estimator}")
    
    estimate = None
    try:
        # TabPFNì˜ ê²½ìš° ìƒˆ ë²„ì „ ì‚¬ìš© (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)
        if estimator == 'tabpfn':
            # CUDA 0ë²ˆ GPU ì‚¬ìš© (Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œëŠ” í• ë‹¹ëœ GPUê°€ 0ë²ˆìœ¼ë¡œ ë³´ì„)
            import torch
            if torch.cuda.is_available():
                torch.cuda.set_device(0)
            
            # ê¸°ë³¸ TabPFN ì„¤ì • (CUDA 0ë²ˆ ì‚¬ìš©)
            # device_idsë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •í•˜ì—¬ ë‹¨ì¼ GPU ëª¨ë“œ ì‚¬ìš©
            # torch.cuda.set_device(0)ìœ¼ë¡œ ê¸°ë³¸ deviceê°€ 0ë²ˆìœ¼ë¡œ ì„¤ì •ë¨
            default_tabpfn_config = {
                "n_estimators": 8,
                "model_type": "auto",
                "use_multi_gpu": False,
                "device_ids": [],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ = ë‹¨ì¼ GPU ëª¨ë“œ (ê¸°ë³¸ device ì‚¬ìš©, ì¦‰ CUDA 3ë²ˆ)
                "max_num_classes": 10,
                "prediction_batch_size": 64  # ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 64)
            }
            
            # configì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            if tabpfn_config:
                method_params = {**default_tabpfn_config, **tabpfn_config}
                # device_idsëŠ” í•­ìƒ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ê°•ì œ ì„¤ì • (ë‹¨ì¼ GPU ëª¨ë“œ, CUDA 3ë²ˆ ì‚¬ìš©)
                method_params["device_ids"] = []
                method_params["use_multi_gpu"] = False  # ë‹¨ì¼ GPU ëª¨ë“œ
            else:
                method_params = default_tabpfn_config
            
            # device_ids ìë™ ê°ì§€ ë¡œì§ ì œê±° (í•­ìƒ CUDA 0ë²ˆ ì‚¬ìš©)
            
            if logger:
                logger.info("TabPFN ë‹¨ì¼ GPU ëª¨ë“œ ì‚¬ìš© (CUDA 0ë²ˆ)")
                # GPU ìƒíƒœ ë¡œê¹…
                device_id = torch.cuda.current_device()
                device_name = torch.cuda.get_device_name(device_id)
                logger.info(f"ğŸ–¥ï¸ GPU ì •ë³´: {device_name} (cuda:{device_id})")
            
            print("â³ TabPFN ëª¨ë¸ ì¶”ì • ì¤‘... (ì´ ê³¼ì •ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            print(f"   - n_estimators: {method_params.get('n_estimators', 8)}")
            print(f"   - prediction_batch_size: {method_params.get('prediction_batch_size', 64)}")
            
            # Progress bar í‘œì‹œ (TabPFNì€ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ ê°„ë‹¨í•œ progress bar)
            estimate_start_time = time.time()
            with tqdm(total=100, desc="ì¶”ì • ì§„í–‰", unit="%", ncols=100, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}%') as pbar:
                # TabPFN ì¶”ì •ì€ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ progress barëŠ” ëŒ€ëµì ì¸ ì§„í–‰ë¥ ë§Œ í‘œì‹œ
                estimate = model.estimate_effect(
                    identified_estimand,
                    method_name=method,
                    method_params=method_params
                )
                pbar.update(100)  # ì™„ë£Œ ì‹œ 100% í‘œì‹œ
            
            estimate_elapsed = time.time() - estimate_start_time
            print(f"âœ… TabPFN ì¶”ì • ì™„ë£Œ (ì†Œìš” ì‹œê°„: {estimate_elapsed:.2f}ì´ˆ)")
                        
            # ë¡œë“œëœ ëª¨ë¸ì˜ ì‹¤ì œ device í™•ì¸
            if logger and hasattr(estimate, 'estimator'):
                estimator_obj = estimate.estimator
                if hasattr(estimator_obj, '_device'):
                    logger.info(f"ğŸ”§ TabpfnEstimator._device: {estimator_obj._device}")
                if hasattr(estimator_obj, 'tabpfn_model') and estimator_obj.tabpfn_model is not None:
                    tabpfn_model = estimator_obj.tabpfn_model
                    # TabPFNModelWrapperì—ì„œ ë‚´ë¶€ ëª¨ë¸ í™•ì¸
                    if hasattr(tabpfn_model, '_single_model') and tabpfn_model._single_model is not None:
                        inner_model = tabpfn_model._single_model
                        # ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ device í™•ì¸
                        device_info = None
                        try:
                            # ë°©ë²• 1: parameters() ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš° (PyTorch ëª¨ë¸)
                            if hasattr(inner_model, 'parameters'):
                                try:
                                    first_param = next(inner_model.parameters())
                                    device_info = str(first_param.device)
                                except StopIteration:
                                    device_info = "íŒŒë¼ë¯¸í„° ì—†ìŒ"
                            # ë°©ë²• 2: device ì†ì„±ì´ ì§ì ‘ ìˆëŠ” ê²½ìš°
                            elif hasattr(inner_model, 'device'):
                                device_info = str(inner_model.device)
                            # ë°©ë²• 3: ëª¨ë¸ íƒ€ì… í™•ì¸
                            else:
                                model_type = type(inner_model).__name__
                                device_info = f"device ì†ì„± ì—†ìŒ (íƒ€ì…: {model_type})"
                            
                            if device_info:
                                logger.info(f"ğŸ¯ TabPFN ë‚´ë¶€ ëª¨ë¸ device: {device_info}")
                        except Exception as e:
                            logger.info(f"ğŸ¯ TabPFN ë‚´ë¶€ ëª¨ë¸ device í™•ì¸ ì‹¤íŒ¨: {e}")
                    else:
                        logger.info("ğŸ¯ TabPFN _single_model: None (ë©€í‹°í”„ë¡œì„¸ì‹± ëª¨ë“œì´ê±°ë‚˜ ì•„ì§ ë¡œë“œ ì•ˆë¨)")
        else:
            print(f"â³ {estimator} ì¶”ì • ì¤‘...")
            estimate_start_time = time.time()
            with tqdm(total=100, desc="ì¶”ì • ì§„í–‰", unit="%", ncols=100, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}%') as pbar:
                estimate = model.estimate_effect(
                    identified_estimand,
                    method_name=method
                )
                pbar.update(100)  # ì™„ë£Œ ì‹œ 100% í‘œì‹œ
            estimate_elapsed = time.time() - estimate_start_time
            print(f"âœ… {estimator} ì¶”ì • ì™„ë£Œ (ì†Œìš” ì‹œê°„: {estimate_elapsed:.2f}ì´ˆ)")
        
        if logger:
            logger.info("âœ… ì¸ê³¼íš¨ê³¼ ì¶”ì • ì„±ê³µ")
            logger.info(f"ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ (ATE): {estimate.value:.6f}")

        return estimate
        
    except Exception as e:
        # ì‹¤íŒ¨ ì‹œì—ë„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (CUDA 0ë²ˆ)
        if estimator == 'tabpfn':
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.set_device(0)  # CUDA 0ë²ˆìœ¼ë¡œ ì„¤ì •
                    gc.collect()
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    if logger:
                        logger.debug("ì—ëŸ¬ ë°œìƒ í›„ GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ (CUDA 0ë²ˆ)")
            except:
                pass
        
        if logger:
            logger.error(f"âŒ ì¸ê³¼íš¨ê³¼ ì¶”ì • ì‹¤íŒ¨: {e}")
        raise

def extract_significance(estimate):
    """
    CausalEstimate ê°ì²´ì—ì„œ p-valueì™€ confidence_intervalsë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        estimate: CausalEstimate ê°ì²´
    
    Returns:
        tuple: (p_value, confidence_intervals)
    """
    p_value = None
    confidence_intervals = None

    try:
        sig = estimate.test_stat_significance()
        # test_stat_significanceëŠ” dict ë˜ëŠ” dict ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìŒ
        if isinstance(sig, dict):
            p_value = sig.get("p_value")
        elif isinstance(sig, list) and sig:
            first_sig = sig[0]
            if isinstance(first_sig, dict):
                p_value = first_sig.get("p_value")
    except Exception:
        pass
    try:
        # get_confidence_intervalsëŠ” ì—†ëŠ” ê²½ìš° AttributeErrorê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
        confidence_intervals = estimate.get_confidence_intervals()
    except Exception:
        confidence_intervals = getattr(estimate, "confidence_intervals", None)

    return p_value[0], confidence_intervals

def calculate_refutation_pvalue(refutation_result, test_type="placebo", logger=None):
    """
    Refutation í…ŒìŠ¤íŠ¸ ê²°ê³¼ì˜ p-valueë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        refutation_result: CausalRefutation ê°ì²´
        test_type: í…ŒìŠ¤íŠ¸ íƒ€ì… ("placebo", "unobserved", "subset", "dummy")
    
    Returns:
        float: p-value (ê³„ì‚° ë¶ˆê°€ëŠ¥í•œ ê²½ìš° None)
    """
    try:
        log = logger or logging.getLogger(__name__)
        # refutation_resultì—ì„œ refutation_results ì†ì„± í™•ì¸
        if hasattr(refutation_result, 'refutation_results') and refutation_result.refutation_results:
            # refutation_resultsëŠ” ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ
            results = refutation_result.refutation_results
            if isinstance(results, list) and len(results) > 0:
                # ê° ê²°ê³¼ì—ì„œ effect ê°’ ì¶”ì¶œ
                effects = []
                for r in results:
                    if hasattr(r, 'value'):
                        effects.append(r.value)
                    elif isinstance(r, dict) and 'value' in r:
                        effects.append(r['value'])
                
                if len(effects) > 1:
                    # íš¨ê³¼ë“¤ì´ 0ê³¼ ìœ ì˜í•˜ê²Œ ë‹¤ë¥¸ì§€ t-test
                    t_stat, p_value = stats.ttest_1samp(effects, 0)
                    return p_value
        
        # refutation_resultsê°€ ì—†ìœ¼ë©´ new_effectì™€ estimated_effect ë¹„êµ
        if hasattr(refutation_result, 'new_effect') and hasattr(refutation_result, 'estimated_effect'):
            if test_type == "placebo" or test_type == "dummy":
                # new_effectê°€ 0ê³¼ ìœ ì˜í•˜ê²Œ ë‹¤ë¥¸ì§€ (ë‹¨ì¼ ê°’ì´ë¯€ë¡œ ì§ì ‘ ë¹„êµ ë¶ˆê°€)
                # ëŒ€ì‹  new_effectì˜ ì ˆëŒ€ê°’ì´ ì‘ìœ¼ë©´ í†µê³¼ë¡œ ê°„ì£¼
                return None
            elif test_type == "unobserved" or test_type == "subset":
                # new_effectì™€ estimated_effectê°€ ìœ ì˜í•˜ê²Œ ë‹¤ë¥¸ì§€
                # ë‹¨ì¼ ê°’ì´ë¯€ë¡œ ì§ì ‘ t-test ë¶ˆê°€, ì°¨ì´ì˜ ì ˆëŒ€ê°’ìœ¼ë¡œ íŒë‹¨
                return None
        
        return None
    except Exception as e:
        log.error(f"calculate_refutation_pvalue ì‹¤íŒ¨: {e}")
        return None


def run_validation_tests(model, identified_estimand, estimate, logger=None):
    """ê²€ì¦ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ (4ê°œ í…ŒìŠ¤íŠ¸ ëª¨ë‘ í¬í•¨)"""
    if logger:
        logger.info("="*60)
        logger.info("ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘ (4ê°œ í…ŒìŠ¤íŠ¸)")
        logger.info("="*60)
    
    validation_results = {}
    
    # 1. ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ (Placebo Treatment)
    print("1ï¸âƒ£ ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    if logger:
        logger.info("1ï¸âƒ£ ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    try:
        refute_placebo = model.refute_estimate(
            identified_estimand, estimate,
            method_name="placebo_treatment_refuter",
            placebo_type="permute",
            num_simulations=20
        )
        validation_results['placebo'] = refute_placebo
        
        effect_change = abs(refute_placebo.new_effect - refute_placebo.estimated_effect)
        p_value = calculate_refutation_pvalue(refute_placebo, "placebo")
        status = "í†µê³¼" if effect_change < 0.01 else "ì‹¤íŒ¨"
        
        print(f"âœ… ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {status}")
        print(f"   ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_placebo.estimated_effect:.6f}")
        print(f"   ê°€ìƒì²˜ì¹˜ í›„ ì¶”ì •ì¹˜: {refute_placebo.new_effect:.6f}")
        print(f"   íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
        if p_value is not None:
            print(f"   P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
        
        if logger:
            logger.info("âœ… ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {status}")
            logger.info(f"ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_placebo.estimated_effect:.6f}")
            logger.info(f"ê°€ìƒì²˜ì¹˜ í›„ ì¶”ì •ì¹˜: {refute_placebo.new_effect:.6f}")
            logger.info(f"íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
            if p_value is not None:
                logger.info(f"P-value: {p_value:.6f}")
                logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
            
    except Exception as e:
        validation_results['placebo'] = None
        print(f"âŒ ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        if logger:
            logger.error(f"âŒ ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 2. ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ (Add Unobserved Common Cause)
    print("2ï¸âƒ£ ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    if logger:
        logger.info("2ï¸âƒ£ ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    try:
        refute_unobserved = model.refute_estimate(
            identified_estimand, estimate,
            method_name="add_unobserved_common_cause",
            confounders_effect_on_treatment="binary_flip",
            confounders_effect_on_outcome="linear",
            effect_strength_on_treatment=0.10,
            effect_strength_on_outcome=0.10,
            num_simulations=20
        )
        validation_results['unobserved'] = refute_unobserved
        
        change_rate = abs(refute_unobserved.new_effect - refute_unobserved.estimated_effect) / abs(refute_unobserved.estimated_effect) if abs(refute_unobserved.estimated_effect) > 0 else float('inf')
        p_value = calculate_refutation_pvalue(refute_unobserved, "unobserved")
        status = "ê°•ê±´í•¨" if change_rate < 0.2 else "ë¯¼ê°í•¨"
        
        print(f"âœ… ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {status}")
        print(f"   ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_unobserved.estimated_effect:.6f}")
        print(f"   êµë€ ì¶”ê°€ í›„ ì¶”ì •ì¹˜: {refute_unobserved.new_effect:.6f}")
        print(f"   ë³€í™”ìœ¨: {change_rate:.2%}")
        if p_value is not None:
            print(f"   P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
        
        if logger:
            logger.info("âœ… ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {status}")
            logger.info(f"ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_unobserved.estimated_effect:.6f}")
            logger.info(f"êµë€ ì¶”ê°€ í›„ ì¶”ì •ì¹˜: {refute_unobserved.new_effect:.6f}")
            logger.info(f"ë³€í™”ìœ¨: {change_rate:.2%}")
            if p_value is not None:
                logger.info(f"P-value: {p_value:.6f}")
                logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
            
    except Exception as e:
        validation_results['unobserved'] = None
        print(f"âŒ ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        if logger:
            logger.error(f"âŒ ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 3. ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (Data Subset)
    print("3ï¸âƒ£ ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    if logger:
        logger.info("3ï¸âƒ£ ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    try:
        refute_subset = model.refute_estimate(
            identified_estimand, estimate,
            method_name="data_subset_refuter",
            subset_fraction=0.8,  # 80% ì„œë¸Œì…‹ ì‚¬ìš©
            num_simulations=20
        )
        validation_results['subset'] = refute_subset
        
        effect_change = abs(refute_subset.new_effect - refute_subset.estimated_effect)
        change_rate = abs(refute_subset.estimated_effect) > 0 and abs(effect_change / refute_subset.estimated_effect) or float('inf')
        p_value = calculate_refutation_pvalue(refute_subset, "subset")
        status = "í†µê³¼" if change_rate < 0.1 else "ì‹¤íŒ¨"  # 10% ì´ë‚´ ë³€í™”ë©´ í†µê³¼
        
        print(f"âœ… ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {status}")
        print(f"   ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_subset.estimated_effect:.6f}")
        print(f"   ë¶€ë¶„í‘œë³¸ ì¶”ì •ì¹˜: {refute_subset.new_effect:.6f}")
        print(f"   íš¨ê³¼ ë³€í™”: {effect_change:.6f} ({change_rate:.2%})")
        if p_value is not None:
            print(f"   P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
        
        if logger:
            logger.info("âœ… ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {status}")
            logger.info(f"ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_subset.estimated_effect:.6f}")
            logger.info(f"ë¶€ë¶„í‘œë³¸ ì¶”ì •ì¹˜: {refute_subset.new_effect:.6f}")
            logger.info(f"íš¨ê³¼ ë³€í™”: {effect_change:.6f} ({change_rate:.2%})")
            if p_value is not None:
                logger.info(f"P-value: {p_value:.6f}")
                logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
            
    except Exception as e:
        validation_results['subset'] = None
        print(f"âŒ ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        if logger:
            logger.error(f"âŒ ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 4. ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ (Dummy Outcome)
    print("4ï¸âƒ£ ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    if logger:
        logger.info("4ï¸âƒ£ ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    try:
        refute_dummys = model.refute_estimate(
            identified_estimand, estimate,
            method_name="dummy_outcome_refuter",
            num_simulations=20
        )
        refute_dummy = refute_dummys[0]
        validation_results['dummy'] = refute_dummy
        
        # for dummy in refute_dummy:
        #     logger.info(f"refute_dummy ê²°ê³¼1: {dummy}")
        #     logger.info(f"refute_dummy ê²°ê³¼2: {dir(dummy)}")
        p_value = calculate_refutation_pvalue(refute_dummy, "dummy", logger)
        # new_effectê°€ 0ì— ê°€ê¹Œìš°ë©´ í†µê³¼ (0.01 ì´í•˜)
        status = "í†µê³¼" if abs(refute_dummy.new_effect) < 0.01 else "ì‹¤íŒ¨"
        
        print(f"âœ… ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {status}")
        print(f"   ë”ë¯¸ ê²°ê³¼ ì¶”ì •ì¹˜: {refute_dummy.new_effect:.6f}")
        print(f"   (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ, 0.01 ì´í•˜ë©´ í†µê³¼)")
        if p_value is not None:
            print(f"   P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
        
        if logger:
            logger.info("âœ… ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {status}")
            logger.info(f"ë”ë¯¸ ê²°ê³¼ ì¶”ì •ì¹˜: {refute_dummy.new_effect:.6f}")
            logger.info(f"(0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ, 0.01 ì´í•˜ë©´ í†µê³¼)")
            if p_value is not None:
                logger.info(f"P-value: {p_value:.6f}")
                logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
            
    except Exception as e:
        validation_results['dummy'] = None
        print(f"âŒ ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        if logger:
            logger.error(f"âŒ ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print("="*60)
    print("ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (4ê°œ í…ŒìŠ¤íŠ¸)")
    print("="*60)
    
    if logger:
        logger.info("="*60)
        logger.info("ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (4ê°œ í…ŒìŠ¤íŠ¸)")
        logger.info("="*60)
        logger.info("ê²€ì¦ ê²°ê³¼ ìš”ì•½")
        logger.info("="*60)
        
        # ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸
        if validation_results.get('placebo'):
            placebo = validation_results['placebo']
            effect_change = abs(placebo.new_effect - placebo.estimated_effect)
            p_value = calculate_refutation_pvalue(placebo, "placebo")
            status = "í†µê³¼" if effect_change < 0.01 else "ì‹¤íŒ¨"
            logger.info(f"ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸: {status}")
            logger.info(f"  - ê¸°ì¡´ ì¶”ì •ì¹˜: {placebo.estimated_effect:.6f}")
            logger.info(f"  - ê°€ìƒì²˜ì¹˜ í›„ ì¶”ì •ì¹˜: {placebo.new_effect:.6f}")
            logger.info(f"  - íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
            if p_value is not None:
                logger.info(f"  - P-value: {p_value:.6f}")
                logger.info(f"  - í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
        
        # ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸
        if validation_results.get('unobserved'):
            unobserved = validation_results['unobserved']
            change_rate = abs(unobserved.new_effect - unobserved.estimated_effect) / abs(unobserved.estimated_effect) if abs(unobserved.estimated_effect) > 0 else float('inf')
            p_value = calculate_refutation_pvalue(unobserved, "unobserved")
            status = "ê°•ê±´í•¨" if change_rate < 0.2 else "ë¯¼ê°í•¨"
            logger.info(f"ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸: {status}")
            logger.info(f"  - ê¸°ì¡´ ì¶”ì •ì¹˜: {unobserved.estimated_effect:.6f}")
            logger.info(f"  - êµë€ ì¶”ê°€ í›„ ì¶”ì •ì¹˜: {unobserved.new_effect:.6f}")
            logger.info(f"  - ë³€í™”ìœ¨: {change_rate:.2%}")
            if p_value is not None:
                logger.info(f"  - P-value: {p_value:.6f}")
                logger.info(f"  - í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
        
        # ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
        if validation_results.get('subset'):
            subset = validation_results['subset']
            effect_change = abs(subset.new_effect - subset.estimated_effect)
            p_value = calculate_refutation_pvalue(subset, "subset")
            change_rate = abs(subset.estimated_effect) > 0 and abs(effect_change / subset.estimated_effect) or float('inf')
            status = "í†µê³¼" if change_rate < 0.1 else "ì‹¤íŒ¨"
            logger.info(f"ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸: {status}")
            logger.info(f"  - ê¸°ì¡´ ì¶”ì •ì¹˜: {subset.estimated_effect:.6f}")
            logger.info(f"  - ë¶€ë¶„í‘œë³¸ ì¶”ì •ì¹˜: {subset.new_effect:.6f}")
            logger.info(f"  - íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
            if p_value is not None:
                logger.info(f"  - P-value: {p_value:.6f}")
                logger.info(f"  - í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
        
        # ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸
        if validation_results.get('dummy'):
            dummy = validation_results['dummy']
            p_value = calculate_refutation_pvalue(dummy, "dummy")
            status = "í†µê³¼" if abs(dummy.new_effect) < 0.01 else "ì‹¤íŒ¨"
            logger.info(f"ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸: {status}")
            logger.info(f"  - ë”ë¯¸ ê²°ê³¼ ì¶”ì •ì¹˜: {dummy.new_effect:.6f}")
            if p_value is not None:
                logger.info(f"  - P-value: {p_value:.6f}")
                logger.info(f"  - í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    
    return validation_results

def run_sensitivity_analysis(model, identified_estimand, estimate, logger=None):
    """ë¯¼ê°ë„ ë¶„ì„ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜"""
    if logger:
        logger.info("="*60)
        logger.info("ë¯¼ê°ë„ ë¶„ì„ ì‹¤í–‰ ì‹œì‘")
        logger.info("="*60)
        logger.info("íš¨ê³¼ ê°•ë„ ë²”ìœ„: 0.0 ~ 0.5")
        logger.info("ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ ìˆ˜: 5x5 = 25ê°œ")
        logger.info("ì‹œë®¬ë ˆì´ì…˜ ìˆ˜: 50íšŒ")
    
    try:
        grid = np.linspace(0.0, 0.5, 5)
        rows = []
        total_combinations = len(grid) * len(grid)
        processed = 0
        
        if logger:
            logger.info(f"ì´ {total_combinations}ê°œ ì¡°í•© ë¶„ì„ ì‹œì‘...")
        
        for i, et in enumerate(grid):
            for j, eo in enumerate(grid):
                processed += 1
                if logger and processed % 20 == 0:
                    logger.info(f"ì§„í–‰ë¥ : {processed}/{total_combinations} ({processed/total_combinations*100:.1f}%)")
                
                try:
                    ref = model.refute_estimate(
                        identified_estimand, estimate,
                        method_name="add_unobserved_common_cause",
                        confounders_effect_on_treatment="binary_flip",
                        confounders_effect_on_outcome="linear",
                        effect_strength_on_treatment=et,
                        effect_strength_on_outcome=eo,
                        num_simulations=50
                    )
                    rows.append((et, eo, ref.new_effect))
                except Exception as e:
                    rows.append((et, eo, np.nan))
                    if logger:
                        logger.warning(f"ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ ({et:.2f}, {eo:.2f}) ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        sensitivity_df = pd.DataFrame(rows, columns=[
            "effect_strength_on_treatment", 
            "effect_strength_on_outcome", 
            "new_effect"
        ])
        
        if logger:
            logger.info("âœ… ë¯¼ê°ë„ ë¶„ì„ ì™„ë£Œ")
            logger.info(f"ë¶„ì„ëœ ì¡°í•© ìˆ˜: {len(sensitivity_df)}")
            
            if not sensitivity_df.empty:
                valid_effects = sensitivity_df.dropna()
                logger.info(f"ìœ íš¨í•œ ê²°ê³¼ ìˆ˜: {len(valid_effects)}")
                logger.info(f"íš¨ê³¼ ë²”ìœ„: {valid_effects['new_effect'].min():.6f} ~ {valid_effects['new_effect'].max():.6f}")
                
                # íš¨ê³¼ê°€ 0ì— ê°€ê¹Œìš´ ì§€ì  ì°¾ê¸°
                min_abs_effect = valid_effects.loc[valid_effects['new_effect'].abs().idxmin()]
                logger.info(f"ìµœì†Œ ì ˆëŒ€ íš¨ê³¼ ì§€ì : et={min_abs_effect['effect_strength_on_treatment']:.2f}, eo={min_abs_effect['effect_strength_on_outcome']:.2f}")
                logger.info(f"ìµœì†Œ ì ˆëŒ€ íš¨ê³¼ê°’: {min_abs_effect['new_effect']:.6f}")
                
                # ìŒìˆ˜ íš¨ê³¼ ë¹„ìœ¨
                negative_effects = len(valid_effects[valid_effects['new_effect'] < 0])
                logger.info(f"ìŒìˆ˜ íš¨ê³¼ ì¡°í•©: {negative_effects}ê°œ ({negative_effects/len(valid_effects)*100:.1f}%)")
        
        return sensitivity_df
        
    except Exception as e:
        if logger:
            logger.error(f"âŒ ë¯¼ê°ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def create_sensitivity_heatmap(sensitivity_df, logger=None):
    """ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜"""
    if logger:
        logger.info("="*60)
        logger.info("íˆíŠ¸ë§µ ìƒì„± ì‹œì‘")
        logger.info("="*60)
    
    if sensitivity_df.empty:
        if logger:
            logger.warning("âŒ ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆì–´ íˆíŠ¸ë§µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        if logger:
            logger.info("í”¼ë²— í…Œì´ë¸” ìƒì„± ì¤‘...")
        
        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        pivot = sensitivity_df.pivot(
            index="effect_strength_on_treatment",
            columns="effect_strength_on_outcome",
            values="new_effect"
        ).sort_index(ascending=True)
        
        if logger:
            logger.info(f"í”¼ë²— í…Œì´ë¸” í¬ê¸°: {pivot.shape}")
            logger.info("íˆíŠ¸ë§µ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # íˆíŠ¸ë§µ ìƒì„±
        fig, ax = plt.subplots(figsize=(10, 8), dpi=100)
        
        im = ax.imshow(
            pivot.values,
            origin="lower",
            aspect="auto",
            extent=[
                pivot.columns.min(), pivot.columns.max(),
                pivot.index.min(), pivot.index.max()
            ],
            cmap='RdYlBu_r'
        )
        
        # ìƒ‰ìƒë§‰ëŒ€ ì¶”ê°€
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label("New Effect (after unobserved confounding)", fontsize=12)
        
        # 0-ì»¨íˆ¬ì–´ ë¼ì¸ ì¶”ê°€
        X, Y = np.meshgrid(pivot.columns.values, pivot.index.values)
        CS = ax.contour(X, Y, pivot.values, levels=[0.0], linewidths=2, colors='black')
        ax.clabel(CS, inline=True, fmt="effect=0", fontsize=10)
        
        # ì¶• ë ˆì´ë¸” ë° ì œëª©
        ax.set_xlabel("Effect Strength on Outcome (eo)", fontsize=12)
        ax.set_ylabel("Effect Strength on Treatment (et)", fontsize=12)
        ax.set_title("Sensitivity Analysis: Effect of Unobserved Confounders", fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        if logger:
            logger.info("íˆíŠ¸ë§µ ì €ì¥ ì¤‘...")
        
        # ê·¸ë¦¼ ì €ì¥
        script_dir = Path(__file__).parent.parent
        log_dir = script_dir / "log"
        log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"sensitivity_heatmap_{timestamp}.png"
        output_path = log_dir / filename
        
        plt.savefig(output_path, dpi=100, bbox_inches='tight')
        
        if logger:
            logger.info("âœ… íˆíŠ¸ë§µ ìƒì„± ì„±ê³µ")
            logger.info(f"ì €ì¥ ê²½ë¡œ: {output_path}")
            
            # íŒŒì¼ ì •ë³´
            if os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                logger.info(f"íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
                logger.info(f"ì´ë¯¸ì§€ í•´ìƒë„: 10x8 inches, DPI: 100")
        
        return output_path
        
    except Exception as e:
        if logger:
            logger.error(f"âŒ íˆíŠ¸ë§µ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def print_summary_report(estimate, validation_results, sensitivity_df):
    """
    ì „ì²´ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        estimate: ì¶”ì •ëœ ì¸ê³¼íš¨ê³¼ ê°ì²´
        validation_results (dict): ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        sensitivity_df (pd.DataFrame): ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼
    """
    print("\n" + "="*80)
    print("ğŸ“‹ ìµœì¢… ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ")
    print("="*80)
    
    # ê¸°ë³¸ ì¶”ì • ê²°ê³¼
    print(f"\nğŸ¯ ì£¼ìš” ì¶”ì • ê²°ê³¼:")
    print(f"  - ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ (ATE): {estimate.value:.6f}")
    if hasattr(estimate, 'p_value') and estimate.p_value is not None:
        significance = "ìœ ì˜í•¨" if estimate.p_value <= 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
        print(f"  - í†µê³„ì  ìœ ì˜ì„±: {significance} (p-value: {estimate.p_value:.6f})")
    
    # ê²€ì¦ ê²°ê³¼ ìš”ì•½ (4ê°œ í…ŒìŠ¤íŠ¸)
    print(f"\nğŸ”¬ ê²€ì¦ ê²°ê³¼ ìš”ì•½ (4ê°œ í…ŒìŠ¤íŠ¸):")
    
    if validation_results.get('placebo'):
        placebo = validation_results['placebo']
        effect_change = abs(placebo.new_effect - placebo.estimated_effect)
        p_value = calculate_refutation_pvalue(placebo, "placebo")
        status = "í†µê³¼" if effect_change < 0.01 else "ì‹¤íŒ¨"
        print(f"  1. ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸: {status}")
        print(f"     - íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
        if p_value is not None:
            print(f"     - P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
    else:
        print(f"  1. ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸: ì‹¤í–‰ ì‹¤íŒ¨")
    
    if validation_results.get('unobserved'):
        unobserved = validation_results['unobserved']
        change_rate = abs(unobserved.new_effect - unobserved.estimated_effect) / abs(unobserved.estimated_effect) if abs(unobserved.estimated_effect) > 0 else float('inf')
        p_value = calculate_refutation_pvalue(unobserved, "unobserved")
        status = "ê°•ê±´í•¨" if change_rate < 0.2 else "ë¯¼ê°í•¨"
        print(f"  2. ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸: {status}")
        print(f"     - ë³€í™”ìœ¨: {change_rate:.2%}")
        if p_value is not None:
            print(f"     - P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
    else:
        print(f"  2. ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸: ì‹¤í–‰ ì‹¤íŒ¨")
    
    if validation_results.get('subset'):
        subset = validation_results['subset']
        effect_change = abs(subset.new_effect - subset.estimated_effect)
        change_rate = abs(subset.estimated_effect) > 0 and abs(effect_change / subset.estimated_effect) or float('inf')
        p_value = calculate_refutation_pvalue(subset, "subset")
        status = "í†µê³¼" if change_rate < 0.1 else "ì‹¤íŒ¨"
        print(f"  3. ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸: {status}")
        print(f"     - íš¨ê³¼ ë³€í™”ìœ¨: {change_rate:.2%}")
        if p_value is not None:
            print(f"     - P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
    else:
        print(f"  3. ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸: ì‹¤í–‰ ì‹¤íŒ¨")
    
    if validation_results.get('dummy'):
        dummy = validation_results['dummy']
        p_value = calculate_refutation_pvalue(dummy, "dummy")
        status = "í†µê³¼" if abs(dummy.new_effect) < 0.01 else "ì‹¤íŒ¨"
        print(f"  4. ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸: {status}")
        print(f"     - ë”ë¯¸ ê²°ê³¼ ì¶”ì •ì¹˜: {dummy.new_effect:.6f}")
        if p_value is not None:
            print(f"     - P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
    else:
        print(f"  4. ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸: ì‹¤í–‰ ì‹¤íŒ¨")
    
    # ë¯¼ê°ë„ ë¶„ì„ ìš”ì•½
    if not sensitivity_df.empty:
        print(f"\nğŸ“ˆ ë¯¼ê°ë„ ë¶„ì„ ìš”ì•½:")
        print(f"  - ë¶„ì„ëœ ì¡°í•© ìˆ˜: {len(sensitivity_df)}")
        print(f"  - íš¨ê³¼ ë²”ìœ„: {sensitivity_df['new_effect'].min():.6f} ~ {sensitivity_df['new_effect'].max():.6f}")
        
        # íš¨ê³¼ê°€ 0ì— ê°€ê¹Œìš´ ì§€ì  ì°¾ê¸°
        min_abs_effect = sensitivity_df.loc[sensitivity_df['new_effect'].abs().idxmin()]
        print(f"  - ìµœì†Œ ì ˆëŒ€ íš¨ê³¼ ì§€ì : et={min_abs_effect['effect_strength_on_treatment']:.2f}, eo={min_abs_effect['effect_strength_on_outcome']:.2f}")
    
    print(f"\nâœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ!")


# ============================================================================
# Checkpoint ì €ì¥/ë¡œë“œ í•¨ìˆ˜
# ============================================================================

def _json_default(obj):
    """JSON ì§ë ¬í™” ë³´ì¡°: numpy/pandas ê°ì²´ë¥¼ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
    if isinstance(obj, (np.bool_,)):
        return bool(obj)
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, (np.floating,)):
        return float(obj)
    if hasattr(obj, "isoformat"):  # datetime, Timestamp ë“±
        return obj.isoformat()
    return str(obj)


def save_checkpoint(estimate, checkpoint_dir, experiment_id, graph_name=None, logger=None):
    """
    CausalEstimate ê°ì²´ë¥¼ checkpointë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        estimate: CausalEstimate ê°ì²´
        checkpoint_dir (str or Path): checkpoint ì €ì¥ ë””ë ‰í† ë¦¬
        experiment_id (str): ì‹¤í—˜ ID (íŒŒì¼ëª…ì— ì‚¬ìš©)
        graph_name (str, optional): ê·¸ë˜í”„ íŒŒì¼ëª… (metadataì— ì €ì¥)
        logger: ë¡œê±° ê°ì²´
    
    Returns:
        str: ì €ì¥ëœ checkpoint íŒŒì¼ ê²½ë¡œ
    """
    checkpoint_path = Path(checkpoint_dir)
    checkpoint_path.mkdir(parents=True, exist_ok=True)
    
    # checkpoint íŒŒì¼ëª… ìƒì„±
    checkpoint_filename = f"checkpoint_{experiment_id}.pkl"
    checkpoint_file = checkpoint_path / checkpoint_filename
    
    # experiment_idì—ì„œ graph_name ì¶”ì¶œ (ì—†ìœ¼ë©´ ì „ë‹¬ë°›ì€ ê°’ ì‚¬ìš©)
    if graph_name is None and experiment_id:
        # experiment_id í˜•ì‹: exp_0001_graph_name_treatment_outcome_estimator
        parts = experiment_id.split('_')
        if len(parts) >= 3:
            graph_name = parts[2]  # graph_name ìœ„ì¹˜
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥ (numpy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜)
    metadata = {
        "experiment_id": experiment_id,
        "graph_name": graph_name,
        "treatment": estimate._treatment_name[0] if isinstance(estimate._treatment_name, list) else estimate._treatment_name,
        "outcome": estimate._outcome_name[0] if isinstance(estimate._outcome_name, list) else estimate._outcome_name,
        "ate_value": float(estimate.value) if estimate.value is not None else None,
        "control_value": float(estimate.control_value) if estimate.control_value is not None else None,
        "treatment_value": float(estimate.treatment_value) if estimate.treatment_value is not None else None,
        "estimator_type": type(estimate.estimator).__name__ if hasattr(estimate, 'estimator') else None,
        "saved_at": datetime.now().isoformat()
    }
    
    metadata_filename = f"metadata_{experiment_id}.json"
    metadata_file = checkpoint_path / metadata_filename
    
    try:
        # pickle ë¶ˆê°€ëŠ¥í•œ BootstrapEstimates ì œê±°
        if hasattr(estimate.estimator, '_bootstrap_estimates'):
            estimate.estimator._bootstrap_estimates = None

        # CausalEstimate ê°ì²´ ì €ì¥
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(estimate, f)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False, default=_json_default)
        
        if logger:
            logger.info(f"âœ… Checkpoint ì €ì¥ ì™„ë£Œ: {checkpoint_file}")
        print(f"âœ… Checkpoint ì €ì¥ ì™„ë£Œ: {checkpoint_file}")
        
        return str(checkpoint_file)
        
    except Exception as e:
        error_msg = f"Checkpoint ì €ì¥ ì‹¤íŒ¨: {e}"
        if logger:
            logger.error(error_msg)
        print(f"âŒ {error_msg}")
        raise


def load_checkpoint(checkpoint_file, logger=None):
    """
    Checkpointì—ì„œ CausalEstimate ê°ì²´ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        checkpoint_file (str or Path): checkpoint íŒŒì¼ ê²½ë¡œ
        logger: ë¡œê±° ê°ì²´
    
    Returns:
        CausalEstimate: ë¡œë“œëœ CausalEstimate ê°ì²´
    """
    checkpoint_path = Path(checkpoint_file)
    
    if not checkpoint_path.exists():
        error_msg = f"Checkpoint íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_file}"
        if logger:
            logger.error(error_msg)
        raise FileNotFoundError(error_msg)
    
    try:
        with open(checkpoint_path, 'rb') as f:
            estimate = pickle.load(f)
        
        if logger:
            logger.info(f"âœ… Checkpoint ë¡œë“œ ì™„ë£Œ: {checkpoint_file}")
        print(f"âœ… Checkpoint ë¡œë“œ ì™„ë£Œ: {checkpoint_file}")
        
        return estimate
        
    except Exception as e:
        error_msg = f"Checkpoint ë¡œë“œ ì‹¤íŒ¨: {e}"
        if logger:
            logger.error(error_msg)
        print(f"âŒ {error_msg}")
        raise


def find_checkpoint(checkpoint_dir, graph_name, treatment, outcome, estimator, logger=None):
    """
    ì£¼ì–´ì§„ ì¡°ê±´ì— ë§ëŠ” checkpoint íŒŒì¼ì„ ì°¾ëŠ” í•¨ìˆ˜
    
    Args:
        checkpoint_dir (str or Path): checkpoint ë””ë ‰í† ë¦¬
        graph_name (str): ê·¸ë˜í”„ íŒŒì¼ëª…
        treatment (str): ì²˜ì¹˜ ë³€ìˆ˜ëª…
        outcome (str): ê²°ê³¼ ë³€ìˆ˜ëª…
        estimator (str): ì¶”ì • ë°©ë²•
        logger: ë¡œê±° ê°ì²´
    
    Returns:
        str or None: ì°¾ì€ checkpoint íŒŒì¼ ê²½ë¡œ, ì—†ìœ¼ë©´ None
    """
    checkpoint_path = Path(checkpoint_dir)
    
    if not checkpoint_path.exists():
        if logger:
            logger.warning(f"Checkpoint ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {checkpoint_dir}")
        return None
    
    # metadata íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ì¡°ê±´ì— ë§ëŠ” checkpoint ì°¾ê¸°
    metadata_files = list(checkpoint_path.glob("metadata_*.json"))
    
    for metadata_file in metadata_files:
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # ì¡°ê±´ í™•ì¸: graph_name, treatment, outcome, estimator ëª¨ë‘ ì¼ì¹˜í•´ì•¼ í•¨
            metadata_graph = metadata.get("graph_name", "")
            metadata_treatment = metadata.get("treatment", "")
            metadata_outcome = metadata.get("outcome", "")
            metadata_estimator = metadata.get("estimator_type", "").lower().replace("estimator", "")
            target_estimator = estimator.lower().replace("_", "")
            
            if (metadata_graph == graph_name and
                metadata_treatment == treatment and 
                metadata_outcome == outcome and
                metadata_estimator == target_estimator):
                
                # experiment_idì—ì„œ checkpoint íŒŒì¼ëª… ìƒì„±
                experiment_id = metadata.get("experiment_id", "")
                checkpoint_file = checkpoint_path / f"checkpoint_{experiment_id}.pkl"
                
                if checkpoint_file.exists():
                    if logger:
                        logger.info(f"âœ… Checkpoint ë°œê²¬: {checkpoint_file}")
                    return str(checkpoint_file)
                    
        except Exception as e:
            if logger:
                logger.warning(f"Metadata íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({metadata_file}): {e}")
            continue
    
    if logger:
        logger.warning(f"ì¡°ê±´ì— ë§ëŠ” checkpointë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: graph={graph_name}, treatment={treatment}, outcome={outcome}, estimator={estimator}")
    return None


# ============================================================================
# ì‹¤í—˜ ê´€ë¦¬ í•¨ìˆ˜
# ============================================================================

def _get_graph_files(
    config: Dict[str, Any],
    data_dir_path: Path,
    graph_data_dir: str
) -> List[str]:
    """
    ì„¤ì •ì— ë”°ë¼ ê·¸ë˜í”„ íŒŒì¼ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        data_dir_path: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        graph_data_dir: ê·¸ë˜í”„ ë°ì´í„° ë””ë ‰í† ë¦¬ëª…
    
    Returns:
        ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    from . import utils
    graphs = config.get("graphs", [])
    auto_extract_treatments = config.get("auto_extract_treatments", False)
    
    if auto_extract_treatments:
        found_graphs = utils.find_all_graph_files(data_dir_path, graph_data_dir)
        return [str(g) for g in found_graphs]
    
    graph_files = []
    for graph in graphs:
        if isinstance(graph, str):
            graph_path = data_dir_path / graph_data_dir / graph
            if graph_path.exists():
                graph_files.append(str(graph_path))
            else:
                graph_path = Path(graph)
                if graph_path.exists():
                    graph_files.append(str(graph_path))
    
    return graph_files


def _extract_treatments_from_graphs(
    graph_files: List[str],
    auto_extract: bool
) -> Tuple[Dict[str, List[str]], Dict[str, str]]:
    """
    ê·¸ë˜í”„ íŒŒì¼ë“¤ì—ì„œ treatmentì™€ outcomeì„ ì¶”ì¶œí•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
    
    Args:
        graph_files: ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        auto_extract: ìë™ ì¶”ì¶œ ì—¬ë¶€
    
    Returns:
        (graph_treatments_map, graph_outcomes_map) íŠœí”Œ
    """
    from . import utils
    graph_treatments_map = {}
    graph_outcomes_map = {}
    
    if auto_extract:
        for graph_file in graph_files:
            graph_path = Path(graph_file)
            extracted_treatments = utils.extract_treatments_from_graph(graph_path)
            
            if extracted_treatments:
                graph_treatments_map[graph_file] = [
                    t["treatment_var"] for t in extracted_treatments 
                    if t.get("treatment_var")
                ]
                if extracted_treatments[0].get("outcome"):
                    graph_outcomes_map[graph_file] = extracted_treatments[0]["outcome"]
    
    return graph_treatments_map, graph_outcomes_map


def _sort_estimators(estimators: List[str]) -> List[str]:
    """
    estimator ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜ (linear_regression, tabpfn ìš°ì„ )
    
    Args:
        estimators: estimator ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì •ë ¬ëœ estimator ë¦¬ìŠ¤íŠ¸
    """
    sorted_estimators = []
    priority_estimators = ["linear_regression", "tabpfn"]
    
    for est in priority_estimators:
        if est in estimators:
            sorted_estimators.append(est)
    
    for est in estimators:
        if est not in sorted_estimators:
            sorted_estimators.append(est)
    
    return sorted_estimators


def create_experiment_list(
    config: Dict[str, Any],
    data_dir_path: Path,
    graph_data_dir: str
) -> List[Tuple[str, str, str, str]]:
    """
    config.jsonì—ì„œ experiment_listë¥¼ ì½ì–´ì„œ ì‹¤í—˜ ì¡°í•© ë¦¬ìŠ¤íŠ¸ ìƒì„±
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        data_dir_path: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        graph_data_dir: ê·¸ë˜í”„ ë°ì´í„° ë””ë ‰í† ë¦¬ëª…
    
    Returns:
        ì‹¤í—˜ ì¡°í•© ë¦¬ìŠ¤íŠ¸ [(graph_file, treatment, outcome, estimator), ...]
    """
    # config.jsonì— experiment_listê°€ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    experiment_list_config = config.get("experiment_list", [])
    
    if experiment_list_config:
        # config.jsonì—ì„œ ì§ì ‘ ì •ì˜ëœ experiment_list ì‚¬ìš©
        experiment_combinations = []
        graph_data_path = data_dir_path / graph_data_dir
        
        for exp in experiment_list_config:
            if isinstance(exp, list) and len(exp) >= 4:
                # ë°°ì—´ í˜•ì‹: ["graph_1.dot", "BFR_OCTR_CT", "ACQ_180_YN", "tabpfn"]
                graph_name, treatment, outcome, estimator = exp[0], exp[1], exp[2], exp[3]
            elif isinstance(exp, dict):
                # ë”•ì…”ë„ˆë¦¬ í˜•ì‹: {"graph": "graph_1.dot", "treatment": "BFR_OCTR_CT", ...}
                graph_name = exp.get("graph", "")
                treatment = exp.get("treatment", "")
                outcome = exp.get("outcome", "ACQ_180_YN")
                estimator = exp.get("estimator", "tabpfn")
            else:
                print(f"âš ï¸ ì˜ëª»ëœ experiment_list í˜•ì‹: {exp}")
                continue
            
            # ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ í™•ì¸
            graph_path = graph_data_path / graph_name
            if not graph_path.exists():
                # ì ˆëŒ€ ê²½ë¡œë¡œ ì‹œë„
                graph_path = Path(graph_name)
                if not graph_path.exists():
                    print(f"âš ï¸ ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_name}")
                    continue
            
            experiment_combinations.append(
                (str(graph_path), treatment, outcome, estimator)
            )
        
        return experiment_combinations
    
    # ê¸°ì¡´ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
    treatments = config.get("treatments", [])
    outcomes = config.get("outcomes", ["ACQ_180_YN"])
    estimators = config.get("estimators", ["tabpfn"])
    auto_extract_treatments = config.get("auto_extract_treatments", False)
    
    # ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
    graph_files = _get_graph_files(config, data_dir_path, graph_data_dir)
    
    if not graph_files:
        return []
    
    # treatment ìë™ ì¶”ì¶œ
    graph_treatments_map, graph_outcomes_map = _extract_treatments_from_graphs(
        graph_files, auto_extract_treatments
    )
    
    # estimator ì •ë ¬
    sorted_estimators = _sort_estimators(estimators)
    
    # ì‹¤í—˜ ì¡°í•© ìƒì„±
    if auto_extract_treatments and graph_treatments_map:
        experiment_combinations = []
        for graph_file in graph_files:
            graph_treatments = graph_treatments_map.get(graph_file, treatments)
            graph_outcome = graph_outcomes_map.get(
                graph_file, 
                outcomes[0] if outcomes else "ACQ_180_YN"
            )
            
            for treatment in graph_treatments:
                for estimator in sorted_estimators:
                    experiment_combinations.append(
                        (graph_file, treatment, graph_outcome, estimator)
                    )
    else:
        experiment_combinations = list(itertools.product(
            graph_files,
            treatments,
            outcomes,
            sorted_estimators
        ))
    
    return experiment_combinations


def prepare_data_for_causal_model(
    merged_df: pd.DataFrame,
    config: Dict[str, Any],
    data_dir_path: Path,
    graph_data_dir: str
) -> pd.DataFrame:
    """
    ì¸ê³¼ ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ (ê·¸ë˜í”„ ë³€ìˆ˜ì— ë§ê²Œ ë°ì´í„° ì •ë¦¬)
    
    Args:
        merged_df: ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        data_dir_path: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        graph_data_dir: ê·¸ë˜í”„ ë°ì´í„° ë””ë ‰í† ë¦¬ëª…
    
    Returns:
        ì •ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    from . import utils
    treatments = config.get("treatments", [])
    outcomes = config.get("outcomes", ["ACQ_180_YN"])
    auto_extract_treatments = config.get("auto_extract_treatments", False)
    
    # ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
    graph_files = _get_graph_files(config, data_dir_path, graph_data_dir)
    
    if not graph_files:
        return merged_df
    
    # ëª¨ë“  ê·¸ë˜í”„ì˜ ë³€ìˆ˜ ìˆ˜ì§‘
    all_graph_variables = set()
    for graph_file in graph_files:
        graph_path = Path(graph_file)
        try:
            causal_graph = utils.create_causal_graph(str(graph_path))
            all_graph_variables.update(causal_graph.nodes())
        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({graph_path.name}): {e}")
    
    # treatment ìë™ ì¶”ì¶œ
    graph_treatments_map, graph_outcomes_map = _extract_treatments_from_graphs(
        graph_files, auto_extract_treatments
    )
    
    # ë°ì´í„° ì •ë¦¬
    all_treatments = set()
    all_outcomes = set()
    for graph_file in graph_files:
        if graph_file in graph_treatments_map:
            all_treatments.update(graph_treatments_map[graph_file])
        if graph_file in graph_outcomes_map:
            all_outcomes.add(graph_outcomes_map[graph_file])
    if not auto_extract_treatments:
        all_treatments.update(treatments)
    if not graph_outcomes_map:
        all_outcomes.update(outcomes)
    
    essential_vars = all_treatments | all_outcomes | {"JHNT_CTN", "JHNT_MBN"}
    stratification_vars = {"HOPE_JSCD1_NAME"}
    required_vars = list(all_graph_variables | essential_vars | stratification_vars)
    
    merged_df_clean = utils.clean_dataframe_for_causal_model(
        merged_df, 
        required_vars=required_vars, 
        logger=None
    )
    
    data_variables = set(merged_df_clean.columns)
    vars_to_keep = (all_graph_variables | essential_vars | stratification_vars) & data_variables
    vars_to_remove = data_variables - vars_to_keep
    
    if vars_to_remove:
        print(f"ğŸ—‘ï¸ ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ ì œê±° ì¤‘ ({len(vars_to_remove)}ê°œ)...")
        merged_df_clean = merged_df_clean[list(vars_to_keep)]
    
    print(f"âœ… ì •ë¦¬ëœ ë°ì´í„°: {len(merged_df_clean)}ê±´, {len(merged_df_clean.columns)}ê°œ ë³€ìˆ˜")
    
    return merged_df_clean


# ============================================================================
# ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def run_analysis_without_preprocessing(
    merged_df_clean: pd.DataFrame,
    graph_file: str,
    treatment: str,
    outcome: str,
    estimator: str,
    logger: Optional[logging.Logger] = None,
    experiment_id: Optional[str] = None,
    job_category: Optional[str] = None,
    training_size: int = 5000,
    tabpfn_config: Optional[Dict[str, Any]] = None,
    do_refutation: bool = False
) -> Dict[str, Any]:
    """
    ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ê³¼ì¶”ë¡  ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    (estimation â†’ refutation â†’ predictionë§Œ ìˆ˜í–‰)
    
    Args:
        merged_df_clean (pd.DataFrame): ì „ì²˜ë¦¬ ë° ì •ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
        treatment (str): ì²˜ì¹˜ ë³€ìˆ˜ëª…
        outcome (str): ê²°ê³¼ ë³€ìˆ˜ëª…
        estimator (str): ì¶”ì • ë°©ë²•
        logger (Optional[logging.Logger]): ë¡œê±° ê°ì²´
        experiment_id (Optional[str]): ì‹¤í—˜ ID (ì„ íƒì )
        job_category (Optional[str]): ì§ì¢…ì†Œë¶„ë¥˜ëª… (checkpoint ì €ì¥ ê²½ë¡œì— ì‚¬ìš©)
        training_size (int): Train set í¬ê¸° (ê¸°ë³¸ê°’: 5000)
        do_refutation (bool): Refutation ì‹¤í–‰ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
    
    Returns:
        Dict[str, Any]: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    from . import utils
    try:
        step_times = {}
        step_start = time.time()
        
        if experiment_id:
            print(f"\n{'='*80}")
            print(f"ì‹¤í—˜ ID: {experiment_id}")
            print(f"ê·¸ë˜í”„: {Path(graph_file).name}")
            print(f"Treatment: {treatment}, Outcome: {outcome}")
            print(f"Estimator: {estimator}")
            print(f"{'='*80}\n")
        
        # 1. ê·¸ë˜í”„ ë¡œë“œ
        print("1ï¸âƒ£ ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ ì¤‘...")
        step_start = time.time()
        causal_graph = utils.create_causal_graph(graph_file)
        step_times['ê·¸ë˜í”„ ë¡œë“œ'] = time.time() - step_start
        
        # 2. ë°ì´í„° í•„í„°ë§
        print("2ï¸âƒ£ ê·¸ë˜í”„ ë³€ìˆ˜ì— ë§ê²Œ ë°ì´í„° í•„í„°ë§ ì¤‘...")
        step_start = time.time()
        
        graph_variables = set(causal_graph.nodes())
        data_variables = set(merged_df_clean.columns)
        essential_vars = {treatment, outcome, "JHNT_CTN", "JHNT_MBN"}
        stratification_vars = {"HOPE_JSCD1_NAME"}
        vars_to_keep = (graph_variables | essential_vars | stratification_vars) & data_variables
        df_for_analysis = merged_df_clean[list(vars_to_keep)].copy()
        
        missing_vars = [var for var in [treatment, outcome] if var not in df_for_analysis.columns]
        if missing_vars:
            raise ValueError(f"í•„ìˆ˜ ë³€ìˆ˜ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤: {missing_vars}")
        
        step_times['ë°ì´í„° í•„í„°ë§'] = time.time() - step_start
        
        # 3. Train/Test Split (ê³ ì • ê°œìˆ˜ ìƒ˜í”Œë§)
        print(f"3ï¸âƒ£ Train/Test Split ì¤‘...")
        step_start = time.time()
        
        total_size = len(df_for_analysis)
        outcome_data = df_for_analysis[outcome]
        is_binary = outcome_data.nunique() <= 2 and outcome_data.dtype in ['int64', 'int32', 'bool']
        
        # ë°ì´í„°ê°€ training_sizeë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì€ ê²½ìš° 8:2 ë¹„ìœ¨ë¡œ split
        if total_size <= training_size:
            print(f"âš ï¸ ì „ì²´ ë°ì´í„°({total_size}ê±´)ê°€ training_size({training_size}ê±´)ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìŠµë‹ˆë‹¤. 8:2 ë¹„ìœ¨ë¡œ splití•©ë‹ˆë‹¤.")
            if is_binary:
                # Binary outcomeì¸ ê²½ìš° stratify ì‚¬ìš©
                df_train, df_test = train_test_split(
                    df_for_analysis,
                    test_size=0.2,
                    random_state=42,
                    stratify=outcome_data
                )
            else:
                # ì—°ì†í˜• outcomeì¸ ê²½ìš° stratify ì—†ì´ split
                df_train, df_test = train_test_split(
                    df_for_analysis,
                    test_size=0.2,
                    random_state=42
                )
        else:
            # training_sizeë§Œí¼ ìƒ˜í”Œë§í•˜ì—¬ train set ìƒì„±, ë‚˜ë¨¸ì§€ëŠ” test set
            print(f"ğŸ“Š Train: {training_size}ê°œ, Test: ë‚˜ë¨¸ì§€ ({total_size - training_size}ê°œ)")
            if is_binary:
                # Binary outcomeì¸ ê²½ìš° stratify ì‚¬ìš©
                df_train, df_test = train_test_split(
                    df_for_analysis,
                    train_size=training_size,
                    random_state=42,
                    stratify=outcome_data
                )
            else:
                # ì—°ì†í˜• outcomeì¸ ê²½ìš° stratify ì—†ì´ ìƒ˜í”Œë§
                df_train, df_test = train_test_split(
                    df_for_analysis,
                    train_size=training_size,
                    random_state=42
                )
        
        print(f"âœ… Train set: {len(df_train)}ê±´, Test set: {len(df_test)}ê±´")
        step_times['Train/Test Split'] = time.time() - step_start
        
        # 3-1. ì»¬ëŸ¼ë³„ íƒ€ì… ì²´í¬ (int/str í˜¼í•© ê°ì§€)
        print("ğŸ” ì»¬ëŸ¼ë³„ íƒ€ì… ì²´í¬ ì¤‘...")
        for col in df_train.columns:
            if df_train[col].dtype == 'object':
                non_null = df_train[col].dropna()
                if len(non_null) > 0:
                    types = set(type(v).__name__ for v in non_null)
                    if len(types) > 1:
                        print(f"âš ï¸ ì»¬ëŸ¼ '{col}'ì— íƒ€ì… í˜¼í•© ê°ì§€: {types}")
                        if logger:
                            logger.warning(f"ì»¬ëŸ¼ '{col}'ì— íƒ€ì… í˜¼í•© ê°ì§€: {types}")
        
        # 3-2. Categorical ë³€ìˆ˜ Ordinal Encoding (TabPFNìš©)
        ordinal_encoder = None
        categorical_columns = []
        if estimator == 'tabpfn':
            print("ğŸ”¢ Categorical ë³€ìˆ˜ Ordinal Encoding ì¤‘...")
            step_start = time.time()
            
            # Categorical ë³€ìˆ˜ ì°¾ê¸° (Treatment/Outcome ì œì™¸)
            categorical_columns = [
                col for col in df_train.select_dtypes(include=['object', 'string', 'category']).columns
                if col not in [treatment, outcome]
            ]
            
            if categorical_columns:
                print(f"   ë°œê²¬ëœ categorical ë³€ìˆ˜: {categorical_columns}")
                
                # OrdinalEncoder ìƒì„±
                ordinal_encoder = OrdinalEncoder(
                    handle_unknown='use_encoded_value',
                    unknown_value=-1,
                    dtype=np.int64
                )
                
                # ì „ì²˜ë¦¬ í•¨ìˆ˜: NaN ì²˜ë¦¬ ë° ë¬¸ìì—´ ë³€í™˜
                def preprocess_for_encoding(df, cols):
                    df_processed = df.copy()
                    for col in cols:
                        if col in df_processed.columns:
                            df_processed[col] = df_processed[col].fillna('__nan__').astype(str)
                    return df_processed
                
                # Train/Test ë°ì´í„° ì „ì²˜ë¦¬ ë° ì¸ì½”ë”©
                df_train_processed = preprocess_for_encoding(df_train, categorical_columns)
                df_test_processed = preprocess_for_encoding(df_test, categorical_columns)
                
                # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì¸ì½”ë”©
                existing_cols = [col for col in categorical_columns if col in df_train_processed.columns]
                if not existing_cols:
                    print("   âš ï¸ categorical ì»¬ëŸ¼ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
                    ordinal_encoder = None
                    categorical_columns = []
                else:
                    df_train_encoded = df_train.copy()
                    df_train_encoded[existing_cols] = ordinal_encoder.fit_transform(df_train_processed[existing_cols])
                    
                    df_test_encoded = df_test.copy()
                    test_existing_cols = [col for col in existing_cols if col in df_test_processed.columns]
                    if len(test_existing_cols) < len(existing_cols):
                        missing_cols = [col for col in existing_cols if col not in test_existing_cols]
                        print(f"   âš ï¸ Test ë°ì´í„°ì— ì¼ë¶€ categorical ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}")
                    df_test_encoded[test_existing_cols] = ordinal_encoder.transform(df_test_processed[test_existing_cols])
                
                    # Unknown categories ë¡œê¹…
                    for col in existing_cols:
                        if col in df_test.columns and col in df_train.columns:
                            unknown_cats = set(df_test[col].dropna().unique()) - set(df_train[col].dropna().unique())
                            if unknown_cats:
                                unknown_count = df_test[col].isin(unknown_cats).sum()
                                print(f"   âš ï¸ '{col}': {unknown_count}ê°œ unknown categories â†’ -1ë¡œ ì¸ì½”ë”©ë¨")
                    
                    df_train, df_test = df_train_encoded, df_test_encoded
                    categorical_columns = existing_cols
                    print(f"âœ… Ordinal Encoding ì™„ë£Œ: {len(categorical_columns)}ê°œ ë³€ìˆ˜")
            else:
                print("   Categorical ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            step_times['Ordinal Encoding'] = time.time() - step_start
        
        # 4. ì¸ê³¼ëª¨ë¸ ìƒì„±
        print("4ï¸âƒ£ ì¸ê³¼ëª¨ë¸ ìƒì„± ì¤‘...")
        step_start = time.time()
        model = CausalModel(
            data=df_train,
            treatment=treatment,
            outcome=outcome,
            graph=causal_graph
        )
        step_times['ì¸ê³¼ëª¨ë¸ ìƒì„±'] = time.time() - step_start
        
        # 5. ì¸ê³¼íš¨ê³¼ ì‹ë³„
        print("5ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì‹ë³„ ì¤‘...")
        step_start = time.time()
        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
        step_times['ì¸ê³¼íš¨ê³¼ ì‹ë³„'] = time.time() - step_start
        
        # 6. ì¸ê³¼íš¨ê³¼ ì¶”ì •
        print("6ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì¶”ì • ì¤‘...")
        step_start = time.time()
        estimate = estimate_causal_effect(
            model,
            identified_estimand,
            estimator,
            logger,
            tabpfn_config=tabpfn_config
        )
        
        # OrdinalEncoderë¥¼ estimate ê°ì²´ì— ì €ì¥ (ì˜ˆì¸¡ ì‹œ ì‚¬ìš©)
        if ordinal_encoder is not None:
            estimate._ordinal_encoder = ordinal_encoder
            estimate._categorical_columns = categorical_columns
            print(f"ğŸ’¾ OrdinalEncoderë¥¼ estimate ê°ì²´ì— ì €ì¥: {len(categorical_columns)}ê°œ ë³€ìˆ˜")
        
        step_times['ì¸ê³¼íš¨ê³¼ ì¶”ì •'] = time.time() - step_start
        
        # 6-1. Checkpoint ì €ì¥
        checkpoint_path = None
        if experiment_id:
            try:
                script_dir = Path(__file__).parent.parent
                checkpoint_dir = script_dir / "data" / "checkpoint"
                
                if job_category:
                    job_category_safe = str(job_category).replace("/", "_").replace("\\", "_").replace(" ", "_")
                    checkpoint_dir = checkpoint_dir / job_category_safe
                
                graph_name = Path(graph_file).stem if graph_file else None
                checkpoint_path = save_checkpoint(
                    estimate,
                    checkpoint_dir,
                    experiment_id,
                    graph_name=graph_name,
                    logger=logger
                )
            except Exception as e:
                if logger:
                    logger.warning(f"Checkpoint ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
                print(f"âš ï¸ Checkpoint ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        
        # 6-2. Refutation (ì„ íƒ ì‚¬í•­)
        validation_results = {}
        if do_refutation:
            print("ğŸ›¡ï¸ Refutation í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
            step_start = time.time()
            validation_results = run_validation_tests(model, identified_estimand, estimate, logger=logger)
            step_times['Refutation'] = time.time() - step_start
        
        # 7. ì˜ˆì¸¡
        print("7ï¸âƒ£ ì˜ˆì¸¡ ì¤‘...")
        step_start = time.time()
        essential_vars_for_pred = {treatment, outcome}
        if outcome in df_test.columns:
            df_test = df_test.copy()
            df_test[f"{outcome}_actual"] = df_test[outcome].copy()
        
        df_test_clean = utils.clean_dataframe_for_causal_model(
            df_test,
            required_vars=list(essential_vars_for_pred) + [f"{outcome}_actual"] if f"{outcome}_actual" in df_test.columns else list(essential_vars_for_pred),
            logger=logger
        )
        # TabPFN ë°°ì¹˜ í¬ê¸° ì„¤ì • (configì—ì„œ ê°€ì ¸ì˜¤ê¸°, ê¸°ë³¸ê°’: 64)
        metrics, df_with_predictions = predict_conditional_expectation(
            estimate, df_test_clean, logger=logger
        )
        step_times['ì˜ˆì¸¡'] = time.time() - step_start
        
        # ì§ì¢…ì†Œë¶„ë¥˜ ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if job_category is not None:
            df_with_predictions['job_category'] = job_category
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        if experiment_id:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"predictions_{experiment_id}_{timestamp}.xlsx"
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"predictions_{timestamp}.xlsx"
        
        step_start = time.time()
        excel_path = utils.save_predictions_to_excel(df_with_predictions, filename=filename, logger=logger)
        step_times['ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥'] = time.time() - step_start
        
        # 8. ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“Š ì¶”ì • ê²°ê³¼ ìš”ì•½")
        print("="*60)
        print(f"  ATE (Average Treatment Effect): {estimate.value:.6f}")
        
        if validation_results:
            print(f"\nğŸ”¬ Refutation ê²°ê³¼ ìš”ì•½:")
            for test_name, res in validation_results.items():
                if res:
                    # status íŒë‹¨ (run_validation_testsì˜ ë¡œì§ ì°¸ê³ )
                    if test_name == 'placebo':
                        status = "í†µê³¼" if abs(res.new_effect - res.estimated_effect) < 0.01 else "ì‹¤íŒ¨"
                    elif test_name == 'unobserved':
                        change_rate = abs(res.new_effect - res.estimated_effect) / abs(res.estimated_effect) if abs(res.estimated_effect) > 0 else float('inf')
                        status = "ê°•ê±´í•¨" if change_rate < 0.2 else "ë¯¼ê°í•¨"
                    elif test_name == 'subset':
                        change_rate = abs(res.new_effect - res.estimated_effect) / abs(res.estimated_effect) if abs(res.estimated_effect) > 0 else float('inf')
                        status = "í†µê³¼" if change_rate < 0.1 else "ì‹¤íŒ¨"
                    elif test_name == 'dummy':
                        status = "í†µê³¼" if abs(res.new_effect) < 0.01 else "ì‹¤íŒ¨"
                    else:
                        status = "ì™„ë£Œ"
                    print(f"  - {test_name}: {status} (New Effect: {res.new_effect:.6f})")
        
        if metrics:
            print(f"\nğŸ“ˆ ì˜ˆì¸¡ ì„±ëŠ¥:")
            if metrics.get('accuracy') is not None:
                print(f"  Accuracy: {metrics['accuracy']:.4f}")
            if metrics.get('f1_score') is not None:
                print(f"  F1 Score: {metrics['f1_score']:.4f}")
            if metrics.get('auc') is not None:
                print(f"  AUC: {metrics['auc']:.4f}")
        print("="*60)
        
        if not do_refutation:
            print("â„¹ï¸  ë¯¼ê°ë„ ë¶„ì„/Refutation í…ŒìŠ¤íŠ¸ëŠ” ë³„ë„ë¡œ ì‹¤í–‰í•˜ê±°ë‚˜ configì—ì„œ í™œì„±í™”í•˜ì„¸ìš”.")
            print("="*60 + "\n")
        
        # 9. TabPFN ë©”ëª¨ë¦¬ ì •ë¦¬ (ë¶„ì„ ì™„ë£Œ í›„)
        if estimator == 'tabpfn':
            cleanup_tabpfn_memory(estimate, device_id=0, logger=logger)
        
        total_time = sum(step_times.values())
        step_times['ì „ì²´'] = total_time
        
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! (ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ)")
        
        res_dict = {
            "status": "success",
            "estimate": estimate,
            "validation_results": validation_results,
            "metrics": metrics,
            "excel_path": excel_path,
            "checkpoint_path": checkpoint_path,
            "step_times": step_times,
            "train_size": len(df_train),
            "test_size": len(df_test)
        }
        
        # CSV ë¡œê¹…ì„ ìœ„í•´ Refutation ê²°ê³¼ë¥¼ í‰íƒ„í™”í•˜ì—¬ ì¶”ê°€
        if validation_results:
            for test_name, res in validation_results.items():
                if res:
                    p_val = calculate_refutation_pvalue(res, test_name, logger)
                    res_dict[f"{test_name}_pvalue"] = p_val
                    
                    if test_name == 'placebo':
                        res_dict['placebo_passed'] = abs(res.new_effect - res.estimated_effect) < 0.01
                    elif test_name == 'unobserved':
                        change_rate = abs(res.new_effect - res.estimated_effect) / abs(res.estimated_effect) if abs(res.estimated_effect) > 0 else float('inf')
                        res_dict['unobserved_passed'] = change_rate < 0.2
                    elif test_name == 'subset':
                        change_rate = abs(res.new_effect - res.estimated_effect) / abs(res.estimated_effect) if abs(res.estimated_effect) > 0 else float('inf')
                        res_dict['subset_passed'] = change_rate < 0.1
                    elif test_name == 'dummy':
                        res_dict['dummy_passed'] = abs(res.new_effect) < 0.01
        
        return res_dict
        
    except Exception as e:
        if logger:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


def run_single_experiment(
    merged_df_clean: pd.DataFrame,
    graph_file: str,
    treatment: str,
    outcome: str,
    estimator: str,
    experiment_id: str,
    logger: Optional[logging.Logger] = None,
    split_by_job_category: bool = True,
    training_size: int = 5000,
    tabpfn_config: Optional[Dict[str, Any]] = None,
    do_refutation: bool = False
) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤
    
    Args:
        merged_df_clean (pd.DataFrame): ì „ì²˜ë¦¬ ë° ì •ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
        treatment (str): ì²˜ì¹˜ ë³€ìˆ˜ëª…
        outcome (str): ê²°ê³¼ ë³€ìˆ˜ëª…
        estimator (str): ì¶”ì • ë°©ë²•
        experiment_id (str): ì‹¤í—˜ ID
        logger (Optional[logging.Logger]): ë¡œê±° ê°ì²´
        split_by_job_category (bool): ì§ì¢…ì†Œë¶„ë¥˜ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì‹¤í—˜ ì‹¤í–‰ ì—¬ë¶€
        training_size (int): Train set í¬ê¸° (ê¸°ë³¸ê°’: 5000)
        do_refutation (bool): Refutation ì‹¤í–‰ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
    
    Returns:
        Dict[str, Any]: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    from . import utils
    start_time = datetime.now()
    try:
        # ì§ì¢…ì†Œë¶„ë¥˜ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì‹¤í—˜ ì‹¤í–‰
        if split_by_job_category and "HOPE_JSCD1_NAME" in merged_df_clean.columns:
            job_categories = merged_df_clean["HOPE_JSCD1_NAME"].dropna().unique()
            print(f"ğŸ“Š ì§ì¢…ì†Œë¶„ë¥˜ë³„ ì‹¤í—˜ ì‹¤í–‰: {len(job_categories)}ê°œ ì§ì¢…ì†Œë¶„ë¥˜")
            
            all_results = []
            all_predictions = []
            all_metrics = []
            job_category_list = []  # ì§ì¢…ì†Œë¶„ë¥˜ ë¦¬ìŠ¤íŠ¸ ì €ì¥ (ì„±ê³µí•œ ê²ƒë§Œ)
            
            for job_category in job_categories:
                job_df = merged_df_clean[merged_df_clean["HOPE_JSCD1_NAME"] == job_category].copy()
                
                if len(job_df) < 10:
                    if logger:
                        logger.warning(f"ì§ì¢…ì†Œë¶„ë¥˜ '{job_category}' ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ ê±´ë„ˆëœë‹ˆë‹¤: {len(job_df)}ê±´")
                    print(f"âš ï¸ ì§ì¢…ì†Œë¶„ë¥˜ '{job_category}' ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ ê±´ë„ˆëœë‹ˆë‹¤: {len(job_df)}ê±´")
                    continue
                
                job_category_safe = str(job_category).replace("/", "_").replace("\\", "_").replace(" ", "_")
                job_experiment_id = f"{experiment_id}_{job_category_safe}"
                
                print(f"\n  ğŸ”¹ ì§ì¢…ì†Œë¶„ë¥˜: {job_category} ({len(job_df)}ê±´)")
                
                try:
                    job_result = run_analysis_without_preprocessing(
                        merged_df_clean=job_df,
                        graph_file=graph_file,
                        treatment=treatment,
                        outcome=outcome,
                        estimator=estimator,
                        logger=logger,
                        experiment_id=job_experiment_id,
                        job_category=job_category,
                        training_size=training_size,
                        tabpfn_config=tabpfn_config,
                        do_refutation=do_refutation
                    )
                    
                    all_results.append(job_result)
                    job_category_list.append(job_category)  # ì„±ê³µí•œ ì§ì¢…ì†Œë¶„ë¥˜ë§Œ ì €ì¥
                    
                    if job_result.get("excel_path"):
                        try:
                            pred_df = pd.read_excel(job_result["excel_path"])
                            all_predictions.append(pred_df)
                        except:
                            pass
                    
                    if job_result.get("metrics"):
                        all_metrics.append(job_result["metrics"])
                    
                    # TabPFN ì‚¬ìš© ì‹œ ê° ì‹¤í—˜ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (CUDA 0ë²ˆ)
                    if estimator == 'tabpfn' and job_result.get('estimate'):
                        cleanup_tabpfn_memory(job_result['estimate'], device_id=0, logger=logger)
                        
                except Exception as e:
                    # ì‹¤íŒ¨ ì‹œì—ë„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (CUDA 0ë²ˆ)
                    if estimator == 'tabpfn':
                        try:
                            import torch
                            if torch.cuda.is_available():
                                torch.cuda.set_device(0)  # CUDA 0ë²ˆìœ¼ë¡œ ì„¤ì •
                                gc.collect()
                                torch.cuda.empty_cache()
                                torch.cuda.synchronize()
                        except:
                            pass
                    
                    if logger:
                        logger.error(f"ì§ì¢…ì†Œë¶„ë¥˜ '{job_category}' ì‹¤í—˜ ì‹¤íŒ¨: {e}")
                    print(f"  âŒ ì§ì¢…ì†Œë¶„ë¥˜ '{job_category}' ì‹¤í—˜ ì‹¤íŒ¨: {e}")
                    continue
            
            if not all_results:
                raise ValueError("ëª¨ë“  ì§ì¢…ì†Œë¶„ë¥˜ ì‹¤í—˜ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            # ì˜ˆì¸¡ ê²°ê³¼ í•©ì¹˜ê¸° ë° í‰ê°€ì§€í‘œ ê³„ì‚°
            if all_predictions:
                combined_predictions = pd.concat(all_predictions, ignore_index=True)
                
                # ê° ì§ì¢…ì†Œë¶„ë¥˜ë³„ í‰ê°€ì§€í‘œ ìˆ˜ì§‘
                job_category_metrics_list = []
                for idx, job_result in enumerate(all_results):
                    job_category = job_category_list[idx] if idx < len(job_category_list) else None
                    job_metrics = job_result.get("metrics", {})
                    
                    if job_category and job_metrics:
                        job_metric_row = {
                            "job_category": job_category,
                            "train_size": job_result.get("train_size", 0),
                            "test_size": job_result.get("test_size", 0),
                            "accuracy": job_metrics.get("accuracy"),
                            "f1_score": job_metrics.get("f1_score"),
                            "auc": job_metrics.get("auc")
                        }
                        job_category_metrics_list.append(job_metric_row)
                
                # ì „ì²´ ë°ì´í„° í‰ê°€ì§€í‘œ ê³„ì‚°
                combined_metrics = {'accuracy': None, 'f1_score': None, 'auc': None}
                if all_metrics:
                    actual_outcome_col = f"{outcome}_actual"
                    if actual_outcome_col in combined_predictions.columns and outcome in combined_predictions.columns:
                        actual_y = combined_predictions[actual_outcome_col]
                        predicted_y = combined_predictions[outcome]
                        prob_col = f"{outcome}_prob"
                        prob_y = combined_predictions[prob_col] if prob_col in combined_predictions.columns else None
                        
                        combined_metrics = utils.calculate_metrics(actual_y, predicted_y, prob_y=prob_y, logger=logger)
                
                # ì „ì²´ í‰ê°€ì§€í‘œë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                if combined_metrics:
                    overall_metric_row = {
                        "job_category": "ì „ì²´",
                        "train_size": sum([r.get("train_size", 0) for r in all_results]),
                        "test_size": sum([r.get("test_size", 0) for r in all_results]),
                        "accuracy": combined_metrics.get("accuracy"),
                        "f1_score": combined_metrics.get("f1_score"),
                        "auc": combined_metrics.get("auc")
                    }
                    job_category_metrics_list.append(overall_metric_row)
                
                # í‰ê°€ì§€í‘œë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ CSV ì €ì¥
                if job_category_metrics_list:
                    metrics_df = pd.DataFrame(job_category_metrics_list)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    script_dir = Path(__file__).parent.parent
                    output_dir = script_dir / "log"
                    output_dir.mkdir(parents=True, exist_ok=True)
                    metrics_csv_path = output_dir / f"metrics_{experiment_id}_{timestamp}.csv"
                    metrics_df.to_csv(metrics_csv_path, index=False, encoding='utf-8-sig')
                    if logger:
                        logger.info(f"âœ… í‰ê°€ì§€í‘œ CSV ì €ì¥ ì™„ë£Œ: {metrics_csv_path}")
                    print(f"âœ… í‰ê°€ì§€í‘œ CSV ì €ì¥ ì™„ë£Œ: {metrics_csv_path}")
                
                # ì˜ˆì¸¡ ê²°ê³¼ Excel ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                excel_path = utils.save_predictions_to_excel(
                    combined_predictions, 
                    filename=f"predictions_{experiment_id}_combined_{timestamp}.xlsx",
                    logger=logger
                )
            else:
                combined_metrics = {}
                excel_path = None
            
            base_result = all_results[0]
            result = {
                "status": "success",
                "estimate": base_result.get("estimate"),
                "validation_results": base_result.get("validation_results", {}),
                "sensitivity_df": base_result.get("sensitivity_df"),
                "metrics": combined_metrics,
                "excel_path": excel_path,
                "step_times": base_result.get("step_times", {}),
                "train_size": sum([r.get("train_size", 0) for r in all_results]),
                "test_size": sum([r.get("test_size", 0) for r in all_results]),
                "job_category_results": all_results,
                "num_job_categories": len(all_results)
            }
        else:
            result = run_analysis_without_preprocessing(
                merged_df_clean=merged_df_clean,
                graph_file=graph_file,
                treatment=treatment,
                outcome=outcome,
                estimator=estimator,
                logger=logger,
                experiment_id=experiment_id,
                training_size=training_size,
                tabpfn_config=tabpfn_config,
                do_refutation=do_refutation
            )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        metrics = result.get("metrics", {})
        estimate = result.get("estimate")
        
        ate_value = None
        if estimate and hasattr(estimate, 'value'):
            ate_value = estimate.value
        
        return_dict = {
            "experiment_id": experiment_id,
            "status": "success",
            "duration_seconds": duration,
            "graph": graph_file,
            "graph_name": Path(graph_file).stem,
            "treatment": treatment,
            "outcome": outcome,
            "estimator": estimator,
            "ate_value": ate_value,
            "metrics": metrics,
            "accuracy": metrics.get("accuracy") if metrics else None,
            "f1_score": metrics.get("f1_score") if metrics else None,
            "auc": metrics.get("auc") if metrics else None,
            "excel_path": result.get("excel_path"),
            "train_size": result.get("train_size"),
            "test_size": result.get("test_size"),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat()
        }
        
        # Refutation ê²°ê³¼ ì¶”ê°€
        for key in ['placebo_passed', 'placebo_pvalue', 'unobserved_passed', 'unobserved_pvalue', 
                    'subset_passed', 'subset_pvalue', 'dummy_passed', 'dummy_pvalue']:
            if key in result:
                return_dict[key] = result[key]
                
        return return_dict
    except Exception as e:
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        return {
            "experiment_id": experiment_id,
            "status": "failed",
            "duration_seconds": duration,
            "graph": graph_file,
            "treatment": treatment,
            "outcome": outcome,
            "estimator": estimator,
            "error": str(e),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
        }


def run_inference(
    merged_df_clean: pd.DataFrame,
    graph_file: str,
    checkpoint_dir: Path,
    treatment: str,
    outcome: str,
    estimator: str,
    logger: Optional[logging.Logger] = None,
    experiment_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Inference ëª¨ë“œ: checkpointì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        merged_df_clean (pd.DataFrame): ì „ì²˜ë¦¬ ë° ì •ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
        checkpoint_dir (Path): checkpoint ë””ë ‰í† ë¦¬ ê²½ë¡œ
        treatment (str): ì²˜ì¹˜ ë³€ìˆ˜ëª…
        outcome (str): ê²°ê³¼ ë³€ìˆ˜ëª…
        estimator (str): ì¶”ì • ë°©ë²•
        logger (Optional[logging.Logger]): ë¡œê±° ê°ì²´
        experiment_id (Optional[str]): ì‹¤í—˜ ID (ì„ íƒì )
    
    Returns:
        Dict[str, Any]: ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    from . import utils
    try:
        step_times = {}
        step_start = time.time()
        
        if experiment_id:
            print(f"\n{'='*80}")
            print(f"Inference ëª¨ë“œ - ì‹¤í—˜ ID: {experiment_id}")
            print(f"ê·¸ë˜í”„: {Path(graph_file).name}")
            print(f"Treatment: {treatment}, Outcome: {outcome}, Estimator: {estimator}")
            print(f"{'='*80}\n")
        
        graph_name = Path(graph_file).stem
        
        # ì§ì¢…ì†Œë¶„ë¥˜ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì˜ˆì¸¡
        if "HOPE_JSCD1_NAME" in merged_df_clean.columns:
            job_categories = merged_df_clean["HOPE_JSCD1_NAME"].dropna().unique()
            print(f"ğŸ“Š ì§ì¢…ì†Œë¶„ë¥˜ë³„ Inference ì‹¤í–‰: {len(job_categories)}ê°œ ì§ì¢…ì†Œë¶„ë¥˜")
            
            all_predictions = []
            all_metrics = []
            
            for job_category in job_categories:
                job_df = merged_df_clean[merged_df_clean["HOPE_JSCD1_NAME"] == job_category].copy()
                
                if len(job_df) == 0:
                    continue
                
                job_category_safe = str(job_category).replace("/", "_").replace("\\", "_").replace(" ", "_")
                job_checkpoint_dir = checkpoint_dir / job_category_safe
                
                print(f"\n  ğŸ”¹ ì§ì¢…ì†Œë¶„ë¥˜: {job_category} ({len(job_df)}ê±´)")
                
                checkpoint_file = find_checkpoint(
                    job_checkpoint_dir,
                    graph_name,
                    treatment,
                    outcome,
                    estimator,
                    logger
                )
                
                if not checkpoint_file:
                    print(f"  âš ï¸ Checkpointë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤: {job_category}")
                    continue
                
                try:
                    estimate = load_checkpoint(checkpoint_file, logger)
                    
                    essential_vars = {treatment, outcome, "JHNT_CTN", "JHNT_MBN"}
                    data_variables = set(job_df.columns)
                    # causal_graph = utils.create_causal_graph(graph_file)
                    # graph_vars = set(causal_graph.nodes())
                    vars_to_keep = essential_vars | data_variables
                    
                    missing_vars = [var for var in [treatment, outcome] if var not in job_df.columns]
                    if missing_vars:
                        print(f"  âš ï¸ í•„ìˆ˜ ë³€ìˆ˜ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤: {missing_vars}")
                        continue
                    
                    df_for_prediction = job_df[list(vars_to_keep)].copy()
                    
                    if outcome in df_for_prediction.columns:
                        df_for_prediction[f"{outcome}_actual"] = df_for_prediction[outcome].copy()
                    
                    df_pred_clean = utils.clean_dataframe_for_causal_model(
                        df_for_prediction,
                        required_vars=list(essential_vars) + [f"{outcome}_actual"] if f"{outcome}_actual" in df_for_prediction.columns else list(essential_vars),
                        logger=logger
                    )
                    metrics, df_with_predictions = predict_conditional_expectation(
                        estimate, df_pred_clean, logger=logger
                    )
                    
                    all_predictions.append(df_with_predictions)
                    if metrics:
                        all_metrics.append(metrics)
                    
                    print(f"  âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(df_with_predictions)}ê±´")
                    
                except Exception as e:
                    print(f"  âŒ ì§ì¢…ì†Œë¶„ë¥˜ '{job_category}' ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    if logger:
                        logger.error(f"ì§ì¢…ì†Œë¶„ë¥˜ '{job_category}' ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    continue
            
            if not all_predictions:
                raise ValueError("ëª¨ë“  ì§ì¢…ì†Œë¶„ë¥˜ ì˜ˆì¸¡ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            combined_predictions = pd.concat(all_predictions, ignore_index=True)
            
            # í†µí•© ë©”íŠ¸ë¦­ ê³„ì‚°
            combined_metrics = {'accuracy': None, 'f1_score': None, 'auc': None}
            actual_outcome_col = f"{outcome}_actual"
            if actual_outcome_col in combined_predictions.columns and outcome in combined_predictions.columns:
                actual_y = combined_predictions[actual_outcome_col]
                predicted_y = combined_predictions[outcome]
                prob_col = f"{outcome}_prob"
                prob_y = combined_predictions[prob_col] if prob_col in combined_predictions.columns else None
                
                combined_metrics = utils.calculate_metrics(actual_y, predicted_y, prob_y=prob_y, logger=logger)
            
            # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            step_start = time.time()
            if experiment_id:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"predictions_inference_{experiment_id}_combined_{timestamp}.xlsx"
            else:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"predictions_inference_combined_{timestamp}.xlsx"
            
            excel_path = utils.save_predictions_to_excel(combined_predictions, filename=filename, logger=logger)
            step_times['ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥'] = time.time() - step_start
            
        else:
            raise ValueError("HOPE_JSCD1_NAME ë³€ìˆ˜ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ì§ì¢…ì†Œë¶„ë¥˜ë³„ ë¶„ë¦¬ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        total_time = sum(step_times.values())
        step_times['ì „ì²´'] = total_time
        
        print(f"\nâœ… Inference ì™„ë£Œ! (ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ)")
        if combined_metrics:
            print(f"   Accuracy: {combined_metrics.get('accuracy', 'N/A')}")
            print(f"   F1 Score: {combined_metrics.get('f1_score', 'N/A')}")
            print(f"   AUC: {combined_metrics.get('auc', 'N/A')}")
        
        return {
            "status": "success",
            "metrics": combined_metrics,
            "excel_path": excel_path,
            "step_times": step_times,
            "data_size": len(combined_predictions)
        }
        
    except Exception as e:
        if logger:
            logger.error(f"Inference ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ Inference ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise
