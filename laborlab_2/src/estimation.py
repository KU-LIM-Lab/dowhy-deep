"""
DoWhy ì¸ê³¼íš¨ê³¼ ì¶”ì • ë° ê²€ì¦ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ì¸ê³¼íš¨ê³¼ ì¶”ì •, ê²€ì¦ í…ŒìŠ¤íŠ¸, ë¯¼ê°ë„ ë¶„ì„ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import logging
from pathlib import Path
from datetime import datetime
import os
import sys
import pickle
import json
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score

from dowhy.causal_estimators.regression_estimator import RegressionEstimator

# ë¡œì»¬ DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

# DoWhy ë‚´ë¶€ í•¨ìˆ˜ ì„í¬íŠ¸
from dowhy.causal_estimator import estimate_effect as dowhy_estimate_effect

def log_estimation_results(logger, estimate, method_name):
    """
    ì¶”ì • ê²°ê³¼ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        logger: ë¡œê±° ê°ì²´
        estimate: ì¶”ì •ëœ ì¸ê³¼íš¨ê³¼ ê°ì²´
        method_name (str): ì¶”ì • ë°©ë²•ëª…
    """
    logger.info("="*60)
    logger.info("ì¸ê³¼ íš¨ê³¼ ì¶”ì • ê²°ê³¼")
    logger.info("="*60)
    logger.info(f"ì¶”ì • ë°©ë²•: {method_name}")
    logger.info(f"ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ (ATE): {estimate.value:.6f}")
    
    if hasattr(estimate, 'p_value') and estimate.p_value is not None:
        logger.info(f"P-value: {estimate.p_value:.6f}")
        significance = "ìœ ì˜í•¨" if estimate.p_value <= 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
        logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {significance}")
    
    # ì¶”ì •ì¹˜ì˜ ì‹ ë¢°êµ¬ê°„ì´ ìˆë‹¤ë©´ ë¡œê¹…
    if hasattr(estimate, 'confidence_intervals'):
        logger.info(f"ì‹ ë¢°êµ¬ê°„: {estimate.confidence_intervals}")


def predict_conditional_expectation(estimate, data_df, treatment_value=None, logger=None):
    """
    E(Y|A, X) ì¡°ê±´ë¶€ ê¸°ëŒ€ê°’ ì˜ˆì¸¡
    
    Args:
        estimate: CausalEstimate ê°ì²´
        data_df: ì˜ˆì¸¡í•  ë°ì´í„°í”„ë ˆì„
        treatment_value: ì²˜ì¹˜ ê°’ (Noneì´ë©´ ì‹¤ì œ ê°’ ì‚¬ìš©)
        logger: ë¡œê±° ê°ì²´
    
    Returns:
        tuple: (metrics_dict, data_df_with_predictions)
            - metrics_dict: {'accuracy': float, 'f1_score': float, 'auc': float} ë˜ëŠ” None
            - data_df_with_predictions: outcome ì—´ì— ì˜ˆì¸¡ê°’ì´ ì±„ì›Œì§„ ë°ì´í„°í”„ë ˆì„
    """
    if not hasattr(estimate, 'estimator'):
        raise ValueError("estimate.estimatorê°€ ì—†ìŠµë‹ˆë‹¤. estimate_causal_effectë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    
    estimator = estimate.estimator
    if not isinstance(estimator, RegressionEstimator):
        raise ValueError(f"{type(estimator).__name__}ëŠ” ì˜ˆì¸¡ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    if logger:
        logger.info(f"E(Y|A, X) ì˜ˆì¸¡ ì‹œì‘: {len(data_df)}ê°œ")
        if treatment_value is not None:
            logger.info(f"ì²˜ì¹˜ ê°’: {treatment_value}")
    
    try:
        # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ (ì›ë³¸ ë³´í˜¸)
        data_df_clean = data_df.copy()
        
        # treatmentì™€ outcome ë³€ìˆ˜ëŠ” ë°˜ë“œì‹œ ìœ ì§€í•´ì•¼ í•¨
        # _treatment_nameê³¼ _outcome_nameì€ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ (private attribute)
        treatment_var = estimate._treatment_name[0] if isinstance(estimate._treatment_name, list) else estimate._treatment_name
        outcome_var = estimate._outcome_name[0] if isinstance(estimate._outcome_name, list) else estimate._outcome_name

        # treatment ë³€ìˆ˜ê°€ ìˆëŠ”ì§€ í™•ì¸
        if treatment_var not in data_df_clean.columns:
            raise ValueError(f"Treatment ë³€ìˆ˜ '{treatment_var}'ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(data_df_clean.columns)}")
        
        # predict_fnì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡ (RegressionEstimatorì˜ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)
        if treatment_value is not None:
            predictions = estimator.interventional_outcomes(data_df_clean, treatment_value)
        else:
            predictions = estimator.predict(data_df_clean)
        
        predictions_series = pd.Series(predictions, index=data_df_clean.index)
        
        # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ í›„ ì˜ˆì¸¡ê°’ ì±„ìš°ê¸°
        result_df = data_df_clean.copy()
        # _outcome_nameì€ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ
        outcome_name = estimate._outcome_name[0] if isinstance(estimate._outcome_name, list) else estimate._outcome_name
        result_df[outcome_name] = predictions_series
        
        # ì‹¤ì œ Y ê°’ê³¼ ë¹„êµí•˜ì—¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {}
        if outcome_name in data_df_clean.columns:
            actual_y = data_df_clean[outcome_name]
            # actual_yê°€ ìˆ«ì íƒ€ì…ì¸ì§€ í™•ì¸
            if not pd.api.types.is_numeric_dtype(actual_y):
                actual_y = pd.to_numeric(actual_y, errors='coerce')
            
            # NaN ì œê±°
            valid_mask = ~(pd.isna(actual_y) | pd.isna(predictions_series))
            if valid_mask.sum() > 0:
                actual_y_clean = actual_y[valid_mask]
                predictions_clean = predictions_series[valid_mask]
                
                # ì´ì§„ ë¶„ë¥˜ì¸ì§€ í™•ì¸ (0ê³¼ 1ë§Œ ìˆëŠ”ì§€)
                unique_values = set(actual_y_clean.dropna().unique())
                is_binary = len(unique_values) <= 2 and all(v in [0, 1] for v in unique_values if not pd.isna(v))
                
                if is_binary:
                    # ì´ì§„ ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚°
                    predicted_classes = (predictions_clean > 0.5).astype(int)
                    metrics['accuracy'] = accuracy_score(actual_y_clean, predicted_classes)
                    metrics['f1_score'] = f1_score(actual_y_clean, predicted_classes, zero_division=0)
                    
                    # AUC ê³„ì‚° (ì˜ˆì¸¡ í™•ë¥  ì‚¬ìš©)
                    try:
                        # predictionsê°€ í™•ë¥ ì¸ì§€ í™•ì¸ (0~1 ë²”ìœ„)
                        if predictions_clean.min() >= 0 and predictions_clean.max() <= 1:
                            metrics['auc'] = roc_auc_score(actual_y_clean, predictions_clean)
                        else:
                            # í™•ë¥ ì´ ì•„ë‹ˆë©´ sigmoid ë³€í™˜ ì‹œë„
                            from scipy.special import expit
                            prob_predictions = expit(predictions_clean)
                            metrics['auc'] = roc_auc_score(actual_y_clean, prob_predictions)
                    except Exception as e:
                        if logger:
                            logger.warning(f"AUC ê³„ì‚° ì‹¤íŒ¨: {e}")
                        metrics['auc'] = None
                    
                    if logger:
                        logger.info(f"ì˜ˆì¸¡ ì™„ë£Œ: Accuracy={metrics['accuracy']:.4f}, F1={metrics['f1_score']:.4f}, AUC={metrics.get('auc', 'N/A')}")
                else:
                    # ì—°ì†í˜• ë³€ìˆ˜ì¸ ê²½ìš°
                    metrics['accuracy'] = None
                    metrics['f1_score'] = None
                    metrics['auc'] = None
                    if logger:
                        logger.info(f"ì˜ˆì¸¡ ì™„ë£Œ: í‰ê· ={predictions_clean.mean():.6f} (ì—°ì†í˜• ë³€ìˆ˜)")
            else:
                if logger:
                    logger.warning(f"ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            if logger:
                logger.warning(f"ì‹¤ì œ Y ê°’({outcome_name})ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return metrics, result_df
        
    except Exception as e:
        if logger:
            logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        raise

def estimate_causal_effect(model, identified_estimand, estimator, logger=None):
    """ì¸ê³¼íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” í•¨ìˆ˜"""
    if logger:
        logger.info("="*60)
        logger.info("ì¸ê³¼íš¨ê³¼ ì¶”ì • ì‹œì‘")
        logger.info("="*60)
    
    method_map = {
        'linear_regression': 'backdoor.linear_regression',
        'tabpfn': 'backdoor.tabpfn',
        'propensity_score': 'backdoor.propensity_score_stratification',
        'instrumental_variable': 'iv.instrumental_variable'
    }
    
    method = method_map.get(estimator, 'backdoor.linear_regression')
    
    if logger:
        logger.info(f"ì‚¬ìš©í•  ì¶”ì • ë°©ë²•: {method}")
        logger.info(f"ìš”ì²­ëœ ì¶”ì •ê¸°: {estimator}")
    
    try:
        # TabPFNì˜ ê²½ìš° ìƒˆ ë²„ì „ ì‚¬ìš© (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)
        if estimator == 'tabpfn':
            estimate = model.estimate_effect(
                identified_estimand,
                method_name=method,
                test_significance=True,
                method_params={
                    "n_estimators": 8,
                    "model_type": "auto"
                }
            )
        else:
            estimate = model.estimate_effect(
                identified_estimand,
                method_name=method,
                test_significance=True
            )
        
        if logger:
            logger.info("âœ… ì¸ê³¼íš¨ê³¼ ì¶”ì • ì„±ê³µ")
            logger.info(f"ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ (ATE): {estimate.value:.6f}")
            if hasattr(estimate, 'p_value') and estimate.p_value is not None:
                logger.info(f"P-value: {estimate.p_value:.6f}")
                significance = "ìœ ì˜í•¨" if estimate.p_value <= 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
                logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {significance}")
            
            # ì‹ ë¢°êµ¬ê°„ ì •ë³´
            if hasattr(estimate, 'confidence_intervals'):
                logger.info(f"ì‹ ë¢°êµ¬ê°„: {estimate.confidence_intervals}")
        
        return estimate
        
    except Exception as e:
        if logger:
            logger.error(f"âŒ ì¸ê³¼íš¨ê³¼ ì¶”ì • ì‹¤íŒ¨: {e}")
        raise

def calculate_refutation_pvalue(refutation_result, test_type="placebo"):
    """
    Refutation í…ŒìŠ¤íŠ¸ ê²°ê³¼ì˜ p-valueë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        refutation_result: CausalRefutation ê°ì²´
        test_type: í…ŒìŠ¤íŠ¸ íƒ€ì… ("placebo", "unobserved", "subset", "dummy")
    
    Returns:
        float: p-value (ê³„ì‚° ë¶ˆê°€ëŠ¥í•œ ê²½ìš° None)
    """
    try:
        # refutation_resultì—ì„œ refutation_results ì†ì„± í™•ì¸
        if hasattr(refutation_result, 'refutation_results') and refutation_result.refutation_results:
            # refutation_resultsëŠ” ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìŒ
            results = refutation_result.refutation_results
            if isinstance(results, list) and len(results) > 0:
                # ê° ê²°ê³¼ì—ì„œ effect ê°’ ì¶”ì¶œ
                effects = []
                for r in results:
                    if hasattr(r, 'value'):
                        effects.append(r.value)
                    elif isinstance(r, dict) and 'value' in r:
                        effects.append(r['value'])
                
                if len(effects) > 1:
                    # íš¨ê³¼ë“¤ì´ 0ê³¼ ìœ ì˜í•˜ê²Œ ë‹¤ë¥¸ì§€ t-test
                    t_stat, p_value = stats.ttest_1samp(effects, 0)
                    return p_value
        
        # refutation_resultsê°€ ì—†ìœ¼ë©´ new_effectì™€ estimated_effect ë¹„êµ
        if hasattr(refutation_result, 'new_effect') and hasattr(refutation_result, 'estimated_effect'):
            if test_type == "placebo" or test_type == "dummy":
                # new_effectê°€ 0ê³¼ ìœ ì˜í•˜ê²Œ ë‹¤ë¥¸ì§€ (ë‹¨ì¼ ê°’ì´ë¯€ë¡œ ì§ì ‘ ë¹„êµ ë¶ˆê°€)
                # ëŒ€ì‹  new_effectì˜ ì ˆëŒ€ê°’ì´ ì‘ìœ¼ë©´ í†µê³¼ë¡œ ê°„ì£¼
                return None
            elif test_type == "unobserved" or test_type == "subset":
                # new_effectì™€ estimated_effectê°€ ìœ ì˜í•˜ê²Œ ë‹¤ë¥¸ì§€
                # ë‹¨ì¼ ê°’ì´ë¯€ë¡œ ì§ì ‘ t-test ë¶ˆê°€, ì°¨ì´ì˜ ì ˆëŒ€ê°’ìœ¼ë¡œ íŒë‹¨
                return None
        
        return None
    except Exception as e:
        return None


def log_validation_results(logger, validation_results):
    """
    ê²€ì¦ ê²°ê³¼ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        logger: ë¡œê±° ê°ì²´
        validation_results (dict): ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("="*60)
    logger.info("ê²€ì¦ ê²°ê³¼ ìš”ì•½")
    logger.info("="*60)
    
    # ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸
    if validation_results.get('placebo'):
        placebo = validation_results['placebo']
        effect_change = abs(placebo.new_effect - placebo.estimated_effect)
        p_value = calculate_refutation_pvalue(placebo, "placebo")
        # íš¨ê³¼ ë³€í™”ê°€ ì‘ìœ¼ë©´ í†µê³¼ (0.01 ì´í•˜)
        status = "í†µê³¼" if effect_change < 0.01 else "ì‹¤íŒ¨"
        logger.info(f"ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸: {status}")
        logger.info(f"  - ê¸°ì¡´ ì¶”ì •ì¹˜: {placebo.estimated_effect:.6f}")
        logger.info(f"  - ê°€ìƒì²˜ì¹˜ í›„ ì¶”ì •ì¹˜: {placebo.new_effect:.6f}")
        logger.info(f"  - íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
        if p_value is not None:
            logger.info(f"  - P-value: {p_value:.6f}")
            logger.info(f"  - í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    
    # ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸
    if validation_results.get('unobserved'):
        unobserved = validation_results['unobserved']
        change_rate = abs(unobserved.new_effect - unobserved.estimated_effect) / abs(unobserved.estimated_effect) if abs(unobserved.estimated_effect) > 0 else float('inf')
        p_value = calculate_refutation_pvalue(unobserved, "unobserved")
        # ë³€í™”ìœ¨ì´ 20% ë¯¸ë§Œì´ë©´ ê°•ê±´í•¨
        status = "ê°•ê±´í•¨" if change_rate < 0.2 else "ë¯¼ê°í•¨"
        logger.info(f"ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸: {status}")
        logger.info(f"  - ê¸°ì¡´ ì¶”ì •ì¹˜: {unobserved.estimated_effect:.6f}")
        logger.info(f"  - êµë€ ì¶”ê°€ í›„ ì¶”ì •ì¹˜: {unobserved.new_effect:.6f}")
        logger.info(f"  - ë³€í™”ìœ¨: {change_rate:.2%}")
        if p_value is not None:
            logger.info(f"  - P-value: {p_value:.6f}")
            logger.info(f"  - í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    
    # ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
    if validation_results.get('subset'):
        subset = validation_results['subset']
        effect_change = abs(subset.new_effect - subset.estimated_effect)
        p_value = calculate_refutation_pvalue(subset, "subset")
        # íš¨ê³¼ ë³€í™”ê°€ ì‘ìœ¼ë©´ í†µê³¼ (10% ì´í•˜)
        change_rate = abs(subset.estimated_effect) > 0 and abs(effect_change / subset.estimated_effect) or float('inf')
        status = "í†µê³¼" if change_rate < 0.1 else "ì‹¤íŒ¨"
        logger.info(f"ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸: {status}")
        logger.info(f"  - ê¸°ì¡´ ì¶”ì •ì¹˜: {subset.estimated_effect:.6f}")
        logger.info(f"  - ë¶€ë¶„í‘œë³¸ ì¶”ì •ì¹˜: {subset.new_effect:.6f}")
        logger.info(f"  - íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
        if p_value is not None:
            logger.info(f"  - P-value: {p_value:.6f}")
            logger.info(f"  - í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
    
    # ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸
    if validation_results.get('dummy'):
        dummy = validation_results['dummy']
        p_value = calculate_refutation_pvalue(dummy, "dummy")
        # new_effectê°€ 0ì— ê°€ê¹Œìš°ë©´ í†µê³¼ (0.01 ì´í•˜)
        status = "í†µê³¼" if abs(dummy.new_effect) < 0.01 else "ì‹¤íŒ¨"
        logger.info(f"ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸: {status}")
        logger.info(f"  - ë”ë¯¸ ê²°ê³¼ ì¶”ì •ì¹˜: {dummy.new_effect:.6f}")
        if p_value is not None:
            logger.info(f"  - P-value: {p_value:.6f}")
            logger.info(f"  - í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")

def run_validation_tests(model, identified_estimand, estimate, logger=None):
    """ê²€ì¦ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ (4ê°œ í…ŒìŠ¤íŠ¸ ëª¨ë‘ í¬í•¨)"""
    if logger:
        logger.info("="*60)
        logger.info("ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘ (4ê°œ í…ŒìŠ¤íŠ¸)")
        logger.info("="*60)
    
    validation_results = {}
    
    # 1. ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ (Placebo Treatment)
    print("1ï¸âƒ£ ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    if logger:
        logger.info("1ï¸âƒ£ ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    try:
        refute_placebo = model.refute_estimate(
            identified_estimand, estimate,
            method_name="placebo_treatment_refuter",
            placebo_type="permute",
            num_simulations=100
        )
        validation_results['placebo'] = refute_placebo
        
        effect_change = abs(refute_placebo.new_effect - refute_placebo.estimated_effect)
        p_value = calculate_refutation_pvalue(refute_placebo, "placebo")
        status = "í†µê³¼" if effect_change < 0.01 else "ì‹¤íŒ¨"
        
        print(f"âœ… ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {status}")
        print(f"   ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_placebo.estimated_effect:.6f}")
        print(f"   ê°€ìƒì²˜ì¹˜ í›„ ì¶”ì •ì¹˜: {refute_placebo.new_effect:.6f}")
        print(f"   íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
        if p_value is not None:
            print(f"   P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
        
        if logger:
            logger.info("âœ… ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {status}")
            logger.info(f"ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_placebo.estimated_effect:.6f}")
            logger.info(f"ê°€ìƒì²˜ì¹˜ í›„ ì¶”ì •ì¹˜: {refute_placebo.new_effect:.6f}")
            logger.info(f"íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
            if p_value is not None:
                logger.info(f"P-value: {p_value:.6f}")
                logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
            
    except Exception as e:
        validation_results['placebo'] = None
        print(f"âŒ ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        if logger:
            logger.error(f"âŒ ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 2. ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ (Add Unobserved Common Cause)
    print("2ï¸âƒ£ ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    if logger:
        logger.info("2ï¸âƒ£ ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    try:
        refute_unobserved = model.refute_estimate(
            identified_estimand, estimate,
            method_name="add_unobserved_common_cause",
            confounders_effect_on_treatment="binary_flip",
            confounders_effect_on_outcome="linear",
            effect_strength_on_treatment=0.10,
            effect_strength_on_outcome=0.10,
            num_simulations=100
        )
        validation_results['unobserved'] = refute_unobserved
        
        change_rate = abs(refute_unobserved.new_effect - refute_unobserved.estimated_effect) / abs(refute_unobserved.estimated_effect) if abs(refute_unobserved.estimated_effect) > 0 else float('inf')
        p_value = calculate_refutation_pvalue(refute_unobserved, "unobserved")
        status = "ê°•ê±´í•¨" if change_rate < 0.2 else "ë¯¼ê°í•¨"
        
        print(f"âœ… ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {status}")
        print(f"   ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_unobserved.estimated_effect:.6f}")
        print(f"   êµë€ ì¶”ê°€ í›„ ì¶”ì •ì¹˜: {refute_unobserved.new_effect:.6f}")
        print(f"   ë³€í™”ìœ¨: {change_rate:.2%}")
        if p_value is not None:
            print(f"   P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
        
        if logger:
            logger.info("âœ… ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {status}")
            logger.info(f"ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_unobserved.estimated_effect:.6f}")
            logger.info(f"êµë€ ì¶”ê°€ í›„ ì¶”ì •ì¹˜: {refute_unobserved.new_effect:.6f}")
            logger.info(f"ë³€í™”ìœ¨: {change_rate:.2%}")
            if p_value is not None:
                logger.info(f"P-value: {p_value:.6f}")
                logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
            
    except Exception as e:
        validation_results['unobserved'] = None
        print(f"âŒ ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        if logger:
            logger.error(f"âŒ ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 3. ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (Data Subset)
    print("3ï¸âƒ£ ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    if logger:
        logger.info("3ï¸âƒ£ ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    try:
        refute_subset = model.refute_estimate(
            identified_estimand, estimate,
            method_name="data_subset_refuter",
            subset_fraction=0.8,  # 80% ì„œë¸Œì…‹ ì‚¬ìš©
            num_simulations=100
        )
        validation_results['subset'] = refute_subset
        
        effect_change = abs(refute_subset.new_effect - refute_subset.estimated_effect)
        change_rate = abs(refute_subset.estimated_effect) > 0 and abs(effect_change / refute_subset.estimated_effect) or float('inf')
        p_value = calculate_refutation_pvalue(refute_subset, "subset")
        status = "í†µê³¼" if change_rate < 0.1 else "ì‹¤íŒ¨"  # 10% ì´ë‚´ ë³€í™”ë©´ í†µê³¼
        
        print(f"âœ… ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {status}")
        print(f"   ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_subset.estimated_effect:.6f}")
        print(f"   ë¶€ë¶„í‘œë³¸ ì¶”ì •ì¹˜: {refute_subset.new_effect:.6f}")
        print(f"   íš¨ê³¼ ë³€í™”: {effect_change:.6f} ({change_rate:.2%})")
        if p_value is not None:
            print(f"   P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
        
        if logger:
            logger.info("âœ… ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {status}")
            logger.info(f"ê¸°ì¡´ ì¶”ì •ì¹˜: {refute_subset.estimated_effect:.6f}")
            logger.info(f"ë¶€ë¶„í‘œë³¸ ì¶”ì •ì¹˜: {refute_subset.new_effect:.6f}")
            logger.info(f"íš¨ê³¼ ë³€í™”: {effect_change:.6f} ({change_rate:.2%})")
            if p_value is not None:
                logger.info(f"P-value: {p_value:.6f}")
                logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
            
    except Exception as e:
        validation_results['subset'] = None
        print(f"âŒ ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        if logger:
            logger.error(f"âŒ ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # 4. ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ (Dummy Outcome)
    print("4ï¸âƒ£ ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    if logger:
        logger.info("4ï¸âƒ£ ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    try:
        refute_dummy = model.refute_estimate(
            identified_estimand, estimate,
            method_name="dummy_outcome_refuter",
            num_simulations=100
        )
        validation_results['dummy'] = refute_dummy
        
        p_value = calculate_refutation_pvalue(refute_dummy, "dummy")
        # new_effectê°€ 0ì— ê°€ê¹Œìš°ë©´ í†µê³¼ (0.01 ì´í•˜)
        status = "í†µê³¼" if abs(refute_dummy.new_effect) < 0.01 else "ì‹¤íŒ¨"
        
        print(f"âœ… ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {status}")
        print(f"   ë”ë¯¸ ê²°ê³¼ ì¶”ì •ì¹˜: {refute_dummy.new_effect:.6f}")
        print(f"   (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ, 0.01 ì´í•˜ë©´ í†µê³¼)")
        if p_value is not None:
            print(f"   P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
        
        if logger:
            logger.info("âœ… ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {status}")
            logger.info(f"ë”ë¯¸ ê²°ê³¼ ì¶”ì •ì¹˜: {refute_dummy.new_effect:.6f}")
            logger.info(f"(0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ, 0.01 ì´í•˜ë©´ í†µê³¼)")
            if p_value is not None:
                logger.info(f"P-value: {p_value:.6f}")
                logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
            
    except Exception as e:
        validation_results['dummy'] = None
        print(f"âŒ ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        if logger:
            logger.error(f"âŒ ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print("="*60)
    print("ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (4ê°œ í…ŒìŠ¤íŠ¸)")
    print("="*60)
    
    if logger:
        logger.info("="*60)
        logger.info("ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (4ê°œ í…ŒìŠ¤íŠ¸)")
        logger.info("="*60)
        log_validation_results(logger, validation_results)
    
    return validation_results

def log_sensitivity_analysis(logger, sensitivity_df, config):
    """
    ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        logger: ë¡œê±° ê°ì²´
        sensitivity_df (pd.DataFrame): ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼
        config (dict): ë¯¼ê°ë„ ë¶„ì„ ì„¤ì •
    """
    logger.info("="*60)
    logger.info("ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼")
    logger.info("="*60)
    
    logger.info(f"íš¨ê³¼ ê°•ë„ ë²”ìœ„: {config['effect_strength_range'][0]} ~ {config['effect_strength_range'][1]}")
    logger.info(f"ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ ìˆ˜: {config['num_points']}")
    logger.info(f"ì‹œë®¬ë ˆì´ì…˜ ìˆ˜: {config['num_simulations']}")
    logger.info(f"ë¶„ì„ëœ ì¡°í•© ìˆ˜: {len(sensitivity_df)}")
    
    if not sensitivity_df.empty:
        logger.info(f"íš¨ê³¼ ë²”ìœ„: {sensitivity_df['new_effect'].min():.6f} ~ {sensitivity_df['new_effect'].max():.6f}")
        
        # íš¨ê³¼ê°€ 0ì— ê°€ê¹Œìš´ ì§€ì  ì°¾ê¸°
        min_abs_effect = sensitivity_df.loc[sensitivity_df['new_effect'].abs().idxmin()]
        logger.info(f"ìµœì†Œ ì ˆëŒ€ íš¨ê³¼ ì§€ì :")
        logger.info(f"  - ì²˜ì¹˜ ê°•ë„ (et): {min_abs_effect['effect_strength_on_treatment']:.2f}")
        logger.info(f"  - ê²°ê³¼ ê°•ë„ (eo): {min_abs_effect['effect_strength_on_outcome']:.2f}")
        logger.info(f"  - íš¨ê³¼ê°’: {min_abs_effect['new_effect']:.6f}")
        
        # íš¨ê³¼ê°€ ìŒìˆ˜ì¸ ì¡°í•© ìˆ˜
        negative_effects = len(sensitivity_df[sensitivity_df['new_effect'] < 0])
        logger.info(f"ìŒìˆ˜ íš¨ê³¼ ì¡°í•© ìˆ˜: {negative_effects} ({negative_effects/len(sensitivity_df)*100:.1f}%)")
        
        # íš¨ê³¼ê°€ 0ì— ê°€ê¹Œìš´ ì¡°í•© ìˆ˜ (ì ˆëŒ€ê°’ < 0.01)
        near_zero_effects = len(sensitivity_df[sensitivity_df['new_effect'].abs() < 0.01])
        logger.info(f"0ì— ê°€ê¹Œìš´ íš¨ê³¼ ì¡°í•© ìˆ˜: {near_zero_effects} ({near_zero_effects/len(sensitivity_df)*100:.1f}%)")

def run_sensitivity_analysis(model, identified_estimand, estimate, config, logger=None):
    """
    ë¯¼ê°ë„ ë¶„ì„ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        model: CausalModel ê°ì²´
        identified_estimand: ì‹ë³„ëœ ì¶”ì •ëŸ‰ ê°ì²´
        estimate: ì¶”ì •ëœ ì¸ê³¼íš¨ê³¼ ê°ì²´
        config (dict): ë¯¼ê°ë„ ë¶„ì„ ì„¤ì •
        logger: ë¡œê±° ê°ì²´ (ì„ íƒì‚¬í•­)
    
    Returns:
        pd.DataFrame: ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
    """
    try:
        effect_range = config['effect_strength_range']
        num_points = config['num_points']
        num_simulations = config['num_simulations']
        
        grid = np.linspace(effect_range[0], effect_range[1], num_points)
        
        rows = []
        for i, et in enumerate(grid):
            for j, eo in enumerate(grid):
                try:
                    ref = model.refute_estimate(
                        identified_estimand, estimate,
                        method_name="add_unobserved_common_cause",
                        confounders_effect_on_treatment="binary_flip",
                        confounders_effect_on_outcome="linear",
                        effect_strength_on_treatment=et,
                        effect_strength_on_outcome=eo,
                        num_simulations=num_simulations
                    )
                    rows.append((et, eo, ref.new_effect))
                except Exception as e:
                    rows.append((et, eo, np.nan))
                    if logger:
                        logger.warning(f"ë¯¼ê°ë„ ë¶„ì„ ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ ({et}, {eo}) ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        sensitivity_df = pd.DataFrame(rows, columns=[
            "effect_strength_on_treatment", 
            "effect_strength_on_outcome", 
            "new_effect"
        ])
        
        if logger:
            log_sensitivity_analysis(logger, sensitivity_df, config)
        
        return sensitivity_df
        
    except Exception as e:
        if logger:
            logger.error(f"ë¯¼ê°ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

def log_heatmap_info(logger, heatmap_path, config):
    """
    íˆíŠ¸ë§µ ì •ë³´ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        logger: ë¡œê±° ê°ì²´
        heatmap_path (str): íˆíŠ¸ë§µ íŒŒì¼ ê²½ë¡œ
        config (dict): ì‹œê°í™” ì„¤ì •
    """
    logger.info("="*60)
    logger.info("ì‹œê°í™” ê²°ê³¼")
    logger.info("="*60)
    
    if heatmap_path and os.path.exists(heatmap_path):
        file_size = os.path.getsize(heatmap_path)
        logger.info(f"íˆíŠ¸ë§µ íŒŒì¼: {heatmap_path}")
        logger.info(f"íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
        logger.info(f"ì´ë¯¸ì§€ í•´ìƒë„: {config['figsize'][0]}x{config['figsize'][1]} inches")
        logger.info(f"DPI: {config['dpi']}")
    else:
        logger.warning("íˆíŠ¸ë§µ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def run_sensitivity_analysis(model, identified_estimand, estimate, logger=None):
    """ë¯¼ê°ë„ ë¶„ì„ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜"""
    if logger:
        logger.info("="*60)
        logger.info("ë¯¼ê°ë„ ë¶„ì„ ì‹¤í–‰ ì‹œì‘")
        logger.info("="*60)
        logger.info("íš¨ê³¼ ê°•ë„ ë²”ìœ„: 0.0 ~ 0.5")
        logger.info("ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ ìˆ˜: 11x11 = 121ê°œ")
        logger.info("ì‹œë®¬ë ˆì´ì…˜ ìˆ˜: 200íšŒ")
    
    try:
        grid = np.linspace(0.0, 0.5, 11)
        rows = []
        total_combinations = len(grid) * len(grid)
        processed = 0
        
        if logger:
            logger.info(f"ì´ {total_combinations}ê°œ ì¡°í•© ë¶„ì„ ì‹œì‘...")
        
        for i, et in enumerate(grid):
            for j, eo in enumerate(grid):
                processed += 1
                if logger and processed % 20 == 0:
                    logger.info(f"ì§„í–‰ë¥ : {processed}/{total_combinations} ({processed/total_combinations*100:.1f}%)")
                
                try:
                    ref = model.refute_estimate(
                        identified_estimand, estimate,
                        method_name="add_unobserved_common_cause",
                        confounders_effect_on_treatment="binary_flip",
                        confounders_effect_on_outcome="linear",
                        effect_strength_on_treatment=et,
                        effect_strength_on_outcome=eo,
                        num_simulations=200
                    )
                    rows.append((et, eo, ref.new_effect))
                except Exception as e:
                    rows.append((et, eo, np.nan))
                    if logger:
                        logger.warning(f"ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ ({et:.2f}, {eo:.2f}) ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        sensitivity_df = pd.DataFrame(rows, columns=[
            "effect_strength_on_treatment", 
            "effect_strength_on_outcome", 
            "new_effect"
        ])
        
        if logger:
            logger.info("âœ… ë¯¼ê°ë„ ë¶„ì„ ì™„ë£Œ")
            logger.info(f"ë¶„ì„ëœ ì¡°í•© ìˆ˜: {len(sensitivity_df)}")
            
            if not sensitivity_df.empty:
                valid_effects = sensitivity_df.dropna()
                logger.info(f"ìœ íš¨í•œ ê²°ê³¼ ìˆ˜: {len(valid_effects)}")
                logger.info(f"íš¨ê³¼ ë²”ìœ„: {valid_effects['new_effect'].min():.6f} ~ {valid_effects['new_effect'].max():.6f}")
                
                # íš¨ê³¼ê°€ 0ì— ê°€ê¹Œìš´ ì§€ì  ì°¾ê¸°
                min_abs_effect = valid_effects.loc[valid_effects['new_effect'].abs().idxmin()]
                logger.info(f"ìµœì†Œ ì ˆëŒ€ íš¨ê³¼ ì§€ì : et={min_abs_effect['effect_strength_on_treatment']:.2f}, eo={min_abs_effect['effect_strength_on_outcome']:.2f}")
                logger.info(f"ìµœì†Œ ì ˆëŒ€ íš¨ê³¼ê°’: {min_abs_effect['new_effect']:.6f}")
                
                # ìŒìˆ˜ íš¨ê³¼ ë¹„ìœ¨
                negative_effects = len(valid_effects[valid_effects['new_effect'] < 0])
                logger.info(f"ìŒìˆ˜ íš¨ê³¼ ì¡°í•©: {negative_effects}ê°œ ({negative_effects/len(valid_effects)*100:.1f}%)")
        
        return sensitivity_df
        
    except Exception as e:
        if logger:
            logger.error(f"âŒ ë¯¼ê°ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def create_sensitivity_heatmap(sensitivity_df, logger=None):
    """ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜"""
    if logger:
        logger.info("="*60)
        logger.info("íˆíŠ¸ë§µ ìƒì„± ì‹œì‘")
        logger.info("="*60)
    
    if sensitivity_df.empty:
        if logger:
            logger.warning("âŒ ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆì–´ íˆíŠ¸ë§µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        if logger:
            logger.info("í”¼ë²— í…Œì´ë¸” ìƒì„± ì¤‘...")
        
        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        pivot = sensitivity_df.pivot(
            index="effect_strength_on_treatment",
            columns="effect_strength_on_outcome",
            values="new_effect"
        ).sort_index(ascending=True)
        
        if logger:
            logger.info(f"í”¼ë²— í…Œì´ë¸” í¬ê¸°: {pivot.shape}")
            logger.info("íˆíŠ¸ë§µ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # íˆíŠ¸ë§µ ìƒì„±
        fig, ax = plt.subplots(figsize=(10, 8), dpi=100)
        
        im = ax.imshow(
            pivot.values,
            origin="lower",
            aspect="auto",
            extent=[
                pivot.columns.min(), pivot.columns.max(),
                pivot.index.min(), pivot.index.max()
            ],
            cmap='RdYlBu_r'
        )
        
        # ìƒ‰ìƒë§‰ëŒ€ ì¶”ê°€
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label("New Effect (after unobserved confounding)", fontsize=12)
        
        # 0-ì»¨íˆ¬ì–´ ë¼ì¸ ì¶”ê°€
        X, Y = np.meshgrid(pivot.columns.values, pivot.index.values)
        CS = ax.contour(X, Y, pivot.values, levels=[0.0], linewidths=2, colors='black')
        ax.clabel(CS, inline=True, fmt="effect=0", fontsize=10)
        
        # ì¶• ë ˆì´ë¸” ë° ì œëª©
        ax.set_xlabel("Effect Strength on Outcome (eo)", fontsize=12)
        ax.set_ylabel("Effect Strength on Treatment (et)", fontsize=12)
        ax.set_title("Sensitivity Analysis: Effect of Unobserved Confounders", fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        if logger:
            logger.info("íˆíŠ¸ë§µ ì €ì¥ ì¤‘...")
        
        # ê·¸ë¦¼ ì €ì¥
        script_dir = Path(__file__).parent.parent
        log_dir = script_dir / "log"
        log_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"sensitivity_heatmap_{timestamp}.png"
        output_path = log_dir / filename
        
        plt.savefig(output_path, dpi=100, bbox_inches='tight')
        
        if logger:
            logger.info("âœ… íˆíŠ¸ë§µ ìƒì„± ì„±ê³µ")
            logger.info(f"ì €ì¥ ê²½ë¡œ: {output_path}")
            
            # íŒŒì¼ ì •ë³´
            if os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                logger.info(f"íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
                logger.info(f"ì´ë¯¸ì§€ í•´ìƒë„: 10x8 inches, DPI: 100")
        
        return output_path
        
    except Exception as e:
        if logger:
            logger.error(f"âŒ íˆíŠ¸ë§µ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def print_summary_report(estimate, validation_results, sensitivity_df):
    """
    ì „ì²´ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        estimate: ì¶”ì •ëœ ì¸ê³¼íš¨ê³¼ ê°ì²´
        validation_results (dict): ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        sensitivity_df (pd.DataFrame): ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼
    """
    print("\n" + "="*80)
    print("ğŸ“‹ ìµœì¢… ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ")
    print("="*80)
    
    # ê¸°ë³¸ ì¶”ì • ê²°ê³¼
    print(f"\nğŸ¯ ì£¼ìš” ì¶”ì • ê²°ê³¼:")
    print(f"  - ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ (ATE): {estimate.value:.6f}")
    if hasattr(estimate, 'p_value') and estimate.p_value is not None:
        significance = "ìœ ì˜í•¨" if estimate.p_value <= 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
        print(f"  - í†µê³„ì  ìœ ì˜ì„±: {significance} (p-value: {estimate.p_value:.6f})")
    
    # ê²€ì¦ ê²°ê³¼ ìš”ì•½ (4ê°œ í…ŒìŠ¤íŠ¸)
    print(f"\nğŸ”¬ ê²€ì¦ ê²°ê³¼ ìš”ì•½ (4ê°œ í…ŒìŠ¤íŠ¸):")
    
    if validation_results.get('placebo'):
        placebo = validation_results['placebo']
        effect_change = abs(placebo.new_effect - placebo.estimated_effect)
        p_value = calculate_refutation_pvalue(placebo, "placebo")
        status = "í†µê³¼" if effect_change < 0.01 else "ì‹¤íŒ¨"
        print(f"  1. ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸: {status}")
        print(f"     - íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
        if p_value is not None:
            print(f"     - P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
    else:
        print(f"  1. ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸: ì‹¤í–‰ ì‹¤íŒ¨")
    
    if validation_results.get('unobserved'):
        unobserved = validation_results['unobserved']
        change_rate = abs(unobserved.new_effect - unobserved.estimated_effect) / abs(unobserved.estimated_effect) if abs(unobserved.estimated_effect) > 0 else float('inf')
        p_value = calculate_refutation_pvalue(unobserved, "unobserved")
        status = "ê°•ê±´í•¨" if change_rate < 0.2 else "ë¯¼ê°í•¨"
        print(f"  2. ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸: {status}")
        print(f"     - ë³€í™”ìœ¨: {change_rate:.2%}")
        if p_value is not None:
            print(f"     - P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
    else:
        print(f"  2. ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸: ì‹¤í–‰ ì‹¤íŒ¨")
    
    if validation_results.get('subset'):
        subset = validation_results['subset']
        effect_change = abs(subset.new_effect - subset.estimated_effect)
        change_rate = abs(subset.estimated_effect) > 0 and abs(effect_change / subset.estimated_effect) or float('inf')
        p_value = calculate_refutation_pvalue(subset, "subset")
        status = "í†µê³¼" if change_rate < 0.1 else "ì‹¤íŒ¨"
        print(f"  3. ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸: {status}")
        print(f"     - íš¨ê³¼ ë³€í™”ìœ¨: {change_rate:.2%}")
        if p_value is not None:
            print(f"     - P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
    else:
        print(f"  3. ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸: ì‹¤í–‰ ì‹¤íŒ¨")
    
    if validation_results.get('dummy'):
        dummy = validation_results['dummy']
        p_value = calculate_refutation_pvalue(dummy, "dummy")
        status = "í†µê³¼" if abs(dummy.new_effect) < 0.01 else "ì‹¤íŒ¨"
        print(f"  4. ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸: {status}")
        print(f"     - ë”ë¯¸ ê²°ê³¼ ì¶”ì •ì¹˜: {dummy.new_effect:.6f}")
        if p_value is not None:
            print(f"     - P-value: {p_value:.6f} ({'ìœ ì˜í•¨' if p_value <= 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'})")
    else:
        print(f"  4. ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸: ì‹¤í–‰ ì‹¤íŒ¨")
    
    # ë¯¼ê°ë„ ë¶„ì„ ìš”ì•½
    if not sensitivity_df.empty:
        print(f"\nğŸ“ˆ ë¯¼ê°ë„ ë¶„ì„ ìš”ì•½:")
        print(f"  - ë¶„ì„ëœ ì¡°í•© ìˆ˜: {len(sensitivity_df)}")
        print(f"  - íš¨ê³¼ ë²”ìœ„: {sensitivity_df['new_effect'].min():.6f} ~ {sensitivity_df['new_effect'].max():.6f}")
        
        # íš¨ê³¼ê°€ 0ì— ê°€ê¹Œìš´ ì§€ì  ì°¾ê¸°
        min_abs_effect = sensitivity_df.loc[sensitivity_df['new_effect'].abs().idxmin()]
        print(f"  - ìµœì†Œ ì ˆëŒ€ íš¨ê³¼ ì§€ì : et={min_abs_effect['effect_strength_on_treatment']:.2f}, eo={min_abs_effect['effect_strength_on_outcome']:.2f}")
    
    print(f"\nâœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ!")


# ============================================================================
# Checkpoint ì €ì¥/ë¡œë“œ í•¨ìˆ˜
# ============================================================================

def save_checkpoint(estimate, checkpoint_dir, experiment_id, graph_name=None, logger=None):
    """
    CausalEstimate ê°ì²´ë¥¼ checkpointë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        estimate: CausalEstimate ê°ì²´
        checkpoint_dir (str or Path): checkpoint ì €ì¥ ë””ë ‰í† ë¦¬
        experiment_id (str): ì‹¤í—˜ ID (íŒŒì¼ëª…ì— ì‚¬ìš©)
        graph_name (str, optional): ê·¸ë˜í”„ íŒŒì¼ëª… (metadataì— ì €ì¥)
        logger: ë¡œê±° ê°ì²´
    
    Returns:
        str: ì €ì¥ëœ checkpoint íŒŒì¼ ê²½ë¡œ
    """
    checkpoint_path = Path(checkpoint_dir)
    checkpoint_path.mkdir(parents=True, exist_ok=True)
    
    # checkpoint íŒŒì¼ëª… ìƒì„±
    checkpoint_filename = f"checkpoint_{experiment_id}.pkl"
    checkpoint_file = checkpoint_path / checkpoint_filename
    
    # experiment_idì—ì„œ graph_name ì¶”ì¶œ (ì—†ìœ¼ë©´ ì „ë‹¬ë°›ì€ ê°’ ì‚¬ìš©)
    if graph_name is None and experiment_id:
        # experiment_id í˜•ì‹: exp_0001_graph_name_treatment_outcome_estimator
        parts = experiment_id.split('_')
        if len(parts) >= 3:
            graph_name = parts[2]  # graph_name ìœ„ì¹˜
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        "experiment_id": experiment_id,
        "graph_name": graph_name,
        "treatment": estimate._treatment_name[0] if isinstance(estimate._treatment_name, list) else estimate._treatment_name,
        "outcome": estimate._outcome_name[0] if isinstance(estimate._outcome_name, list) else estimate._outcome_name,
        "ate_value": estimate.value,
        "control_value": estimate.control_value,
        "treatment_value": estimate.treatment_value,
        "estimator_type": type(estimate.estimator).__name__ if hasattr(estimate, 'estimator') else None,
        "saved_at": datetime.now().isoformat()
    }
    
    metadata_filename = f"metadata_{experiment_id}.json"
    metadata_file = checkpoint_path / metadata_filename
    
    try:
        # CausalEstimate ê°ì²´ ì €ì¥
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(estimate, f)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        if logger:
            logger.info(f"âœ… Checkpoint ì €ì¥ ì™„ë£Œ: {checkpoint_file}")
        print(f"âœ… Checkpoint ì €ì¥ ì™„ë£Œ: {checkpoint_file}")
        
        return str(checkpoint_file)
        
    except Exception as e:
        error_msg = f"Checkpoint ì €ì¥ ì‹¤íŒ¨: {e}"
        if logger:
            logger.error(error_msg)
        print(f"âŒ {error_msg}")
        raise


def load_checkpoint(checkpoint_file, logger=None):
    """
    Checkpointì—ì„œ CausalEstimate ê°ì²´ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        checkpoint_file (str or Path): checkpoint íŒŒì¼ ê²½ë¡œ
        logger: ë¡œê±° ê°ì²´
    
    Returns:
        CausalEstimate: ë¡œë“œëœ CausalEstimate ê°ì²´
    """
    checkpoint_path = Path(checkpoint_file)
    
    if not checkpoint_path.exists():
        error_msg = f"Checkpoint íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_file}"
        if logger:
            logger.error(error_msg)
        raise FileNotFoundError(error_msg)
    
    try:
        with open(checkpoint_path, 'rb') as f:
            estimate = pickle.load(f)
        
        if logger:
            logger.info(f"âœ… Checkpoint ë¡œë“œ ì™„ë£Œ: {checkpoint_file}")
        print(f"âœ… Checkpoint ë¡œë“œ ì™„ë£Œ: {checkpoint_file}")
        
        return estimate
        
    except Exception as e:
        error_msg = f"Checkpoint ë¡œë“œ ì‹¤íŒ¨: {e}"
        if logger:
            logger.error(error_msg)
        print(f"âŒ {error_msg}")
        raise


def find_checkpoint(checkpoint_dir, graph_name, treatment, outcome, estimator, logger=None):
    """
    ì£¼ì–´ì§„ ì¡°ê±´ì— ë§ëŠ” checkpoint íŒŒì¼ì„ ì°¾ëŠ” í•¨ìˆ˜
    
    Args:
        checkpoint_dir (str or Path): checkpoint ë””ë ‰í† ë¦¬
        graph_name (str): ê·¸ë˜í”„ íŒŒì¼ëª…
        treatment (str): ì²˜ì¹˜ ë³€ìˆ˜ëª…
        outcome (str): ê²°ê³¼ ë³€ìˆ˜ëª…
        estimator (str): ì¶”ì • ë°©ë²•
        logger: ë¡œê±° ê°ì²´
    
    Returns:
        str or None: ì°¾ì€ checkpoint íŒŒì¼ ê²½ë¡œ, ì—†ìœ¼ë©´ None
    """
    checkpoint_path = Path(checkpoint_dir)
    
    if not checkpoint_path.exists():
        if logger:
            logger.warning(f"Checkpoint ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {checkpoint_dir}")
        return None
    
    # metadata íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ì¡°ê±´ì— ë§ëŠ” checkpoint ì°¾ê¸°
    metadata_files = list(checkpoint_path.glob("metadata_*.json"))
    
    for metadata_file in metadata_files:
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # ì¡°ê±´ í™•ì¸: graph_name, treatment, outcome, estimator ëª¨ë‘ ì¼ì¹˜í•´ì•¼ í•¨
            metadata_graph = metadata.get("graph_name", "")
            metadata_treatment = metadata.get("treatment", "")
            metadata_outcome = metadata.get("outcome", "")
            metadata_estimator = metadata.get("estimator_type", "").lower().replace("estimator", "")
            target_estimator = estimator.lower().replace("_", "")
            
            if (metadata_graph == graph_name and
                metadata_treatment == treatment and 
                metadata_outcome == outcome and
                metadata_estimator == target_estimator):
                
                # experiment_idì—ì„œ checkpoint íŒŒì¼ëª… ìƒì„±
                experiment_id = metadata.get("experiment_id", "")
                checkpoint_file = checkpoint_path / f"checkpoint_{experiment_id}.pkl"
                
                if checkpoint_file.exists():
                    if logger:
                        logger.info(f"âœ… Checkpoint ë°œê²¬: {checkpoint_file}")
                    return str(checkpoint_file)
                    
        except Exception as e:
            if logger:
                logger.warning(f"Metadata íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({metadata_file}): {e}")
            continue
    
    if logger:
        logger.warning(f"ì¡°ê±´ì— ë§ëŠ” checkpointë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: graph={graph_name}, treatment={treatment}, outcome={outcome}, estimator={estimator}")
    return None
