"""
ë°ì´í„°ë¥¼ 3ê°œì—ì„œ 50ê°œë¡œ í™•ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
- data.csv í™•ì¥
- RESUME_JSON.json í™•ì¥
- COVERLETTERS_JSON.json í™•ì¥
- TRAININGS_JSON.json í™•ì¥
- LICENSES_JSON.json í™•ì¥
"""

import pandas as pd
import json
import random
from datetime import datetime, timedelta
from pathlib import Path

# ëœë¤ ì‹œë“œ ì„¤ì • (ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•´)
random.seed(42)

def expand_csv_data(input_file, output_file, target_count=50):
    """CSV ë°ì´í„°ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."""
    df = pd.read_csv(input_file)
    original_count = len(df)
    
    if original_count >= target_count:
        print(f"ì´ë¯¸ {original_count}ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. í™•ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ì—¬ í™•ì¥
    new_rows = []
    for i in range(original_count, target_count):
        # ê¸°ì¡´ ë°ì´í„° ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤í•˜ê²Œ ì„ íƒí•˜ì—¬ ë³µì‚¬
        base_row = df.iloc[i % original_count].copy()
        
        # ê³ ìœ  ID ìƒì„± (W10001 í˜•ì‹ ìœ ì§€)
        base_row['JHNT_MBN'] = f"W{i+1+10000:05d}"
        base_row['JHNT_CTN'] = f"C{i+1+10000:05d}"
        
        # ë‚ ì§œ ë³€í˜• (ê¸°ì¡´ ë‚ ì§œì— ëœë¤ ì¼ìˆ˜ ì¶”ê°€)
        if pd.notna(base_row.get('JHCR_DE')):
            try:
                base_date = datetime.strptime(str(base_row['JHCR_DE']), '%Y-%m-%d')
                new_date = base_date + timedelta(days=random.randint(-30, 30))
                base_row['JHCR_DE'] = new_date.strftime('%Y-%m-%d')
            except:
                pass
        
        # ìˆ«ì ê°’ì— ì•½ê°„ì˜ ë³€í˜• ì¶”ê°€
        numeric_cols = ['HOPE_WAGE_SM_AMT', 'AGE', 'BFR_OCTR_CT', 'CARR_MYCT1', 
                       'SFID_IEM_NUM', 'SFID_LTTR_NUM', 'IPS_IRDS_NMPR', 'JBHT_NMPR', 
                       'RCIT_NMPR', 'NTR_BPLC_PSNT_WAGE_AMT', 'AVG_EMPN_VS_CRQF_CT',
                       'IPS_VS_RCIT_RATE', 'IPS_VS_JBHT_RATE', 'AVG_OTIO_MYAV_RMNT_AMT',
                       'AVG_HOPE_WAGE_SM_AMT', 'CRQF_CT']
        
        for col in numeric_cols:
            if col in base_row and pd.notna(base_row[col]):
                try:
                    val = float(base_row[col])
                    # Â±10% ë²”ìœ„ ë‚´ì—ì„œ ë³€í˜•
                    base_row[col] = val * (1 + random.uniform(-0.1, 0.1))
                except:
                    pass
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ëœë¤ ì„ íƒ
        categorical_options = {
            'EMPL_STLE_CD': ['ê¸°ê°„ì˜ ì •í•¨ì´ ì—†ëŠ” ê·¼ë¡œê³„ì•½', 'ê¸°ê°„ì˜ ì •í•¨ì´ ìˆëŠ” ê·¼ë¡œê³„ì•½', 'ì¼ìš©ê·¼ë¡œ', 'ë‹¨ì‹œê°„ê·¼ë¡œ'],
            'DSPT_LABR_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'COMM_WAGE_TYCD': ['ìƒìš©', 'ì¼ìš©', 'ë‹¨ì‹œê°„'],
            'SXDS_CD': ['ë‚¨', 'ì—¬'],
            'ACCR_STCD': ['ì¡¸ì—…', 'ì¤‘í‡´', 'ì¬í•™ì¤‘'],
            'ACCR_CD': ['4ë…„ì œ ëŒ€í•™', 'ëŒ€í•™ì›', 'ê³ ë“±í•™êµ', 'ì „ë¬¸ëŒ€í•™', '2ë…„ì œ ëŒ€í•™'],
            'JHNT_PPOS_CD': ['êµ¬ì§ê¸‰ì—¬', 'ì·¨ì—…ì•Œì„ ', 'ê¸°íƒ€'],
            'JHNT_RQUT_CHNL_SECD': ['ì˜¨ë¼ì¸', 'ê³ ìš©24', 'ì˜¤í”„ë¼ì¸', 'ê¸°íƒ€'],
            'INFO_OTPB_GRAD_CD': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'MDTN_HOPE_GRD_CD': ['í•„ìš”', 'ë¶ˆí•„ìš”'],
            'IDIF_AOFR_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'EMAIL_RCYN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'DRV_PSBL_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'SAEIL_CNTC_AGRE_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'SHRS_IDIF_AOFR_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'SULC_IDIF_AOFR_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'IDIF_IQRY_AGRE_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'DLY_LABR_HOPE_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'RQAG_HOPE_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'SHSY_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'MDTN_HOPE_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'SMS_RCYN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'EMAIL_OTPB_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'MPNO_OTPB_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'AFIV_RDJT_PSBL_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'BFR_OCTR_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
            'UEPS_RECP_YN': ['ì˜ˆ', 'ì•„ë‹ˆìš”'],
        }
        
        for col, options in categorical_options.items():
            if col in base_row:
                base_row[col] = random.choice(options)
        
        # ACQ_180_YN ëœë¤ ì„¤ì • (0 ë˜ëŠ” 1)
        if 'ACQ_180_YN' in base_row:
            base_row['ACQ_180_YN'] = random.choice([0, 1])
        
        new_rows.append(base_row)
    
    # ìƒˆ í–‰ ì¶”ê°€
    new_df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)
    new_df.to_csv(output_file, index=False, encoding='utf-8')
    print(f"âœ… CSV ë°ì´í„° í™•ì¥ ì™„ë£Œ: {original_count}ê°œ â†’ {len(new_df)}ê°œ")


def expand_resume_json(input_file, output_file, target_count=50):
    """ì´ë ¥ì„œ JSON ë°ì´í„°ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."""
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    original_count = len(data)
    
    if original_count >= target_count:
        print(f"ì´ë¯¸ {original_count}ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. í™•ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ì—¬ í™•ì¥
    for i in range(original_count, target_count):
        base_item = json.loads(json.dumps(data[i % original_count]))  # Deep copy
        
        # ê³ ìœ  ID ìƒì„± (W10001 í˜•ì‹ ìœ ì§€)
        base_item['JHNT_MBN'] = f"W{i+1+10000:05d}"
        base_item['JHNT_CTN'] = f"C{i+1+10000:05d}"
        
        # ITEMS ë‚´ë¶€ì˜ ì¼ë¶€ ê°’ ë³€í˜•
        if 'RESUMES' in base_item and len(base_item['RESUMES']) > 0:
            resume = base_item['RESUMES'][0]
            if 'ITEMS' in resume:
                for item in resume['ITEMS']:
                    # ë‚ ì§œ ë³€í˜•
                    for date_key in ['HIST_STDT', 'HIST_ENDT']:
                        if date_key in item and item[date_key]:
                            try:
                                base_date = datetime.strptime(item[date_key], '%Y-%m-%d')
                                new_date = base_date + timedelta(days=random.randint(-365, 365))
                                item[date_key] = new_date.strftime('%Y-%m-%d')
                            except:
                                pass
        
        data.append(base_item)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì´ë ¥ì„œ JSON ë°ì´í„° í™•ì¥ ì™„ë£Œ: {original_count}ê°œ â†’ {len(data)}ê°œ")


def expand_coverletter_json(input_file, output_file, target_count=50):
    """ìê¸°ì†Œê°œì„œ JSON ë°ì´í„°ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."""
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    original_count = len(data)
    
    if original_count >= target_count:
        print(f"ì´ë¯¸ {original_count}ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. í™•ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ì—¬ í™•ì¥
    for i in range(original_count, target_count):
        base_item = json.loads(json.dumps(data[i % original_count]))  # Deep copy
        
        # ê³ ìœ  ID ìƒì„± (W10001 í˜•ì‹ ìœ ì§€)
        base_item['JHNT_MBN'] = f"W{i+1+10000:05d}"
        base_item['JHNT_CTN'] = f"C{i+1+10000:05d}"
        
        # SFID_NO ë³€í˜•
        if 'COVERLETTERS' in base_item:
            for coverletter in base_item['COVERLETTERS']:
                if 'SFID_NO' in coverletter:
                    coverletter['SFID_NO'] = f"{(i+1)*1000000000 + random.randint(1, 999):012d}"
        
        data.append(base_item)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ìê¸°ì†Œê°œì„œ JSON ë°ì´í„° í™•ì¥ ì™„ë£Œ: {original_count}ê°œ â†’ {len(data)}ê°œ")


def expand_training_json(input_file, output_file, target_count=50):
    """ì§ì—…í›ˆë ¨ JSON ë°ì´í„°ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."""
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    original_count = len(data)
    
    if original_count >= target_count:
        print(f"ì´ë¯¸ {original_count}ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. í™•ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ì—¬ í™•ì¥
    for i in range(original_count, target_count):
        base_item = json.loads(json.dumps(data[i % original_count]))  # Deep copy
        
        # ê³ ìœ  ID ìƒì„± (W10001 í˜•ì‹ ìœ ì§€)
        base_item['JHNT_MBN'] = f"W{i+1+10000:05d}"
        base_item['JHNT_CTN'] = f"C{i+1+10000:05d}"
        
        # ë‚ ì§œ ë³€í˜•
        if 'JHCR_DE' in base_item and base_item['JHCR_DE']:
            try:
                base_date = datetime.strptime(base_item['JHCR_DE'], '%Y-%m-%d')
                new_date = base_date + timedelta(days=random.randint(-30, 30))
                base_item['JHCR_DE'] = new_date.strftime('%Y-%m-%d')
            except:
                pass
        
        # TRAININGS ë‚´ë¶€ì˜ ë‚ ì§œ ë³€í˜•
        if 'TRAININGS' in base_item:
            for training in base_item['TRAININGS']:
                for date_key in ['TRNG_BGDE', 'TRNG_ENDE']:
                    if date_key in training and training[date_key]:
                        try:
                            base_date = datetime.strptime(training[date_key], '%Y-%m-%d')
                            new_date = base_date + timedelta(days=random.randint(-180, 180))
                            training[date_key] = new_date.strftime('%Y-%m-%d')
                        except:
                            pass
                
                # CRSE_ID ë³€í˜•
                if 'CRSE_ID' in training:
                    training['CRSE_ID'] = str(random.randint(10000000000000000, 99999999999999999))
        
        data.append(base_item)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì§ì—…í›ˆë ¨ JSON ë°ì´í„° í™•ì¥ ì™„ë£Œ: {original_count}ê°œ â†’ {len(data)}ê°œ")


def expand_license_json(input_file, output_file, target_count=50):
    """ìê²©ì¦ JSON ë°ì´í„°ë¥¼ í™•ì¥í•©ë‹ˆë‹¤."""
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    original_count = len(data)
    
    if original_count >= target_count:
        print(f"ì´ë¯¸ {original_count}ê°œ ì´ìƒì˜ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. í™•ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    # ìê²©ì¦ ì˜µì…˜
    license_options = [
        {"QULF_ITNM": "ì „ê¸°ê¸°ëŠ¥ì‚¬", "QULF_LCNS_LCFN": "êµ­ê°€ê¸°ìˆ ìê²©"},
        {"QULF_ITNM": "ì‚°ì—…ì•ˆì „ê¸°ì‚¬", "QULF_LCNS_LCFN": "êµ­ê°€ê¸°ìˆ ìê²©"},
        {"QULF_ITNM": "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬", "QULF_LCNS_LCFN": "êµ­ê°€ê¸°ìˆ ìê²©"},
        {"QULF_ITNM": "ê±´ì„¤ê¸°ê³„ì •ë¹„ê¸°ëŠ¥ì‚¬", "QULF_LCNS_LCFN": "êµ­ê°€ê¸°ìˆ ìê²©"},
        {"QULF_ITNM": "ì»´í“¨í„°í™œìš©ëŠ¥ë ¥ 2ê¸‰", "QULF_LCNS_LCFN": "êµ­ê°€ê¸°ìˆ ìê²©"},
        {"QULF_ITNM": "ê°€ìŠ¤ê¸°ëŠ¥ì‚¬", "QULF_LCNS_LCFN": "êµ­ê°€ê¸°ìˆ ìê²©"},
        {"QULF_ITNM": "ADsP(ë°ì´í„°ë¶„ì„ì¤€ì „ë¬¸ê°€)", "QULF_LCNS_LCFN": "ë¯¼ê°„ìê²©"},
        {"QULF_ITNM": "SQLD(ë°ì´í„°ë² ì´ìŠ¤ ê°œë°œì)", "QULF_LCNS_LCFN": "ë¯¼ê°„ìê²©"},
        {"QULF_ITNM": "í† ìµ", "QULF_LCNS_LCFN": "ë¯¼ê°„ìê²©"},
        {"QULF_ITNM": "í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜", "QULF_LCNS_LCFN": "ë¯¼ê°„ìê²©"},
    ]
    
    # ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ì—¬ í™•ì¥
    for i in range(original_count, target_count):
        base_item = json.loads(json.dumps(data[i % original_count]))  # Deep copy
        
        # ê³ ìœ  ID ìƒì„± (W10001 í˜•ì‹ ìœ ì§€)
        base_item['JHNT_MBN'] = f"W{i+1+10000:05d}"
        base_item['JHNT_CTN'] = f"C{i+1+10000:05d}"
        
        # LICENSES ë‚´ë¶€ì˜ ê°’ ë³€í˜•
        if 'LICENSES' in base_item:
            for license_item in base_item['LICENSES']:
                # ìê²©ì¦ ì •ë³´ ëœë¤ ì„ íƒ
                license_info = random.choice(license_options)
                license_item['QULF_ITNM'] = license_info['QULF_ITNM']
                license_item['QULF_LCNS_LCFN'] = license_info['QULF_LCNS_LCFN']
                
                # CRQF_CD ë³€í˜•
                if 'CRQF_CD' in license_item:
                    license_item['CRQF_CD'] = str(random.randint(1000000, 9999999))
                
                # ë‚ ì§œ ë³€í˜•
                if 'ETL_DT' in license_item and license_item['ETL_DT']:
                    try:
                        # ë‚ ì§œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        date_str = license_item['ETL_DT'].split()[0]
                        base_date = datetime.strptime(date_str, '%Y-%m-%d')
                        new_date = base_date + timedelta(days=random.randint(-365, 365))
                        time_part = license_item['ETL_DT'].split()[1] if ' ' in license_item['ETL_DT'] else 'ì˜¤ì „ 12:00:00'
                        license_item['ETL_DT'] = f"{new_date.strftime('%Y-%m-%d')} {time_part}"
                    except:
                        pass
        
        data.append(base_item)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ìê²©ì¦ JSON ë°ì´í„° í™•ì¥ ì™„ë£Œ: {original_count}ê°œ â†’ {len(data)}ê°œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    data_dir = Path(__file__).parent / "data"
    fixed_data_dir = data_dir / "fixed_data"
    variant_data_dir = data_dir / "variant_data"
    
    target_count = 50
    
    print(f"ğŸ“Š ë°ì´í„°ë¥¼ {target_count}ê°œë¡œ í™•ì¥í•©ë‹ˆë‹¤...\n")
    
    # CSV ë°ì´í„° í™•ì¥
    csv_input = fixed_data_dir / "data.csv"
    csv_output = fixed_data_dir / "data.csv"
    if csv_input.exists():
        expand_csv_data(csv_input, csv_output, target_count)
    else:
        print(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_input}")
    
    # JSON ë°ì´í„° í™•ì¥
    json_files = [
        ("RESUME_JSON.json", expand_resume_json),
        ("COVERLETTERS_JSON.json", expand_coverletter_json),
        ("TRAININGS_JSON.json", expand_training_json),
        ("LICENSES_JSON.json", expand_license_json),
    ]
    
    for filename, expand_func in json_files:
        json_input = variant_data_dir / filename
        json_output = variant_data_dir / filename
        if json_input.exists():
            expand_func(json_input, json_output, target_count)
        else:
            print(f"âš ï¸ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_input}")
    
    print(f"\nâœ… ëª¨ë“  ë°ì´í„° í™•ì¥ ì™„ë£Œ! (ëª©í‘œ: {target_count}ê°œ)")


if __name__ == "__main__":
    main()

