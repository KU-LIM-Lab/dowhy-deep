"""
DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ ì¸ê³¼ëª¨ë¸ êµ¬ì¶•, ì¶”ì •, ê²€ì¦ End-to-End íŒŒì´í”„ë¼ì¸

ìˆ˜ì • ì‚¬í•­:
- ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°(JSON) í†µí•© ë¡œë“œ
- JHNT_CTNì„ PKë¡œ ë°ì´í„° ë³‘í•©
- treatment íŒŒë¼ë¯¸í„°ë¥¼ argparserë¡œ ì…ë ¥ë°›ì•„ ë‹¤ì–‘í•œ ì‹¤í—˜ ì§€ì›
"""

import argparse
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import warnings
from pathlib import Path
import logging
from datetime import datetime
import os
import sys
import json
import re

# DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import dowhy
from dowhy import CausalModel
import networkx as nx

# ë¡œì»¬ DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

# ëª¨ë“ˆ ì„í¬íŠ¸
from . import preprocess
from . import estimation

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# DoWhy ë¡œê±° ë ˆë²¨ ì„¤ì •
import logging as dowhy_logging
dowhy_logging.getLogger("dowhy.causal_estimator").setLevel(dowhy_logging.WARNING)
dowhy_logging.getLogger("dowhy.causal_estimators").setLevel(dowhy_logging.WARNING)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False


def create_causal_graph(graph_file):
    """
    GML í˜•ì‹ ê·¸ë˜í”„ íŒŒì¼ì„ ì½ì–´ì„œ NetworkX ì¸ê³¼ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    ì‚¬ìš©ì ì œê³µ GML í˜•ì‹:
    graph [
        directed 1
        node [id "gps" label "gps"]
        edge [source "gps" target "hippocampus"]
    ]
    
    Args:
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (GML í˜•ì‹)
    
    Returns:
        nx.DiGraph: ì¸ê³¼ ê·¸ë˜í”„ ê°ì²´
    """
    # GML íŒŒì¼ ì½ê¸°
    with open(graph_file, 'r', encoding='utf-8') as f:
        gml_content = f.read()
    
    G = nx.DiGraph()
    
    # graph [ ... ] ë¸”ë¡ ì¶”ì¶œ
    graph_match = re.search(r'graph\s*\[(.*?)\]', gml_content, re.DOTALL)
    if not graph_match:
        raise ValueError("GML í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: 'graph [' ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    graph_body = graph_match.group(1)
    
    # directed í”Œë˜ê·¸ í™•ì¸
    directed = re.search(r'directed\s+(\d+)', graph_body)
    is_directed = directed and directed.group(1) == "1"
    
    # ëª¨ë“  node ë¸”ë¡ ì¶”ì¶œ
    node_pattern = r'node\s*\[(.*?)\]'
    for node_match in re.finditer(node_pattern, graph_body, re.DOTALL):
        node_content = node_match.group(1)
        
        # idì™€ label ì¶”ì¶œ (ë”°ì˜´í‘œ ì²˜ë¦¬)
        id_match = re.search(r'id\s+"([^"]+)"', node_content)
        label_match = re.search(r'label\s+"([^"]+)"', node_content)
        
        if id_match:
            node_id = id_match.group(1)
            label = label_match.group(1) if label_match else node_id
            G.add_node(node_id, label=label)
    
    # ëª¨ë“  edge ë¸”ë¡ ì¶”ì¶œ
    edge_pattern = r'edge\s*\[(.*?)\]'
    for edge_match in re.finditer(edge_pattern, graph_body, re.DOTALL):
        edge_content = edge_match.group(1)
        
        # sourceì™€ target ì¶”ì¶œ (ë”°ì˜´í‘œ ì²˜ë¦¬)
        source_match = re.search(r'source\s+"([^"]+)"', edge_content)
        target_match = re.search(r'target\s+"([^"]+)"', edge_content)
        
        if source_match and target_match:
            source = source_match.group(1)
            target = target_match.group(1)
            G.add_edge(source, target)
    
    # ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ë³€í™˜
    if not G.is_directed() and is_directed:
        G = G.to_directed()
    
    return G


def load_all_data(data_dir, graph_file=None):
    """
    ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°(JSON)ë¥¼ ëª¨ë‘ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        graph_file (str, optional): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ. Noneì´ë©´ data_dir/main_graph ì‚¬ìš©
    
    Returns:
        tuple: (ì •í˜•ë°ì´í„°_df, ë¹„ì •í˜•ë°ì´í„°_ë”•ì…”ë„ˆë¦¬, ì¸ê³¼ê·¸ë˜í”„)
    """
    data_path = Path(data_dir)
    
    # 1. ì •í˜• ë°ì´í„° ë¡œë“œ
    structured_data = pd.read_csv(data_path / "data.csv", encoding='utf-8')
    print(f"âœ… ì •í˜• ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(structured_data)}ê±´")
    
    # 2. ë¹„ì •í˜• ë°ì´í„°(JSON) ë¡œë“œ
    unstructured_data = {}
    
    json_files = [
        ("COVERLETTERS_JSON.json", "ìê¸°ì†Œê°œì„œ"),
        ("RESUME_JSON.json", "ì´ë ¥ì„œ"),
        ("TRAININGS_JSON.json", "ì§ì—…í›ˆë ¨"),
        ("LICENSES_JSON.json", "ìê²©ì¦")
    ]
    
    for filename, json_type in json_files:
        json_path = data_path / filename
        if json_path.exists():
            with open(json_path, 'r', encoding='utf-8') as f:
                unstructured_data[json_type] = json.load(f)
            print(f"âœ… {json_type} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(unstructured_data[json_type])}ê±´")
        else:
            print(f"âš ï¸ {json_type} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    # 3. ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ
    # graph_fileì´ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ data_dir/main_graph ì‚¬ìš©
    if graph_file is None:
        graph_file = data_path / "main_graph"
    else:
        graph_file = Path(graph_file)
    
    if not graph_file.exists():
        raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_file}")
    
    causal_graph = create_causal_graph(str(graph_file))
    print(f"âœ… ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {causal_graph.number_of_nodes()}ê°œ ë…¸ë“œ, {causal_graph.number_of_edges()}ê°œ ì—£ì§€")
    
    return structured_data, unstructured_data, causal_graph


def preprocess_unstructured_data(unstructured_data, data_dir):
    """
    ë¹„ì •í˜• ë°ì´í„°(JSON)ë¥¼ ì •í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        unstructured_data (dict): ë¹„ì •í˜• ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
    """
    preprocessor = preprocess.Preprocessor([])
    
    processed_dfs = {}
    
    # ê° JSON íƒ€ì…ë³„ë¡œ ì „ì²˜ë¦¬ ìˆ˜í–‰
    for json_type, data in unstructured_data.items():
        try:
            # JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            if json_type == "ìê¸°ì†Œê°œì„œ":
                df = _convert_coverletters_to_df(data)
            elif json_type == "ì´ë ¥ì„œ":
                df = _convert_resume_to_df(data)
            elif json_type == "ì§ì—…í›ˆë ¨":
                df = _convert_trainings_to_df(data)
            elif json_type == "ìê²©ì¦":
                df = _convert_licenses_to_df(data)
            else:
                df = pd.DataFrame()
            
            if not df.empty:
                processed_dfs[json_type] = df
                print(f"âœ… {json_type} ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê±´")
            
        except Exception as e:
            print(f"âš ï¸ {json_type} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            processed_dfs[json_type] = pd.DataFrame()
    
    return processed_dfs


def _convert_coverletters_to_df(data):
    """ìê¸°ì†Œê°œì„œ JSONì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    rows = []
    for record in data:
        seek_cust_no = record.get("SEEK_CUST_NO")
        for coverletter in record.get("COVERLETTERS", []):
            row = {"SEEK_CUST_NO": seek_cust_no}
            row["SFID_NO"] = coverletter.get("SFID_NO")
            row["SFID_IEM_NUM"] = len(coverletter.get("ITEMS", []))
            row["SFID_LTTR_NUM"] = sum(
                len(item.get("SELF_INTRO_CONT", "")) 
                for item in coverletter.get("ITEMS", [])
            )
            rows.append(row)
    return pd.DataFrame(rows)


def _convert_resume_to_df(data):
    """ì´ë ¥ì„œ JSONì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    rows = []
    for record in data:
        seek_cust_no = record.get("SEEK_CUST_NO")
        # ê°„ë‹¨í•œ ì§‘ê³„ ì •ë³´ë§Œ ì¶”ì¶œ
        row = {"SEEK_CUST_NO": seek_cust_no}
        rows.append(row)
    return pd.DataFrame(rows)


def _convert_trainings_to_df(data):
    """ì§ì—…í›ˆë ¨ JSONì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    rows = []
    for record in data:
        seek_cust_no = record.get("SEEK_CUST_NO")
        jhnt_ctn = record.get("JHNT_CTN")
        for training in record.get("TRAININGS", []):
            row = {
                "SEEK_CUST_NO": seek_cust_no,
                "JHNT_CTN": jhnt_ctn,
                "KECO_CD": training.get("KECO_CD"),
                "TRNG_JSCD": training.get("TRNG_JSCD")
            }
            rows.append(row)
    
    df = pd.DataFrame(rows)
    # JHNT_CTNë³„ë¡œ ì§‘ê³„
    if not df.empty:
        df = df.groupby("JHNT_CTN").agg({
            "KECO_CD": lambda x: ",".join([str(v) for v in x if pd.notna(v)]) if len(x) > 0 else "",
            "TRNG_JSCD": lambda x: ",".join([str(v) for v in x if pd.notna(v)]) if len(x) > 0 else ""
        }).reset_index()
        return df
    else:
        return pd.DataFrame()


def _convert_licenses_to_df(data):
    """ìê²©ì¦ JSONì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    rows = []
    for record in data:
        seek_cust_no = record.get("SEEK_CUST_NO")
        jhnt_ctn = record.get("JHNT_CTN")
        licenses = record.get("LICENSES", [])
        row = {
            "SEEK_CUST_NO": seek_cust_no,
            "JHNT_CTN": jhnt_ctn,
            "CRQF_CT": len(licenses)
        }
        rows.append(row)
    return pd.DataFrame(rows)


def merge_all_data(structured_data, processed_dfs):
    """
    ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°ë¥¼ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        structured_data (pd.DataFrame): ì •í˜• ë°ì´í„°
        processed_dfs (dict): ì „ì²˜ë¦¬ëœ ë¹„ì •í˜• ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    
    Returns:
        pd.DataFrame: ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„
    """
    # ì •í˜• ë°ì´í„°ê°€ ê¸°ì¤€ì´ ë¨
    merged_df = structured_data.copy()
    
    # JHNT_CTN ë˜ëŠ” SEEK_CUST_NOë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
    for json_type, df in processed_dfs.items():
        if df.empty:
            continue
        
        # ë³‘í•© í‚¤ ê²°ì •
        if "JHNT_CTN" in df.columns:
            merge_key = "JHNT_CTN"
        elif "SEEK_CUST_NO" in df.columns:
            # SEEK_CUST_NOê°€ ìˆëŠ” ê²½ìš°, ì •í˜• ë°ì´í„°ì˜ JHNT_MBNê³¼ ë§¤í•‘ í•„ìš”
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ skipí•˜ê³  ë‚˜ì¤‘ì— êµ¬í˜„
            continue
        else:
            continue
        
        if merge_key in merged_df.columns:
            merged_df = merged_df.merge(
                df,
                on=merge_key,
                how="left",
                suffixes=('', f'_{json_type}')
            )
            print(f"âœ… {json_type} ë°ì´í„° ë³‘í•© ì™„ë£Œ")
    
    return merged_df


def setup_logging(args):
    """ë¡œê¹…ì„ ì„¤ì •í•˜ëŠ” í†µí•© í•¨ìˆ˜"""
    if args.no_logs:
        return None
    
    # log í´ë” ìƒì„±
    script_dir = Path(__file__).parent.parent
    log_dir = script_dir / "log"
    log_dir.mkdir(exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # í„°ë¯¸ë„ ì¶œë ¥ ë¡œê¹… ì„¤ì •
    output_dir = os.environ.get('TERMINAL_OUTPUT_DIR', 'log')
    terminal_output_file = os.path.join(output_dir, f'python_output_{timestamp}.log')
    
    # í„°ë¯¸ë„ ì¶œë ¥ì„ íŒŒì¼ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜
    import sys
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    
    class TeeOutput:
        def __init__(self, *files):
            self.files = files
        def write(self, obj):
            for f in self.files:
                f.write(obj)
                f.flush()
        def flush(self):
            for f in self.files:
                f.flush()
    
    output_file = open(terminal_output_file, 'w', encoding='utf-8')
    sys.stdout = TeeOutput(original_stdout, output_file)
    sys.stderr = TeeOutput(original_stderr, output_file)
    
    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    if args.graph:
        graph_name = Path(args.graph).stem
    else:
        graph_name = "main_graph"
    log_filename = f"{graph_name}_{args.treatment}_{timestamp}.log"
    log_filepath = log_dir / log_filename
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=20,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filepath, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¶„ì„ ì‹œì‘ - {timestamp}")
    graph_display = args.graph if args.graph else f"{args.data_dir}/main_graph"
    logger.info(f"ë°ì´í„°: {args.data_dir}, ê·¸ë˜í”„: {graph_display}")
    logger.info(f"ì²˜ì¹˜: {args.treatment}, ê²°ê³¼: {args.outcome}, ì¶”ì •ë°©ë²•: {args.estimator}")
    
    return logger


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="DoWhy ì¸ê³¼ì¶”ë¡  ë¶„ì„")
    
    parser.add_argument('--data-dir', type=str, required=True, help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--graph', type=str, default=None, help='ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: data_dir/main_graph)')
    parser.add_argument('--estimator', type=str, choices=['tabpfn', 'linear_regression', 'propensity_score', 'instrumental_variable'],
                       default='linear_regression', help='ì¶”ì • ë°©ë²•')
    parser.add_argument('--treatment', type=str, required=True, help='ì²˜ì¹˜ ë³€ìˆ˜ëª…')
    parser.add_argument('--outcome', type=str, required=True, help='ê²°ê³¼ ë³€ìˆ˜ëª…')
    parser.add_argument('--no-logs', action='store_true', help='ë¡œê·¸ ì €ì¥ ë¹„í™œì„±í™”')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ì¶œë ¥ í™œì„±í™”')
    
    return parser.parse_args()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_arguments()
    logger = setup_logging(args)
    
    try:
        print(f"\nğŸš€ DoWhy ì¸ê³¼ì¶”ë¡  ë¶„ì„ ì‹œì‘")
        print(f"ğŸ“Š ë°ì´í„° ë””ë ‰í† ë¦¬: {args.data_dir}")
        graph_display = args.graph if args.graph else f"{args.data_dir}/main_graph"
        print(f"ğŸ•¸ï¸ ê·¸ë˜í”„: {graph_display}")
        print(f"ğŸ¯ ì²˜ì¹˜: {args.treatment}, ğŸ“ˆ ê²°ê³¼: {args.outcome}")
        print(f"ğŸ”§ ì¶”ì •ë°©ë²•: {args.estimator}")
        print(f"ğŸ“¦ DoWhy ë²„ì „: {dowhy.__version__}")
        print("="*60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ì¤‘...")
        # graph ì¸ìê°€ ì—†ìœ¼ë©´ data_dir/main_graphë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        graph_path = args.graph if args.graph else None
        structured_data, unstructured_data, causal_graph = load_all_data(
            args.data_dir,
            graph_path
        )
        
        # 2. ë¹„ì •í˜• ë°ì´í„° ì „ì²˜ë¦¬
        print("2ï¸âƒ£ ë¹„ì •í˜• ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        processed_dfs = preprocess_unstructured_data(unstructured_data, args.data_dir)
        
        # 3. ë°ì´í„° ë³‘í•©
        print("3ï¸âƒ£ ë°ì´í„° ë³‘í•© ì¤‘...")
        merged_df = merge_all_data(structured_data, processed_dfs)
        print(f"âœ… ìµœì¢… ë³‘í•© ë°ì´í„°: {len(merged_df)}ê±´, {len(merged_df.columns)}ê°œ ë³€ìˆ˜")
        
        if logger:
            logger.info("="*60)
            logger.info("ë°ì´í„° ë¡œë“œ ë° ë³‘í•© ì™„ë£Œ")
            logger.info("="*60)
            logger.info(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {merged_df.shape}")
            logger.info(f"ë…¸ë“œ ìˆ˜: {causal_graph.number_of_nodes()}")
            logger.info(f"ì—£ì§€ ìˆ˜: {causal_graph.number_of_edges()}")
        
        # 4. ì¸ê³¼ëª¨ë¸ ìƒì„± ë° ë¶„ì„
        print("4ï¸âƒ£ ì¸ê³¼ëª¨ë¸ ìƒì„± ì¤‘...")
        model = CausalModel(
            data=merged_df,
            treatment=args.treatment,
            outcome=args.outcome,
            graph=causal_graph
        )
        
        print("5ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì‹ë³„ ì¤‘...")
        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
        
        print("6ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì¶”ì • ì¤‘...")
        estimate = estimation.estimate_causal_effect(
            model,
            identified_estimand,
            args.estimator,
            logger
        )
        
        print("7ï¸âƒ£ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        validation_results = estimation.run_validation_tests(
            model,
            identified_estimand,
            estimate,
            logger
        )
        
        print("8ï¸âƒ£ ë¯¼ê°ë„ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
        sensitivity_df = estimation.run_sensitivity_analysis(
            model,
            identified_estimand,
            estimate,
            logger
        )
        
        print("9ï¸âƒ£ ì‹œê°í™” ìƒì„± ì¤‘...")
        heatmap_path = estimation.create_sensitivity_heatmap(
            sensitivity_df,
            logger
        ) if not sensitivity_df.empty else None
        
        print("ğŸ”Ÿ ìµœì¢… ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥ ì¤‘...")
        estimation.print_summary_report(estimate, validation_results, sensitivity_df)
        
        if logger:
            logger.info("ë¶„ì„ ì™„ë£Œ")
        
        print(f"\nâœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        if logger:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
