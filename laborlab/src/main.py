"""
DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ ì¸ê³¼ëª¨ë¸ êµ¬ì¶•, ì¶”ì •, ê²€ì¦ End-to-End íŒŒì´í”„ë¼ì¸

ìˆ˜ì • ì‚¬í•­:
- ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°(JSON) í†µí•© ë¡œë“œ
- JHNT_CTNì„ PKë¡œ ë°ì´í„° ë³‘í•©
- treatment íŒŒë¼ë¯¸í„°ë¥¼ argparserë¡œ ì…ë ¥ë°›ì•„ ë‹¤ì–‘í•œ ì‹¤í—˜ ì§€ì›
"""

import argparse
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import warnings
from pathlib import Path
import logging
from datetime import datetime
import os
import sys
import json
import re

# DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import dowhy
from dowhy import CausalModel
import networkx as nx

# ë¡œì»¬ DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

# ëª¨ë“ˆ ì„í¬íŠ¸
from . import preprocess
from . import estimation

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# DoWhy ë¡œê±° ë ˆë²¨ ì„¤ì •
import logging as dowhy_logging
dowhy_logging.getLogger("dowhy.causal_estimator").setLevel(dowhy_logging.WARNING)
dowhy_logging.getLogger("dowhy.causal_estimators").setLevel(dowhy_logging.WARNING)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False


def create_causal_graph(graph_file):
    """
    GML í˜•ì‹ ê·¸ë˜í”„ íŒŒì¼ì„ ì½ì–´ì„œ NetworkX ì¸ê³¼ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    ì‚¬ìš©ì ì œê³µ GML í˜•ì‹:
    graph [
        directed 1
        node [id "gps" label "gps"]
        edge [source "gps" target "hippocampus"]
    ]
    
    Args:
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (GML í˜•ì‹)
    
    Returns:
        nx.DiGraph: ì¸ê³¼ ê·¸ë˜í”„ ê°ì²´
    """
    # GML íŒŒì¼ ì½ê¸°
    with open(graph_file, 'r', encoding='utf-8') as f:
        gml_content = f.read()
    
    G = nx.DiGraph()
    
    # graph [ ... ] ë¸”ë¡ ì¶”ì¶œ
    graph_match = re.search(r'graph\s*\[(.*?)\]', gml_content, re.DOTALL)
    if not graph_match:
        raise ValueError("GML í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: 'graph [' ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    graph_body = graph_match.group(1)
    
    # directed í”Œë˜ê·¸ í™•ì¸
    directed = re.search(r'directed\s+(\d+)', graph_body)
    is_directed = directed and directed.group(1) == "1"
    
    # ëª¨ë“  node ë¸”ë¡ ì¶”ì¶œ
    node_pattern = r'node\s*\[(.*?)\]'
    for node_match in re.finditer(node_pattern, graph_body, re.DOTALL):
        node_content = node_match.group(1)
        
        # idì™€ label ì¶”ì¶œ (ë”°ì˜´í‘œ ì²˜ë¦¬)
        id_match = re.search(r'id\s+"([^"]+)"', node_content)
        label_match = re.search(r'label\s+"([^"]+)"', node_content)
        
        if id_match:
            node_id = id_match.group(1)
            label = label_match.group(1) if label_match else node_id
            G.add_node(node_id, label=label)
    
    # ëª¨ë“  edge ë¸”ë¡ ì¶”ì¶œ
    edge_pattern = r'edge\s*\[(.*?)\]'
    for edge_match in re.finditer(edge_pattern, graph_body, re.DOTALL):
        edge_content = edge_match.group(1)
        
        # sourceì™€ target ì¶”ì¶œ (ë”°ì˜´í‘œ ì²˜ë¦¬)
        source_match = re.search(r'source\s+"([^"]+)"', edge_content)
        target_match = re.search(r'target\s+"([^"]+)"', edge_content)
        
        if source_match and target_match:
            source = source_match.group(1)
            target = target_match.group(1)
            G.add_edge(source, target)
    
    # ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ë³€í™˜
    if not G.is_directed() and is_directed:
        G = G.to_directed()
    
    return G


def load_all_data(data_dir, graph_file=None):
    """
    ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°(JSON)ë¥¼ ëª¨ë‘ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        graph_file (str, optional): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ. Noneì´ë©´ data_dir/main_graph ì‚¬ìš©
    
    Returns:
        tuple: (íŒŒì¼ê²½ë¡œ_ë¦¬ìŠ¤íŠ¸, ì¸ê³¼ê·¸ë˜í”„)
    """
    data_path = Path(data_dir)
    
    # 1. ì •í˜• ë°ì´í„° íŒŒì¼ ê²½ë¡œ í™•ì¸ (fixed_data í´ë”ì—ì„œ)
    structured_data_path = data_path / "fixed_data" / "data.csv"
    if not structured_data_path.exists():
        # fallback: data_dir ì§ì ‘ ê²½ë¡œ
        structured_data_path = data_path / "data.csv"
    
    if not structured_data_path.exists():
        raise FileNotFoundError(f"ì •í˜• ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {structured_data_path}")
    
    print(f"âœ… ì •í˜• ë°ì´í„° íŒŒì¼ ê²½ë¡œ: {structured_data_path}")
    
    # 2. ë¹„ì •í˜• ë°ì´í„°(JSON) íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ìƒì„± (variant_data í´ë”ì—ì„œ)
    variant_data_path = data_path / "variant_data"
    file_list = []
    
    json_files = [
        ("RESUME_JSON.json", "ì´ë ¥ì„œ"),
        ("COVERLETTERS_JSON.json", "ìê¸°ì†Œê°œì„œ"),
        ("TRAININGS_JSON.json", "ì§ì—…í›ˆë ¨"),
        ("LICENSES_JSON.json", "ìê²©ì¦")
    ]
    
    # ì •í˜• ë°ì´í„° íŒŒì¼ì„ ë¨¼ì € ì¶”ê°€ (Preprocessorì˜ get_merged_df ë°©ì‹ê³¼ ì¼ì¹˜)
    file_list.append(str(structured_data_path))
    
    # JSON íŒŒì¼ ê²½ë¡œ ì¶”ê°€
    for filename, json_type in json_files:
        json_path = variant_data_path / filename
        if json_path.exists():
            file_list.append(str(json_path))
            print(f"âœ… {json_type} íŒŒì¼ ê²½ë¡œ ì¶”ê°€: {json_path}")
        else:
            print(f"âš ï¸ {json_type} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    # 3. ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ
    # graph_fileì´ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ data_dir/main_graph ë˜ëŠ” data_dir/graph_data/graph_1 ì‚¬ìš©
    if graph_file is None:
        graph_file = data_path / "main_graph"
        if not graph_file.exists():
            # fallback: graph_data í´ë”ì—ì„œ ì²« ë²ˆì§¸ ê·¸ë˜í”„ íŒŒì¼ ì°¾ê¸°
            graph_data_path = data_path / "graph_data"
            if graph_data_path.exists():
                # .dot íŒŒì¼ ì œì™¸í•˜ê³  GML í˜•ì‹ íŒŒì¼ ìš°ì„ 
                graph_files = [f for f in graph_data_path.glob("graph_*") if not f.suffix == '.dot']
                if not graph_files:
                    # .dot íŒŒì¼ë„ í¬í•¨
                    graph_files = list(graph_data_path.glob("graph_*"))
                if graph_files:
                    graph_file = sorted(graph_files)[0]  # ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
                    print(f"âš ï¸ main_graphë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ graph_data í´ë”ì˜ {graph_file.name}ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        graph_file = Path(graph_file)
    
    if not graph_file.exists():
        raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_file}")
    
    causal_graph = create_causal_graph(str(graph_file))
    print(f"âœ… ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {causal_graph.number_of_nodes()}ê°œ ë…¸ë“œ, {causal_graph.number_of_edges()}ê°œ ì—£ì§€")
    
    return file_list, causal_graph


def preprocess_and_merge_data(file_list, data_dir, api_key=None):
    """
    Preprocessor í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        file_list (list): íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ [ì •í˜•ë°ì´í„°, ì´ë ¥ì„œ, ìê¸°ì†Œê°œì„œ, ì§ì—…í›ˆë ¨, ìê²©ì¦]
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        api_key (str, optional): LLM API í‚¤
    
    Returns:
        pd.DataFrame: ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„
    """
    # Preprocessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    preprocessor = preprocess.Preprocessor([], api_key=api_key)
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½ ì €ì¥
    original_cwd = os.getcwd()
    data_path = Path(data_dir).resolve()
    
    try:
        # preprocess.pyì˜ load_variable_mappingê³¼ load_job_mappingì´ 
        # '../data/' ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, src/ í´ë”ê°€ ê¸°ì¤€ì´ ë¨
        # ë”°ë¼ì„œ data_dirì˜ ìƒìœ„ í´ë”ì—ì„œ src/ë¥¼ ì°¾ì•„ ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
        # ì¼ë°˜ì ìœ¼ë¡œ laborlab/data -> laborlab/src ê¸°ì¤€ìœ¼ë¡œ '../data/' ì‚¬ìš©
        script_dir = Path(__file__).parent  # src/ í´ë”
        laborlab_dir = script_dir.parent     # laborlab/ í´ë”
        
        # laborlab í´ë”ë¡œ ì´ë™í•˜ì—¬ preprocess.pyì˜ ìƒëŒ€ ê²½ë¡œê°€ ì‘ë™í•˜ë„ë¡ í•¨
        os.chdir(str(laborlab_dir))
        
        # file_listì˜ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        absolute_file_list = [str(Path(f).resolve()) for f in file_list]
        
        # get_merged_dfë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  íŒŒì¼ì„ ë¡œë“œ, ì „ì²˜ë¦¬, ë³‘í•©
        merged_df = preprocessor.get_merged_df(absolute_file_list)
        
        print(f"âœ… ëª¨ë“  ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ì™„ë£Œ")
        return merged_df
    
    finally:
        # ì›ë˜ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ë³µì›
        os.chdir(original_cwd)


def setup_logging(args):
    """ë¡œê¹…ì„ ì„¤ì •í•˜ëŠ” í†µí•© í•¨ìˆ˜"""
    if args.no_logs:
        return None
    
    # log í´ë” ìƒì„±
    script_dir = Path(__file__).parent.parent
    log_dir = script_dir / "log"
    log_dir.mkdir(exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # í„°ë¯¸ë„ ì¶œë ¥ ë¡œê¹… ì„¤ì •
    output_dir = os.environ.get('TERMINAL_OUTPUT_DIR', 'log')
    terminal_output_file = os.path.join(output_dir, f'python_output_{timestamp}.log')
    
    # í„°ë¯¸ë„ ì¶œë ¥ì„ íŒŒì¼ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜
    import sys
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    
    class TeeOutput:
        def __init__(self, *files):
            self.files = files
        def write(self, obj):
            for f in self.files:
                f.write(obj)
                f.flush()
        def flush(self):
            for f in self.files:
                f.flush()
    
    output_file = open(terminal_output_file, 'w', encoding='utf-8')
    sys.stdout = TeeOutput(original_stdout, output_file)
    sys.stderr = TeeOutput(original_stderr, output_file)
    
    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    if args.graph:
        graph_name = Path(args.graph).stem
    else:
        graph_name = "main_graph"
    log_filename = f"{graph_name}_{args.treatment}_{timestamp}.log"
    log_filepath = log_dir / log_filename
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=20,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filepath, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¶„ì„ ì‹œì‘ - {timestamp}")
    graph_display = args.graph if args.graph else f"{args.data_dir}/main_graph"
    logger.info(f"ë°ì´í„°: {args.data_dir}, ê·¸ë˜í”„: {graph_display}")
    logger.info(f"ì²˜ì¹˜: {args.treatment}, ê²°ê³¼: {args.outcome}, ì¶”ì •ë°©ë²•: {args.estimator}")
    
    return logger


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="DoWhy ì¸ê³¼ì¶”ë¡  ë¶„ì„")
    
    parser.add_argument('--data-dir', type=str, required=True, help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--graph', type=str, default=None, help='ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: data_dir/main_graph)')
    parser.add_argument('--estimator', type=str, choices=['tabpfn', 'linear_regression', 'propensity_score', 'instrumental_variable'],
                       default='linear_regression', help='ì¶”ì • ë°©ë²•')
    parser.add_argument('--treatment', type=str, required=True, help='ì²˜ì¹˜ ë³€ìˆ˜ëª…')
    parser.add_argument('--outcome', type=str, required=True, help='ê²°ê³¼ ë³€ìˆ˜ëª…')
    parser.add_argument('--no-logs', action='store_true', help='ë¡œê·¸ ì €ì¥ ë¹„í™œì„±í™”')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ì¶œë ¥ í™œì„±í™”')
    
    return parser.parse_args()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_arguments()
    logger = setup_logging(args)
    
    try:
        print(f"\nğŸš€ DoWhy ì¸ê³¼ì¶”ë¡  ë¶„ì„ ì‹œì‘")
        print(f"ğŸ“Š ë°ì´í„° ë””ë ‰í† ë¦¬: {args.data_dir}")
        graph_display = args.graph if args.graph else f"{args.data_dir}/main_graph"
        print(f"ğŸ•¸ï¸ ê·¸ë˜í”„: {graph_display}")
        print(f"ğŸ¯ ì²˜ì¹˜: {args.treatment}, ğŸ“ˆ ê²°ê³¼: {args.outcome}")
        print(f"ğŸ”§ ì¶”ì •ë°©ë²•: {args.estimator}")
        print(f"ğŸ“¦ DoWhy ë²„ì „: {dowhy.__version__}")
        print("="*60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ì¤‘...")
        # graph ì¸ìê°€ ì—†ìœ¼ë©´ data_dir/main_graphë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        graph_path = args.graph if args.graph else None
        file_list, causal_graph = load_all_data(
            args.data_dir,
            graph_path
        )
        
        # 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© (Preprocessor ì‚¬ìš©)
        print("2ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ì¤‘...")
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸° (ì„ íƒì‚¬í•­)
        api_key = os.environ.get('LLM_API_KEY', None)
        merged_df = preprocess_and_merge_data(file_list, args.data_dir, api_key=api_key)
        print(f"âœ… ìµœì¢… ë³‘í•© ë°ì´í„°: {len(merged_df)}ê±´, {len(merged_df.columns)}ê°œ ë³€ìˆ˜")
        
        if logger:
            logger.info("="*60)
            logger.info("ë°ì´í„° ë¡œë“œ ë° ë³‘í•© ì™„ë£Œ")
            logger.info("="*60)
            logger.info(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {merged_df.shape}")
            logger.info(f"ë…¸ë“œ ìˆ˜: {causal_graph.number_of_nodes()}")
            logger.info(f"ì—£ì§€ ìˆ˜: {causal_graph.number_of_edges()}")
        
        # 4. ì¸ê³¼ëª¨ë¸ ìƒì„± ë° ë¶„ì„
        print("4ï¸âƒ£ ì¸ê³¼ëª¨ë¸ ìƒì„± ì¤‘...")
        model = CausalModel(
            data=merged_df,
            treatment=args.treatment,
            outcome=args.outcome,
            graph=causal_graph
        )
        
        print("5ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì‹ë³„ ì¤‘...")
        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
        
        print("6ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì¶”ì • ì¤‘...")
        estimate = estimation.estimate_causal_effect(
            model,
            identified_estimand,
            args.estimator,
            logger
        )
        
        print("7ï¸âƒ£ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        validation_results = estimation.run_validation_tests(
            model,
            identified_estimand,
            estimate,
            logger
        )
        
        print("8ï¸âƒ£ ë¯¼ê°ë„ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
        sensitivity_df = estimation.run_sensitivity_analysis(
            model,
            identified_estimand,
            estimate,
            logger
        )
        
        print("9ï¸âƒ£ ì‹œê°í™” ìƒì„± ì¤‘...")
        heatmap_path = estimation.create_sensitivity_heatmap(
            sensitivity_df,
            logger
        ) if not sensitivity_df.empty else None
        
        print("ğŸ”Ÿ ìµœì¢… ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥ ì¤‘...")
        estimation.print_summary_report(estimate, validation_results, sensitivity_df)
        
        if logger:
            logger.info("ë¶„ì„ ì™„ë£Œ")
        
        print(f"\nâœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        if logger:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
