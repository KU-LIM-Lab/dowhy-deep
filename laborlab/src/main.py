"""
DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ ì¸ê³¼ëª¨ë¸ êµ¬ì¶•, ì¶”ì •, ê²€ì¦ End-to-End íŒŒì´í”„ë¼ì¸

ìˆ˜ì • ì‚¬í•­:
- ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°(JSON) í†µí•© ë¡œë“œ
- JHNT_CTNì„ PKë¡œ ë°ì´í„° ë³‘í•©
- treatment íŒŒë¼ë¯¸í„°ë¥¼ argparserë¡œ ì…ë ¥ë°›ì•„ ë‹¤ì–‘í•œ ì‹¤í—˜ ì§€ì›
"""

import argparse
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import warnings
from pathlib import Path
import logging
from datetime import datetime
import os
import sys
import json
import re
import time

# DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import dowhy
from dowhy import CausalModel
import networkx as nx

# ë¡œì»¬ DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

# ëª¨ë“ˆ ì„í¬íŠ¸
from . import preprocess
from . import estimation

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# DoWhy ë¡œê±° ë ˆë²¨ ì„¤ì •
import logging as dowhy_logging
dowhy_logging.getLogger("dowhy.causal_estimator").setLevel(dowhy_logging.WARNING)
dowhy_logging.getLogger("dowhy.causal_estimators").setLevel(dowhy_logging.WARNING)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ============================================================================
# GPT API í‚¤ ì„¤ì •
# ============================================================================
# API í‚¤ëŠ” experiment_config.jsonì˜ api_key í•„ë“œì—ì„œ ì„¤ì •í•©ë‹ˆë‹¤.
# run_batch_experiments.pyë¥¼ í†µí•´ ì‹¤í–‰í•˜ë©´ configì˜ api_keyê°€ ìë™ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
# ì§ì ‘ ì‹¤í–‰í•˜ëŠ” ê²½ìš° --api-key ì¸ìë¡œ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ============================================================================


def create_causal_graph(graph_file):
    """
    DOT í˜•ì‹ ê·¸ë˜í”„ íŒŒì¼ì„ ì½ì–´ì„œ NetworkX ì¸ê³¼ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (DOT í˜•ì‹)
    
    Returns:
        nx.DiGraph: ì¸ê³¼ ê·¸ë˜í”„ ê°ì²´
    """
    # ë¬´ì¡°ê±´ DOT í˜•ì‹ìœ¼ë¡œ íŒŒì‹±
    return _parse_dot_graph(graph_file)


def _parse_dot_graph(graph_file):
    """DOT í˜•ì‹ ê·¸ë˜í”„ íŒŒì¼ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    try:
        # pydotì„ ì‚¬ìš©í•˜ì—¬ DOT íŒŒì¼ ì½ê¸°
        import pydot
        graphs = pydot.graph_from_dot_file(graph_file)
        if not graphs:
            raise ValueError(f"DOT íŒŒì¼ì—ì„œ ê·¸ë˜í”„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_file}")
        
        # ì²« ë²ˆì§¸ ê·¸ë˜í”„ ì‚¬ìš©
        dot_graph = graphs[0]
        
        # NetworkX ê·¸ë˜í”„ë¡œ ë³€í™˜
        G = nx.drawing.nx_pydot.from_pydot(dot_graph)
        
        # ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ë³€í™˜ (digraphì¸ ê²½ìš°)
        if not G.is_directed():
            # digraphì¸ì§€ í™•ì¸
            with open(graph_file, 'r', encoding='utf-8') as f:
                content = f.read()
            if content.strip().startswith('digraph'):
                G = G.to_directed()
        
        return G
    except ImportError:
        # pydotì´ ì—†ìœ¼ë©´ ìˆ˜ë™ íŒŒì‹±
        return _parse_dot_manual(graph_file)
    except Exception as e:
        # pydot íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ íŒŒì‹± ì‹œë„
        try:
            return _parse_dot_manual(graph_file)
        except Exception as e2:
            raise ValueError(f"DOT íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {e}. ìˆ˜ë™ íŒŒì‹±ë„ ì‹¤íŒ¨: {e2}")


def _parse_dot_manual(graph_file):
    """DOT í˜•ì‹ì„ ìˆ˜ë™ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤ (pydot ì—†ì´)."""
    with open(graph_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    G = nx.DiGraph()
    
    # digraphì¸ì§€ í™•ì¸
    is_digraph = content.strip().startswith('digraph')
    
    # subgraph cluster_treatments ë¸”ë¡ ì œê±° (treatment ë©”íƒ€ë°ì´í„°ëŠ” DAGì— í¬í•¨í•˜ì§€ ì•ŠìŒ)
    # subgraph cluster_treatments { ... } ë¸”ë¡ ì œê±°
    content_without_subgraph = re.sub(
        r'subgraph\s+cluster_treatments\s*\{[^}]*\}',
        '',
        content,
        flags=re.DOTALL
    )
    
    # ë…¸ë“œ ì •ì˜ ì°¾ê¸°: node_id [label="..."]
    # ë…¸ë“œ IDëŠ” ë³€ìˆ˜ëª… (ì˜ˆ: ACQ_180_YN, cover_score ë“±)
    # ë…¸ë“œëª…ì´ ë¼ë²¨ ì •ì˜ì— ë‚˜íƒ€ë‚¨
    node_pattern = r'([A-Za-z_][A-Za-z0-9_]*)\s*\[[^\]]*label\s*=\s*"([^"]+)"'
    for match in re.finditer(node_pattern, content_without_subgraph):
        node_id = match.group(1)
        label = match.group(2)
        # T1, T2 ë“±ì˜ treatment ë…¸ë“œëŠ” ì œì™¸
        if not re.match(r'^T\d+$', node_id):
            G.add_node(node_id, label=label)
    
    # ì—£ì§€ ì°¾ê¸°: source -> target; ë˜ëŠ” source -> target [label="..."]
    # ì£¼ì„ ì²˜ë¦¬ëœ ë¼ì¸ì€ ì œì™¸
    edge_pattern = r'([A-Za-z_][A-Za-z0-9_]*)\s*->\s*([A-Za-z_][A-Za-z0-9_]*)'
    for match in re.finditer(edge_pattern, content_without_subgraph):
        source = match.group(1)
        target = match.group(2)
        # treatment ë…¸ë“œ(T1, T2 ë“±)ëŠ” ì œì™¸
        if not re.match(r'^T\d+$', source) and not re.match(r'^T\d+$', target):
            G.add_edge(source, target)
    
    # ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ë³€í™˜
    if is_digraph and not G.is_directed():
        G = G.to_directed()
    
    return G


def _parse_gml_graph(graph_file):
    """GML í˜•ì‹ ê·¸ë˜í”„ íŒŒì¼ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    # GML íŒŒì¼ ì½ê¸°
    with open(graph_file, 'r', encoding='utf-8') as f:
        gml_content = f.read()
    
    G = nx.DiGraph()
    
    # graph [ ... ] ë¸”ë¡ ì¶”ì¶œ
    graph_match = re.search(r'graph\s*\[(.*?)\]', gml_content, re.DOTALL)
    if not graph_match:
        raise ValueError("GML í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: 'graph [' ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    graph_body = graph_match.group(1)
    
    # directed í”Œë˜ê·¸ í™•ì¸
    directed = re.search(r'directed\s+(\d+)', graph_body)
    is_directed = directed and directed.group(1) == "1"
    
    # ëª¨ë“  node ë¸”ë¡ ì¶”ì¶œ
    node_pattern = r'node\s*\[(.*?)\]'
    for node_match in re.finditer(node_pattern, graph_body, re.DOTALL):
        node_content = node_match.group(1)
        
        # idì™€ label ì¶”ì¶œ (ë”°ì˜´í‘œ ì²˜ë¦¬)
        id_match = re.search(r'id\s+"([^"]+)"', node_content)
        label_match = re.search(r'label\s+"([^"]+)"', node_content)
        
        if id_match:
            node_id = id_match.group(1)
            label = label_match.group(1) if label_match else node_id
            # treatment_meta roleì´ ìˆëŠ” ë…¸ë“œëŠ” ì œì™¸
            role_match = re.search(r'role\s*=\s*"([^"]+)"', node_content)
            if role_match and role_match.group(1) == "treatment_meta":
                continue
            G.add_node(node_id, label=label)
    
    # ëª¨ë“  edge ë¸”ë¡ ì¶”ì¶œ
    edge_pattern = r'edge\s*\[(.*?)\]'
    for edge_match in re.finditer(edge_pattern, graph_body, re.DOTALL):
        edge_content = edge_match.group(1)
        
        # sourceì™€ target ì¶”ì¶œ (ë”°ì˜´í‘œ ì²˜ë¦¬)
        source_match = re.search(r'source\s+"([^"]+)"', edge_content)
        target_match = re.search(r'target\s+"([^"]+)"', edge_content)
        
        if source_match and target_match:
            source = source_match.group(1)
            target = target_match.group(1)
            # treatment_meta ë…¸ë“œëŠ” ì œì™¸
            if source not in ['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10']:
                G.add_edge(source, target)
    
    # ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ë³€í™˜
    if not G.is_directed() and is_directed:
        G = G.to_directed()
    
    return G


def load_all_data(data_dir, graph_file=None):
    """
    ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°(JSON)ë¥¼ ëª¨ë‘ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        graph_file (str, optional): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ. Noneì´ë©´ data_dir/main_graph ì‚¬ìš©
    
    Returns:
        tuple: (íŒŒì¼ê²½ë¡œ_ë¦¬ìŠ¤íŠ¸, ì¸ê³¼ê·¸ë˜í”„)
    """
    data_path = Path(data_dir)
    
    # 1. ì •í˜• ë°ì´í„° íŒŒì¼ ê²½ë¡œ í™•ì¸ (fixed_data í´ë”ì—ì„œ)
    structured_data_path = data_path / "fixed_data" / "data.csv"
    if not structured_data_path.exists():
        # fallback: data_dir ì§ì ‘ ê²½ë¡œ
        structured_data_path = data_path / "data.csv"
    
    if not structured_data_path.exists():
        raise FileNotFoundError(f"ì •í˜• ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {structured_data_path}")
    
    print(f"âœ… ì •í˜• ë°ì´í„° íŒŒì¼ ê²½ë¡œ: {structured_data_path}")
    
    # 2. ë¹„ì •í˜• ë°ì´í„°(JSON) íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ìƒì„± (variant_data í´ë”ì—ì„œ)
    variant_data_path = data_path / "variant_data"
    file_list = []
    
    json_files = [
        ("RESUME_JSON.json", "ì´ë ¥ì„œ"),
        ("COVERLETTERS_JSON.json", "ìê¸°ì†Œê°œì„œ"),
        ("TRAININGS_JSON.json", "ì§ì—…í›ˆë ¨"),
        ("LICENSES_JSON.json", "ìê²©ì¦")
    ]
    
    # ì •í˜• ë°ì´í„° íŒŒì¼ì„ ë¨¼ì € ì¶”ê°€ (Preprocessorì˜ get_merged_df ë°©ì‹ê³¼ ì¼ì¹˜)
    file_list.append(str(structured_data_path))
    
    # JSON íŒŒì¼ ê²½ë¡œ ì¶”ê°€
    for filename, json_type in json_files:
        json_path = variant_data_path / filename
        if json_path.exists():
            file_list.append(str(json_path))
            print(f"âœ… {json_type} íŒŒì¼ ê²½ë¡œ ì¶”ê°€: {json_path}")
        else:
            print(f"âš ï¸ {json_type} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    # 3. ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ
    # graph_fileì´ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ data_dir/main_graph ë˜ëŠ” data_dir/graph_data/graph_1 ì‚¬ìš©
    if graph_file is None:
        graph_file = data_path / "main_graph"
        if not graph_file.exists():
            # fallback: graph_data í´ë”ì—ì„œ ì²« ë²ˆì§¸ ê·¸ë˜í”„ íŒŒì¼ ì°¾ê¸°
            graph_data_path = data_path / "graph_data"
            if graph_data_path.exists():
                # .dot íŒŒì¼ ì œì™¸í•˜ê³  GML í˜•ì‹ íŒŒì¼ ìš°ì„ 
                graph_files = [f for f in graph_data_path.glob("graph_*") if not f.suffix == '.dot']
                if not graph_files:
                    # .dot íŒŒì¼ë„ í¬í•¨
                    graph_files = list(graph_data_path.glob("graph_*"))
                if graph_files:
                    graph_file = sorted(graph_files)[0]  # ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
                    print(f"âš ï¸ main_graphë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ graph_data í´ë”ì˜ {graph_file.name}ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        graph_file = Path(graph_file)
    
    if not graph_file.exists():
        raise FileNotFoundError(f"ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph_file}")
    
    causal_graph = create_causal_graph(str(graph_file))
    print(f"âœ… ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {causal_graph.number_of_nodes()}ê°œ ë…¸ë“œ, {causal_graph.number_of_edges()}ê°œ ì—£ì§€")
    
    return file_list, causal_graph


def clean_dataframe_for_causal_model(df, required_vars=None, logger=None):
    """
    CausalModel ìƒì„± ì „ì— ë°ì´í„°í”„ë ˆì„ì„ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜
    - Logger ê°ì²´ë‚˜ ë‹¤ë¥¸ ë¹„ë°ì´í„° íƒ€ì… ì»¬ëŸ¼ ì œê±°
    - ìˆ«ì/ë¬¸ìì—´/ë¶ˆë¦° íƒ€ì…ë§Œ ìœ ì§€
    - required_varsì— ì§€ì •ëœ ë³€ìˆ˜ëŠ” í•­ìƒ ìœ ì§€
    
    Args:
        df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        required_vars (list, optional): ë°˜ë“œì‹œ ìœ ì§€í•´ì•¼ í•  ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ (treatment, outcome ë“±)
        logger: ë¡œê±° ê°ì²´
    
    Returns:
        pd.DataFrame: ì •ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    df_clean = df.copy()
    cols_to_drop = []
    
    if required_vars is None:
        required_vars = []
    
    for col in df_clean.columns:
        # object íƒ€ì… ì»¬ëŸ¼ í™•ì¸
        if df_clean[col].dtype == 'object':
            if len(df_clean) > 0:
                # NaNì´ ì•„ë‹Œ ì²« ë²ˆì§¸ ê°’ í™•ì¸
                non_null_values = df_clean[col].dropna()
                if len(non_null_values) > 0:
                    first_val = non_null_values.iloc[0]
                    # Logger ê°™ì€ ê°ì²´ íƒ€ì…ì¸ì§€ í™•ì¸
                    is_logger_object = isinstance(first_val, logging.Logger) or 'Logger' in str(type(first_val))
                    is_invalid_type = not isinstance(first_val, (str, int, float, bool, type(None)))
                    
                    if is_logger_object or is_invalid_type:
                        # í•„ìˆ˜ ë³€ìˆ˜ì¸ ê²½ìš° Logger ê°ì²´ë¥¼ NaNìœ¼ë¡œ ëŒ€ì²´
                        if col in required_vars:
                            if logger:
                                logger.warning(f"í•„ìˆ˜ ë³€ìˆ˜ '{col}'ì˜ ê°’ì´ ê°ì²´ íƒ€ì…({type(first_val).__name__})ì´ì–´ì„œ NaNìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                            else:
                                print(f"âš ï¸ í•„ìˆ˜ ë³€ìˆ˜ '{col}'ì˜ ê°’ì´ ê°ì²´ íƒ€ì…({type(first_val).__name__})ì´ì–´ì„œ NaNìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                            df_clean[col] = np.nan
                        else:
                            # í•„ìˆ˜ ë³€ìˆ˜ê°€ ì•„ë‹Œ ê²½ìš° ì»¬ëŸ¼ ì œê±°
                            cols_to_drop.append(col)
                            if logger:
                                logger.warning(f"ì»¬ëŸ¼ '{col}'ì´ ê°ì²´ íƒ€ì…({type(first_val).__name__})ì´ì–´ì„œ ì œê±°í•©ë‹ˆë‹¤.")
                            else:
                                print(f"âš ï¸ ì»¬ëŸ¼ '{col}'ì´ ê°ì²´ íƒ€ì…({type(first_val).__name__})ì´ì–´ì„œ ì œê±°í•©ë‹ˆë‹¤.")
    
    if cols_to_drop:
        df_clean = df_clean.drop(columns=cols_to_drop)
        if logger:
            logger.info(f"ì œê±°ëœ ì»¬ëŸ¼: {cols_to_drop}")
        else:
            print(f"ì œê±°ëœ ì»¬ëŸ¼: {cols_to_drop}")
    
    return df_clean


def preprocess_and_merge_data(file_list, data_dir, api_key=None):
    """
    Preprocessor í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        file_list (list): íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ [ì •í˜•ë°ì´í„°, ì´ë ¥ì„œ, ìê¸°ì†Œê°œì„œ, ì§ì—…í›ˆë ¨, ìê²©ì¦]
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        api_key (str, optional): LLM API í‚¤
    
    Returns:
        pd.DataFrame: ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„
    """
    # Preprocessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    # preprocess.pyëŠ” __file__ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œë¥¼ ê³„ì‚°í•˜ë¯€ë¡œ ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½ ë¶ˆí•„ìš”
    preprocessor = preprocess.Preprocessor([], api_key=api_key)
    
    # file_listì˜ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    absolute_file_list = [str(Path(f).resolve()) for f in file_list]
    
    # get_merged_dfë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  íŒŒì¼ì„ ë¡œë“œ, ì „ì²˜ë¦¬, ë³‘í•©
    merged_df = preprocessor.get_merged_df(absolute_file_list)
    
    print(f"âœ… ëª¨ë“  ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ì™„ë£Œ")
    return merged_df


def save_predictions_to_excel(df_with_predictions, output_dir=None, filename=None, logger=None):
    """
    ì˜ˆì¸¡ê°’ì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„ì„ Excel íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        df_with_predictions: ì˜ˆì¸¡ê°’ì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ log í´ë” ì‚¬ìš©)
        filename: íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
        logger: ë¡œê±° ê°ì²´
    
    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    if output_dir is None:
        script_dir = Path(__file__).parent.parent
        output_dir = script_dir / "log"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"predictions_{timestamp}.xlsx"
    
    filepath = output_dir / filename
    
    # Excel íŒŒì¼ë¡œ ì €ì¥
    df_with_predictions.to_excel(filepath, index=False, engine='openpyxl')
    
    if logger:
        logger.info(f"ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
        file_size = os.path.getsize(filepath)
        logger.info(f"íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
    
    return str(filepath)


def setup_logging(args):
    """ë¡œê¹…ì„ ì„¤ì •í•˜ëŠ” í†µí•© í•¨ìˆ˜"""
    if args.no_logs:
        return None
    
    # log í´ë” ìƒì„±
    script_dir = Path(__file__).parent.parent
    log_dir = script_dir / "log"
    log_dir.mkdir(exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # í„°ë¯¸ë„ ì¶œë ¥ ë¡œê¹… ì„¤ì •
    output_dir = os.environ.get('TERMINAL_OUTPUT_DIR', 'log')
    terminal_output_file = os.path.join(output_dir, f'python_output_{timestamp}.log')
    
    # í„°ë¯¸ë„ ì¶œë ¥ì„ íŒŒì¼ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜
    import sys
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    
    class TeeOutput:
        def __init__(self, *files):
            self.files = files
        def write(self, obj):
            for f in self.files:
                f.write(obj)
                f.flush()
        def flush(self):
            for f in self.files:
                f.flush()
    
    output_file = open(terminal_output_file, 'w', encoding='utf-8')
    sys.stdout = TeeOutput(original_stdout, output_file)
    sys.stderr = TeeOutput(original_stderr, output_file)
    
    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    if args.graph:
        graph_name = Path(args.graph).stem
    else:
        graph_name = "main_graph"
    log_filename = f"{graph_name}_{args.treatment}_{timestamp}.log"
    log_filepath = log_dir / log_filename
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=20,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filepath, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¶„ì„ ì‹œì‘ - {timestamp}")
    graph_display = args.graph if args.graph else f"{args.data_dir}/main_graph"
    logger.info(f"ë°ì´í„°: {args.data_dir}, ê·¸ë˜í”„: {graph_display}")
    logger.info(f"ì²˜ì¹˜: {args.treatment}, ê²°ê³¼: {args.outcome}, ì¶”ì •ë°©ë²•: {args.estimator}")
    
    return logger


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="DoWhy ì¸ê³¼ì¶”ë¡  ë¶„ì„")
    
    parser.add_argument('--data-dir', type=str, required=True, help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--graph', type=str, default=None, help='ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: data_dir/main_graph)')
    parser.add_argument('--estimator', type=str, choices=['tabpfn', 'linear_regression', 'propensity_score', 'instrumental_variable'],
                       default='linear_regression', help='ì¶”ì • ë°©ë²•')
    parser.add_argument('--treatment', type=str, required=True, help='ì²˜ì¹˜ ë³€ìˆ˜ëª…')
    parser.add_argument('--outcome', type=str, required=True, help='ê²°ê³¼ ë³€ìˆ˜ëª…')
    parser.add_argument('--api-key', type=str, default=None, help='GPT API í‚¤ (experiment_config.jsonì—ì„œ ì„¤ì •)')
    parser.add_argument('--no-logs', action='store_true', help='ë¡œê·¸ ì €ì¥ ë¹„í™œì„±í™”')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ì¶œë ¥ í™œì„±í™”')
    
    return parser.parse_args()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_arguments()
    logger = setup_logging(args)
    
    try:
        # ì „ì²´ ì‹œì‘ ì‹œê°„
        total_start_time = time.time()
        step_times = {}
        
        print(f"\nğŸš€ DoWhy ì¸ê³¼ì¶”ë¡  ë¶„ì„ ì‹œì‘")
        print(f"ğŸ“Š ë°ì´í„° ë””ë ‰í† ë¦¬: {args.data_dir}")
        graph_display = args.graph if args.graph else f"{args.data_dir}/main_graph"
        print(f"ğŸ•¸ï¸ ê·¸ë˜í”„: {graph_display}")
        print(f"ğŸ¯ ì²˜ì¹˜: {args.treatment}, ğŸ“ˆ ê²°ê³¼: {args.outcome}")
        print(f"ğŸ”§ ì¶”ì •ë°©ë²•: {args.estimator}")
        print(f"ğŸ“¦ DoWhy ë²„ì „: {dowhy.__version__}")
        print("="*60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ì¤‘...")
        step_start = time.time()
        # graph ì¸ìê°€ ì—†ìœ¼ë©´ data_dir/main_graphë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        graph_path = args.graph if args.graph else None
        file_list, causal_graph = load_all_data(
            args.data_dir,
            graph_path
        )
        step_times['ë°ì´í„° ë¡œë“œ'] = time.time() - step_start
        print(f"â±ï¸ ë°ì´í„° ë¡œë“œ ì†Œìš” ì‹œê°„: {step_times['ë°ì´í„° ë¡œë“œ']:.2f}ì´ˆ")
        
        # 2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© (Preprocessor ì‚¬ìš©)
        print("2ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ì¤‘...")
        print("âš¡ JSON íŒŒì¼ 4ê°œ(ì´ë ¥ì„œ, ìê¸°ì†Œê°œì„œ, ì§ì—…í›ˆë ¨, ìê²©ì¦) ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
        step_start = time.time()
        # API í‚¤ëŠ” config íŒŒì¼ì—ì„œ ì„¤ì • (run_batch_experiments.pyë¥¼ í†µí•´ ì „ë‹¬ë¨)
        api_key = args.api_key
        if api_key:
            print(f"ğŸ”‘ API í‚¤: config íŒŒì¼ì—ì„œ ì‚¬ìš©")
        else:
            print(f"âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        merged_df = preprocess_and_merge_data(file_list, args.data_dir, api_key=api_key)
        step_times['ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•©'] = time.time() - step_start
        print(f"â±ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ì†Œìš” ì‹œê°„: {step_times['ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•©']:.2f}ì´ˆ")
        print(f"âœ… ìµœì¢… ë³‘í•© ë°ì´í„°: {len(merged_df)}ê±´, {len(merged_df.columns)}ê°œ ë³€ìˆ˜")
        
        # merged_dfì˜ head() ë¡œê¹…
        print("\n" + "="*60)
        print("ğŸ“Š ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„ ë¯¸ë¦¬ë³´ê¸° (head):")
        print("="*60)
        print(merged_df.head())
        print("="*60 + "\n")
        
        if logger:
            logger.info("="*60)
            logger.info("ë°ì´í„° ë¡œë“œ ë° ë³‘í•© ì™„ë£Œ")
            logger.info("="*60)
            logger.info(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {merged_df.shape}")
            logger.info(f"ì»¬ëŸ¼ ëª©ë¡: {list(merged_df.columns)}")
            logger.info(f"ë…¸ë“œ ìˆ˜: {causal_graph.number_of_nodes()}")
            logger.info(f"ì—£ì§€ ìˆ˜: {causal_graph.number_of_edges()}")
            logger.info("\në³‘í•©ëœ ë°ì´í„°í”„ë ˆì„ head():")
            logger.info("\n" + str(merged_df.head()))
        
        # 3. ë°ì´í„° ì •ë¦¬ (Logger ê°ì²´ ë“± ì œê±°)
        print("3ï¸âƒ£ ë°ì´í„° ì •ë¦¬ ì¤‘...")
        step_start = time.time()
        
        # ê·¸ë˜í”„ì— ì •ì˜ëœ ëª¨ë“  ë³€ìˆ˜ ì¶”ì¶œ
        graph_variables = set(causal_graph.nodes())
        print(f"ğŸ“‹ ê·¸ë˜í”„ì— ì •ì˜ëœ ë³€ìˆ˜ ìˆ˜: {len(graph_variables)}ê°œ")
        
        # treatmentì™€ outcome ë³€ìˆ˜ëŠ” ë°˜ë“œì‹œ ìœ ì§€í•´ì•¼ í•¨
        required_vars = [args.treatment, args.outcome]
        # ê·¸ë˜í”„ì— ì •ì˜ëœ ëª¨ë“  ë³€ìˆ˜ë„ í•„ìˆ˜ ë³€ìˆ˜ë¡œ ì¶”ê°€
        required_vars.extend(list(graph_variables))
        required_vars = list(set(required_vars))  # ì¤‘ë³µ ì œê±°
        
        # Logger ê°ì²´ê°€ ë°ì´í„°í”„ë ˆì„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì‚¬ì „ ê²€ì‚¬
        logger_columns = []
        for col in merged_df.columns:
            if merged_df[col].dtype == 'object' and len(merged_df) > 0:
                non_null_values = merged_df[col].dropna()
                if len(non_null_values) > 0:
                    first_val = non_null_values.iloc[0]
                    # Logger ê°ì²´ì¸ì§€ í™•ì¸
                    if isinstance(first_val, logging.Logger) or 'Logger' in str(type(first_val)):
                        logger_columns.append((col, type(first_val).__name__))
                        if logger:
                            logger.error(f"âš ï¸ ê²½ê³ : ì»¬ëŸ¼ '{col}'ì— Logger ê°ì²´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! (íƒ€ì…: {type(first_val).__name__})")
                        else:
                            print(f"âš ï¸ ê²½ê³ : ì»¬ëŸ¼ '{col}'ì— Logger ê°ì²´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! (íƒ€ì…: {type(first_val).__name__})")
        
        if logger_columns:
            print(f"\nâŒ ì˜¤ë¥˜: ë‹¤ìŒ ì»¬ëŸ¼ì— Logger ê°ì²´ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
            for col, col_type in logger_columns:
                print(f"   - {col} (íƒ€ì…: {col_type})")
            print(f"\nì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°ì´í„° ì •ë¦¬ ê³¼ì •ì—ì„œ Logger ê°ì²´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.")
        
        merged_df_clean = clean_dataframe_for_causal_model(merged_df, required_vars=required_vars, logger=logger)
        
        # ê·¸ë˜í”„ ë³€ìˆ˜ì™€ ë°ì´í„° ë³€ìˆ˜ ì¼ì¹˜ ì—¬ë¶€ ê²€ì¦
        data_variables = set(merged_df_clean.columns)
        missing_graph_vars = graph_variables - data_variables
        extra_data_vars = data_variables - graph_variables
        
        if missing_graph_vars:
            print(f"\nâš ï¸ ê²½ê³ : ê·¸ë˜í”„ì— ì •ì˜ëœ ë³€ìˆ˜ ì¤‘ ë°ì´í„°ì— ì—†ëŠ” ë³€ìˆ˜:")
            for var in sorted(missing_graph_vars):
                print(f"   - {var}")
            if logger:
                logger.warning(f"ê·¸ë˜í”„ì— ì •ì˜ëœ ë³€ìˆ˜ ì¤‘ ë°ì´í„°ì— ì—†ëŠ” ë³€ìˆ˜: {sorted(missing_graph_vars)}")
        
        # ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ ì œê±° (í•„ìˆ˜ ë³€ìˆ˜ ì œì™¸)
        essential_vars = {args.treatment, args.outcome, "SEEK_CUST_NO", "JHNT_CTN", "JHNT_MBN"}
        vars_to_keep = set()
        
        # 1. ê·¸ë˜í”„ì— ì •ì˜ëœ ëª¨ë“  ë³€ìˆ˜ ì¶”ê°€
        vars_to_keep.update(graph_variables)
        
        # 2. í•„ìˆ˜ ë³€ìˆ˜ ì¶”ê°€ (treatment, outcome, ë³‘í•© í‚¤)
        vars_to_keep.update(essential_vars)
        
        # 3. ì‹¤ì œ ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” ë³€ìˆ˜ë§Œ í•„í„°ë§
        vars_to_keep = vars_to_keep & data_variables
        
        # 4. ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ ì œê±°
        vars_to_remove = data_variables - vars_to_keep
        
        if vars_to_remove:
            print(f"\nğŸ—‘ï¸ ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ ì œê±° ì¤‘ ({len(vars_to_remove)}ê°œ):")
            for var in sorted(list(vars_to_remove)[:20]):  # ì²˜ìŒ 20ê°œë§Œ ì¶œë ¥
                print(f"   - {var}")
            if len(vars_to_remove) > 20:
                print(f"   ... ì™¸ {len(vars_to_remove) - 20}ê°œ")
            if logger:
                logger.info(f"ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ ì œê±°: {sorted(list(vars_to_remove))}")
            
            # ë³€ìˆ˜ ì œê±°
            merged_df_clean = merged_df_clean[list(vars_to_keep)]
            print(f"âœ… ë³€ìˆ˜ ì œê±° ì™„ë£Œ: {len(merged_df_clean.columns)}ê°œ ë³€ìˆ˜ ìœ ì§€")
        
        step_times['ë°ì´í„° ì •ë¦¬'] = time.time() - step_start
        print(f"â±ï¸ ë°ì´í„° ì •ë¦¬ ì†Œìš” ì‹œê°„: {step_times['ë°ì´í„° ì •ë¦¬']:.2f}ì´ˆ")
        print(f"âœ… ì •ë¦¬ëœ ë°ì´í„°: {len(merged_df_clean)}ê±´, {len(merged_df_clean.columns)}ê°œ ë³€ìˆ˜")
        
        # ìµœì¢… ê²€ì¦: ê·¸ë˜í”„ ë³€ìˆ˜ì™€ ë°ì´í„° ë³€ìˆ˜ ì¼ì¹˜ ì—¬ë¶€
        final_data_variables = set(merged_df_clean.columns)
        final_missing_graph_vars = graph_variables - final_data_variables
        final_extra_data_vars = final_data_variables - graph_variables - essential_vars
        
        if final_missing_graph_vars:
            print(f"\nâš ï¸ ê²½ê³ : ê·¸ë˜í”„ì— ì •ì˜ëœ ë³€ìˆ˜ ì¤‘ ìµœì¢… ë°ì´í„°ì— ì—†ëŠ” ë³€ìˆ˜:")
            for var in sorted(final_missing_graph_vars):
                print(f"   - {var}")
            if logger:
                logger.warning(f"ê·¸ë˜í”„ì— ì •ì˜ëœ ë³€ìˆ˜ ì¤‘ ìµœì¢… ë°ì´í„°ì— ì—†ëŠ” ë³€ìˆ˜: {sorted(final_missing_graph_vars)}")
        
        if final_extra_data_vars:
            print(f"\nâš ï¸ ê²½ê³ : ìµœì¢… ë°ì´í„°ì— ìˆì§€ë§Œ ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ ({len(final_extra_data_vars)}ê°œ):")
            for var in sorted(list(final_extra_data_vars)[:10]):
                print(f"   - {var}")
            if len(final_extra_data_vars) > 10:
                print(f"   ... ì™¸ {len(final_extra_data_vars) - 10}ê°œ")
            if logger:
                logger.warning(f"ìµœì¢… ë°ì´í„°ì— ìˆì§€ë§Œ ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜: {sorted(list(final_extra_data_vars))}")
        
        # treatmentì™€ outcome ë³€ìˆ˜ê°€ ìˆëŠ”ì§€ í™•ì¸
        missing_vars = [var for var in [args.treatment, args.outcome] if var not in merged_df_clean.columns]
        if missing_vars:
            raise ValueError(f"í•„ìˆ˜ ë³€ìˆ˜ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤: {missing_vars}")
        
        # ê·¸ë˜í”„ì˜ í•µì‹¬ ë³€ìˆ˜ë“¤ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
        critical_missing = missing_graph_vars - {args.treatment, args.outcome}  # treatment/outcomeì€ ì´ë¯¸ ì²´í¬ë¨
        if critical_missing:
            print(f"\nâŒ ì˜¤ë¥˜: ê·¸ë˜í”„ì˜ í•µì‹¬ ë³€ìˆ˜ë“¤ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤:")
            for var in sorted(critical_missing):
                print(f"   - {var}")
            if logger:
                logger.error(f"ê·¸ë˜í”„ì˜ í•µì‹¬ ë³€ìˆ˜ë“¤ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤: {sorted(critical_missing)}")
            # ê²½ê³ ë§Œ ì¶œë ¥í•˜ê³  ê³„ì† ì§„í–‰ (ì¼ë¶€ ë³€ìˆ˜ê°€ ì—†ì–´ë„ ë¶„ì„ ê°€ëŠ¥í•  ìˆ˜ ìˆìŒ)
            # raise ValueError(f"ê·¸ë˜í”„ì˜ í•µì‹¬ ë³€ìˆ˜ë“¤ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤: {sorted(critical_missing)}")
        
        # 4. ì¸ê³¼ëª¨ë¸ ìƒì„± ë° ë¶„ì„
        print("4ï¸âƒ£ ì¸ê³¼ëª¨ë¸ ìƒì„± ì¤‘...")
        step_start = time.time()
        model = CausalModel(
            data=merged_df_clean,
            treatment=args.treatment,
            outcome=args.outcome,
            graph=causal_graph
        )
        step_times['ì¸ê³¼ëª¨ë¸ ìƒì„±'] = time.time() - step_start
        print(f"â±ï¸ ì¸ê³¼ëª¨ë¸ ìƒì„± ì†Œìš” ì‹œê°„: {step_times['ì¸ê³¼ëª¨ë¸ ìƒì„±']:.2f}ì´ˆ")
        
        print("5ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì‹ë³„ ì¤‘...")
        step_start = time.time()
        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
        step_times['ì¸ê³¼íš¨ê³¼ ì‹ë³„'] = time.time() - step_start
        print(f"â±ï¸ ì¸ê³¼íš¨ê³¼ ì‹ë³„ ì†Œìš” ì‹œê°„: {step_times['ì¸ê³¼íš¨ê³¼ ì‹ë³„']:.2f}ì´ˆ")
        
        print("6ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì¶”ì • ì¤‘...")
        step_start = time.time()
        estimate = estimation.estimate_causal_effect(
            model,
            identified_estimand,
            args.estimator,
            logger
        )
        step_times['ì¸ê³¼íš¨ê³¼ ì¶”ì •'] = time.time() - step_start
        print(f"â±ï¸ ì¸ê³¼íš¨ê³¼ ì¶”ì • ì†Œìš” ì‹œê°„: {step_times['ì¸ê³¼íš¨ê³¼ ì¶”ì •']:.2f}ì´ˆ")
        
        step_start = time.time()
        # ì˜ˆì¸¡ ì „ì— í•œ ë²ˆ ë” Logger ê°ì²´ ì œê±° (ì•ˆì „ì¥ì¹˜)
        # treatmentì™€ outcome ë³€ìˆ˜ëŠ” í•„ìˆ˜ì´ë¯€ë¡œ ìœ ì§€
        essential_vars_for_pred = {args.treatment, args.outcome}
        merged_df_clean_final = clean_dataframe_for_causal_model(
            merged_df_clean, 
            required_vars=list(essential_vars_for_pred), 
            logger=logger
        )
        accuracy, df_with_predictions = estimation.predict_conditional_expectation(estimate, merged_df_clean_final, logger)
        step_times['ì˜ˆì¸¡'] = time.time() - step_start
        print(f"â±ï¸ ì˜ˆì¸¡ ì†Œìš” ì‹œê°„: {step_times['ì˜ˆì¸¡']:.2f}ì´ˆ")
        print(f"âœ… ì·¨ì—… í™•ë¥  ì˜ˆì¸¡ ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        step_start = time.time()
        excel_path = save_predictions_to_excel(df_with_predictions, logger=logger)
        step_times['ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥'] = time.time() - step_start
        print(f"â±ï¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì†Œìš” ì‹œê°„: {step_times['ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥']:.2f}ì´ˆ")
        print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {excel_path}")

        print("7ï¸âƒ£ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        step_start = time.time()
        validation_results = estimation.run_validation_tests(
            model,
            identified_estimand,
            estimate,
            logger
        )
        step_times['ê²€ì¦ í…ŒìŠ¤íŠ¸'] = time.time() - step_start
        print(f"â±ï¸ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì†Œìš” ì‹œê°„: {step_times['ê²€ì¦ í…ŒìŠ¤íŠ¸']:.2f}ì´ˆ")
        
        print("8ï¸âƒ£ ë¯¼ê°ë„ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
        step_start = time.time()
        sensitivity_df = estimation.run_sensitivity_analysis(
            model,
            identified_estimand,
            estimate,
            logger
        )
        step_times['ë¯¼ê°ë„ ë¶„ì„'] = time.time() - step_start
        print(f"â±ï¸ ë¯¼ê°ë„ ë¶„ì„ ì†Œìš” ì‹œê°„: {step_times['ë¯¼ê°ë„ ë¶„ì„']:.2f}ì´ˆ")
        
        print("9ï¸âƒ£ ì‹œê°í™” ìƒì„± ì¤‘...")
        step_start = time.time()
        heatmap_path = estimation.create_sensitivity_heatmap(
            sensitivity_df,
            logger
        ) if not sensitivity_df.empty else None
        step_times['ì‹œê°í™” ìƒì„±'] = time.time() - step_start
        print(f"â±ï¸ ì‹œê°í™” ìƒì„± ì†Œìš” ì‹œê°„: {step_times['ì‹œê°í™” ìƒì„±']:.2f}ì´ˆ")
        
        print("ğŸ”Ÿ ìµœì¢… ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥ ì¤‘...")
        step_start = time.time()
        estimation.print_summary_report(estimate, validation_results, sensitivity_df)
        step_times['ìš”ì•½ ë³´ê³ ì„œ'] = time.time() - step_start
        print(f"â±ï¸ ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥ ì†Œìš” ì‹œê°„: {step_times['ìš”ì•½ ë³´ê³ ì„œ']:.2f}ì´ˆ")
        
        # ì „ì²´ ì†Œìš” ì‹œê°„ ê³„ì‚°
        total_time = time.time() - total_start_time
        step_times['ì „ì²´'] = total_time
        
        # ì‹œê°„ ìš”ì•½ ì¶œë ¥
        print("\n" + "="*60)
        print("â±ï¸ ë‹¨ê³„ë³„ ì†Œìš” ì‹œê°„ ìš”ì•½")
        print("="*60)
        for step_name, elapsed_time in step_times.items():
            percentage = (elapsed_time / total_time * 100) if step_name != 'ì „ì²´' else 100
            print(f"  {step_name:20s}: {elapsed_time:7.2f}ì´ˆ ({percentage:5.1f}%)")
        print("="*60)
        
        if logger:
            logger.info("ë¶„ì„ ì™„ë£Œ")
            logger.info("="*60)
            logger.info("ë‹¨ê³„ë³„ ì†Œìš” ì‹œê°„ ìš”ì•½")
            logger.info("="*60)
            for step_name, elapsed_time in step_times.items():
                percentage = (elapsed_time / total_time * 100) if step_name != 'ì „ì²´' else 100
                logger.info(f"  {step_name:20s}: {elapsed_time:7.2f}ì´ˆ ({percentage:5.1f}%)")
            logger.info("="*60)
        
        print(f"\nâœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ! (ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ)")
        
    except Exception as e:
        if logger:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
