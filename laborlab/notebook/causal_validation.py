"""
DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ ì¸ê³¼ëª¨ë¸ êµ¬ì¶•, ì¶”ì •, ê²€ì¦ End-to-End íŒŒì´í”„ë¼ì¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” dummy_data.csvì™€ dummy_graphë¥¼ ì‚¬ìš©í•˜ì—¬
í•™ë ¥ì½”ë“œ(ACCR_CD)ê°€ 180ì¼ì´ë‚´ì·¨ì—…ì—¬ë¶€(ACQ_180_YN)ì— ë¯¸ì¹˜ëŠ” ì¸ê³¼íš¨ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""

# =============================================================================
# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# =============================================================================

import argparse
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ì´ ì‘ë™í•˜ë„ë¡ ì„¤ì •
import matplotlib.pyplot as plt
import warnings
from pathlib import Path
import logging
from datetime import datetime
import os

import dowhy
from dowhy import CausalModel
import networkx as nx

# TabPFN ëª¨ë¸ ì‚¬ìš© (í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ í›„ ì„í¬íŠ¸)
import sys
import os
# ë¡œì»¬ DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    import tabpfn
    import torch
    from dowhy.causal_estimators.tabpfn_estimator import TabpfnEstimator
    TABPFN_AVAILABLE = True
    print("âœ“ TabPFN Estimator ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    TABPFN_AVAILABLE = False
    print(f"âš ï¸ TabPFN Estimator ì‚¬ìš© ë¶ˆê°€: {e}")
    print("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜: pip install tabpfn torch")

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# DoWhy íŠ¹ì • ë¡œê±°ë“¤ì˜ INFO ë©”ì‹œì§€ë§Œ ì œê±°
import logging as dowhy_logging
# DoWhyì˜ ë°˜ë³µì ì¸ ë©”ì‹œì§€ë“¤ë§Œ ì œê±°
dowhy_logging.getLogger("dowhy.causal_estimator").setLevel(dowhy_logging.WARNING)
dowhy_logging.getLogger("dowhy.causal_estimators").setLevel(dowhy_logging.WARNING)

# í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œê°í™”ìš©)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# =============================================================================
# CONFIG ì„¤ì • ì„¹ì…˜
# =============================================================================

# ë°ì´í„° ë° ê·¸ë˜í”„ ì„¤ì •
script_dir = Path(__file__).parent
DATA_CONFIG = {
    'data_file': str(script_dir / 'data' / 'dummy_data.csv'),           # ì‚¬ìš©í•  ë°ì´í„° íŒŒì¼
    'graph_file': str(script_dir / 'data' / 'dummy_graph'),             # ì‚¬ìš©í•  ê·¸ë˜í”„ íŒŒì¼
    'treatment': 'ACCR_CD',                  # ì²˜ì¹˜ ë³€ìˆ˜ (í•™ë ¥ì½”ë“œ)
    'outcome': 'ACQ_180_YN',                 # ê²°ê³¼ ë³€ìˆ˜ (180ì¼ì´ë‚´ì·¨ì—…ì—¬ë¶€)
}

# ì¶”ì • ë°©ë²• ì„¤ì • (TabPFN ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ì— ë”°ë¼ ê²°ì •)
if 'TABPFN_AVAILABLE' in globals() and TABPFN_AVAILABLE:
    ESTIMATION_CONFIG = {
        'method': 'backdoor.tabpfn',  # ì¶”ì • ë°©ë²•: TabPFN
        'test_significance': True,               # í†µê³„ì  ìœ ì˜ì„± ê²€ì • ìˆ˜í–‰
        'proceed_when_unidentifiable': True,     # ì‹ë³„ ë¶ˆê°€ëŠ¥í•  ë•Œë„ ì§„í–‰
    }
else:
    ESTIMATION_CONFIG = {
        'method': 'backdoor.linear_regression',  # ì¶”ì • ë°©ë²•: linear regression (fallback)
        'test_significance': True,               # í†µê³„ì  ìœ ì˜ì„± ê²€ì • ìˆ˜í–‰
        'proceed_when_unidentifiable': True,     # ì‹ë³„ ë¶ˆê°€ëŠ¥í•  ë•Œë„ ì§„í–‰
    }

# ê²€ì¦ ì„¤ì • -> ì¶”í›„ ìˆ˜ì •í•„ìš”(í˜„ì¬ ê¸°ë³¸ê°’ ì‚¬ìš©)
VALIDATION_CONFIG = {
    'placebo_treatment': {
        'method': 'placebo_treatment_refuter',
        'placebo_type': 'permute',
        'num_simulations': 100
    },
    'unobserved_confounder': {
        'method': 'add_unobserved_common_cause',
        'confounders_effect_on_treatment': 'binary_flip',
        'confounders_effect_on_outcome': 'linear',
        'effect_strength_on_treatment': 0.10,
        'effect_strength_on_outcome': 0.10,
        'num_simulations': 100
    },
    'data_subset': {
        'method': 'data_subset_refuter',
        'subset_fraction': 0.8,
        'num_simulations': 200,
        'random_state': 42
    },
    'dummy_outcome': {
        'method': 'dummy_outcome',
        'num_simulations': 200
    },
    'sensitivity_analysis': {
        'effect_strength_range': (0.0, 0.5),
        'num_points': 11,
        'num_simulations': 200
    }
}

# ì‹œê°í™” ì„¤ì •
VISUALIZATION_CONFIG = {
    'figsize': (10, 8),
    'dpi': 100,
    'save_plots': True,
    'plot_format': 'png'
}

# ë¡œê¹… ì„¤ì •
LOGGING_CONFIG = {
    'save_logs': True,
    'log_format': '%(asctime)s - %(levelname)s - %(message)s',
    'log_level': 20  # logging.INFO = 20
}

# =============================================================================
# ë¡œê¹… ì„¤ì • í•¨ìˆ˜ë“¤
# =============================================================================

def setup_logging(graph_file, treatment, config):
    """
    ë¡œê¹…ì„ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ëª…
        treatment (str): ì²˜ì¹˜ ë³€ìˆ˜ëª…
        config (dict): ë¡œê¹… ì„¤ì •
    
    Returns:
        str: ìƒì„±ëœ ë¡œê·¸ íŒŒì¼ëª…
    """
    if not config['save_logs']:
        return None
    
    # log í´ë” ìƒì„± (ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê¸°ì¤€)
    script_dir = Path(__file__).parent
    log_dir = script_dir / "log"
    log_dir.mkdir(exist_ok=True)
    
    # ë¡œê·¸ íŒŒì¼ëª… ìƒì„±: ê·¸ë˜í”„ëª…_ì²˜ì¹˜ë³€ìˆ˜_ë‚ ì§œì‹œê°„.log
    graph_name = Path(graph_file).stem  # íŒŒì¼ í™•ì¥ì ì œê±°
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"{graph_name}_{treatment}_{timestamp}.log"
    log_filepath = log_dir / log_filename
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=config['log_level'],
        format=config['log_format'],
        handlers=[
            logging.FileHandler(log_filepath, encoding='utf-8'),
            logging.StreamHandler()  # ì½˜ì†”ì—ë„ ì¶œë ¥
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¡œê¹… ì‹œì‘ - ë¡œê·¸ íŒŒì¼: {log_filepath}")
    logger.info(f"ê·¸ë˜í”„ íŒŒì¼: {graph_file}")
    logger.info(f"ì²˜ì¹˜ ë³€ìˆ˜: {treatment}")
    logger.info(f"ë¶„ì„ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return str(log_filepath)

def log_estimation_results(logger, estimate, method_name):
    """
    ì¶”ì • ê²°ê³¼ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        logger: ë¡œê±° ê°ì²´
        estimate: ì¶”ì •ëœ ì¸ê³¼íš¨ê³¼ ê°ì²´
        method_name (str): ì¶”ì • ë°©ë²•ëª…
    """
    logger.info("="*60)
    logger.info("ì¸ê³¼ íš¨ê³¼ ì¶”ì • ê²°ê³¼")
    logger.info("="*60)
    logger.info(f"ì¶”ì • ë°©ë²•: {method_name}")
    logger.info(f"ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ (ATE): {estimate.value:.6f}")
    
    if hasattr(estimate, 'p_value') and estimate.p_value is not None:
        logger.info(f"P-value: {estimate.p_value:.6f}")
        significance = "ìœ ì˜í•¨" if estimate.p_value <= 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
        logger.info(f"í†µê³„ì  ìœ ì˜ì„±: {significance}")
    
    # ì¶”ì •ì¹˜ì˜ ì‹ ë¢°êµ¬ê°„ì´ ìˆë‹¤ë©´ ë¡œê¹…
    if hasattr(estimate, 'confidence_intervals'):
        logger.info(f"ì‹ ë¢°êµ¬ê°„: {estimate.confidence_intervals}")

def log_validation_results(logger, validation_results):
    """
    ê²€ì¦ ê²°ê³¼ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        logger: ë¡œê±° ê°ì²´
        validation_results (dict): ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("="*60)
    logger.info("ê²€ì¦ ê²°ê³¼ ìš”ì•½")
    logger.info("="*60)
    
    # ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸
    if validation_results.get('placebo'):
        placebo = validation_results['placebo']
        effect_change = abs(placebo.new_effect - placebo.estimated_effect)
        status = "í†µê³¼" if effect_change < 0.01 else "ì‹¤íŒ¨"
        logger.info(f"ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸: {status}")
        logger.info(f"  - ê¸°ì¡´ ì¶”ì •ì¹˜: {placebo.estimated_effect:.6f}")
        logger.info(f"  - ê°€ìƒì²˜ì¹˜ í›„ ì¶”ì •ì¹˜: {placebo.new_effect:.6f}")
        logger.info(f"  - íš¨ê³¼ ë³€í™”: {effect_change:.6f}")
    
    # ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸
    if validation_results.get('unobserved'):
        unobserved = validation_results['unobserved']
        change_rate = abs(unobserved.new_effect - unobserved.estimated_effect) / abs(unobserved.estimated_effect)
        status = "ê°•ê±´í•¨" if change_rate < 0.2 else "ë¯¼ê°í•¨"
        logger.info(f"ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸: {status}")
        logger.info(f"  - ê¸°ì¡´ ì¶”ì •ì¹˜: {unobserved.estimated_effect:.6f}")
        logger.info(f"  - êµë€ ì¶”ê°€ í›„ ì¶”ì •ì¹˜: {unobserved.new_effect:.6f}")
        logger.info(f"  - ë³€í™”ìœ¨: {change_rate:.2%}")
    
    # ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
    if validation_results.get('subset'):
        subset = validation_results['subset']
        logger.info(f"ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸:")
        logger.info(f"  - ê¸°ì¡´ ì¶”ì •ì¹˜: {subset.estimated_effect:.6f}")
        logger.info(f"  - ë¶€ë¶„í‘œë³¸ ì¶”ì •ì¹˜: {subset.new_effect:.6f}")
    
    # ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸
    if validation_results.get('dummy'):
        dummy = validation_results['dummy']
        status = "í†µê³¼" if abs(dummy.new_effect) < 0.01 else "ì‹¤íŒ¨"
        logger.info(f"ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸: {status}")
        logger.info(f"  - ë”ë¯¸ ê²°ê³¼ ì¶”ì •ì¹˜: {dummy.new_effect:.6f}")

def log_sensitivity_analysis(logger, sensitivity_df, config):
    """
    ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        logger: ë¡œê±° ê°ì²´
        sensitivity_df (pd.DataFrame): ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼
        config (dict): ë¯¼ê°ë„ ë¶„ì„ ì„¤ì •
    """
    logger.info("="*60)
    logger.info("ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼")
    logger.info("="*60)
    
    logger.info(f"íš¨ê³¼ ê°•ë„ ë²”ìœ„: {config['effect_strength_range'][0]} ~ {config['effect_strength_range'][1]}")
    logger.info(f"ê·¸ë¦¬ë“œ í¬ì¸íŠ¸ ìˆ˜: {config['num_points']}")
    logger.info(f"ì‹œë®¬ë ˆì´ì…˜ ìˆ˜: {config['num_simulations']}")
    logger.info(f"ë¶„ì„ëœ ì¡°í•© ìˆ˜: {len(sensitivity_df)}")
    
    if not sensitivity_df.empty:
        logger.info(f"íš¨ê³¼ ë²”ìœ„: {sensitivity_df['new_effect'].min():.6f} ~ {sensitivity_df['new_effect'].max():.6f}")
        
        # íš¨ê³¼ê°€ 0ì— ê°€ê¹Œìš´ ì§€ì  ì°¾ê¸°
        min_abs_effect = sensitivity_df.loc[sensitivity_df['new_effect'].abs().idxmin()]
        logger.info(f"ìµœì†Œ ì ˆëŒ€ íš¨ê³¼ ì§€ì :")
        logger.info(f"  - ì²˜ì¹˜ ê°•ë„ (et): {min_abs_effect['effect_strength_on_treatment']:.2f}")
        logger.info(f"  - ê²°ê³¼ ê°•ë„ (eo): {min_abs_effect['effect_strength_on_outcome']:.2f}")
        logger.info(f"  - íš¨ê³¼ê°’: {min_abs_effect['new_effect']:.6f}")
        
        # íš¨ê³¼ê°€ ìŒìˆ˜ì¸ ì¡°í•© ìˆ˜
        negative_effects = len(sensitivity_df[sensitivity_df['new_effect'] < 0])
        logger.info(f"ìŒìˆ˜ íš¨ê³¼ ì¡°í•© ìˆ˜: {negative_effects} ({negative_effects/len(sensitivity_df)*100:.1f}%)")
        
        # íš¨ê³¼ê°€ 0ì— ê°€ê¹Œìš´ ì¡°í•© ìˆ˜ (ì ˆëŒ€ê°’ < 0.01)
        near_zero_effects = len(sensitivity_df[sensitivity_df['new_effect'].abs() < 0.01])
        logger.info(f"0ì— ê°€ê¹Œìš´ íš¨ê³¼ ì¡°í•© ìˆ˜: {near_zero_effects} ({near_zero_effects/len(sensitivity_df)*100:.1f}%)")

def log_heatmap_info(logger, heatmap_path, config):
    """
    íˆíŠ¸ë§µ ì •ë³´ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        logger: ë¡œê±° ê°ì²´
        heatmap_path (str): íˆíŠ¸ë§µ íŒŒì¼ ê²½ë¡œ
        config (dict): ì‹œê°í™” ì„¤ì •
    """
    logger.info("="*60)
    logger.info("ì‹œê°í™” ê²°ê³¼")
    logger.info("="*60)
    
    if heatmap_path and os.path.exists(heatmap_path):
        file_size = os.path.getsize(heatmap_path)
        logger.info(f"íˆíŠ¸ë§µ íŒŒì¼: {heatmap_path}")
        logger.info(f"íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
        logger.info(f"ì´ë¯¸ì§€ í•´ìƒë„: {config['figsize'][0]}x{config['figsize'][1]} inches")
        logger.info(f"DPI: {config['dpi']}")
    else:
        logger.warning("íˆíŠ¸ë§µ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def create_causal_graph():
    """
    NetworkXë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ê³¼ ê·¸ë˜í”„ë¥¼ ì§ì ‘ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Returns:
        nx.DiGraph: ì¸ê³¼ ê·¸ë˜í”„ ê°ì²´
    """
    # ë°©í–¥ì„± ê·¸ë˜í”„ ìƒì„±
    G = nx.DiGraph()
    
    # ë…¸ë“œ ì¶”ê°€ (ì‹¤ì œ ë°ì´í„°ì— ìˆëŠ” ë³€ìˆ˜ë“¤ë§Œ ì‚¬ìš©)
    G.add_node("ACCR_CD", label="í•™ë ¥ì½”ë“œ")
    G.add_node("ACQ_180_YN", label="180ì¼ì´ë‚´ì·¨ì—…ì—¬ë¶€")
    G.add_node("HOPE_WAGE_SM_AMT", label="í¬ë§ì„ê¸ˆí•©ê³„ê¸ˆì•¡")
    G.add_node("AVG_HOPE_WAGE_SM_AMT", label="í‰ê· í¬ë§ì„ê¸ˆí•©ê³„ê¸ˆì•¡")
    
    # ì—£ì§€ ì¶”ê°€ (ì¸ê³¼ê´€ê³„) - DAG êµ¬ì¡°ë¡œ ìˆ˜ì •
    G.add_edge("ACCR_CD", "HOPE_WAGE_SM_AMT")
    G.add_edge("ACCR_CD", "AVG_HOPE_WAGE_SM_AMT")
    G.add_edge("HOPE_WAGE_SM_AMT", "ACQ_180_YN")
    G.add_edge("AVG_HOPE_WAGE_SM_AMT", "ACQ_180_YN")
    
    return G

def preprocess_data(df):
    """
    ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
    
    Args:
        df (pd.DataFrame): ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    df_processed = df.copy()
    
    # ACCR_CDë¥¼ ìˆ«ìë¡œ ì¸ì½”ë”©
    accr_mapping = {
        'ê³ ë“±í•™êµ': 1,
        '(2/3ë…„ì œ) ëŒ€í•™': 2,
        '4ë…„ì œ ëŒ€í•™': 3,
        'ëŒ€í•™ì›': 4,
        'ì¤‘í‡´': 5
    }
    
    df_processed['ACCR_CD'] = df_processed['ACCR_CD'].map(accr_mapping)
    df_processed = df_processed.dropna(subset=['ACCR_CD', 'ACQ_180_YN'])
    
    return df_processed

def load_data_and_graph(data_file, graph_file):
    """
    ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        data_file (str): ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    
    Returns:
        tuple: (ë°ì´í„°í”„ë ˆì„, ê·¸ë˜í”„ ê°ì²´)
    """
    df = pd.read_csv(data_file)
    df_processed = preprocess_data(df)
    causal_graph = create_causal_graph()
    
    return df_processed, causal_graph

def create_causal_model(df, causal_graph, treatment, outcome):
    """
    ì¸ê³¼ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        df (pd.DataFrame): ë°ì´í„°í”„ë ˆì„
        causal_graph (nx.DiGraph): NetworkX ê·¸ë˜í”„ ê°ì²´
        treatment (str): ì²˜ì¹˜ ë³€ìˆ˜ëª…
        outcome (str): ê²°ê³¼ ë³€ìˆ˜ëª…
    
    Returns:
        CausalModel: DoWhy ì¸ê³¼ëª¨ë¸ ê°ì²´
    """
    return CausalModel(
        data=df,
        treatment=treatment,
        outcome=outcome,
        graph=causal_graph
    )

def identify_effect(model, proceed_when_unidentifiable=True):
    """
    ì¸ê³¼íš¨ê³¼ë¥¼ ì‹ë³„í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        model (CausalModel): ì¸ê³¼ëª¨ë¸ ê°ì²´
        proceed_when_unidentifiable (bool): ì‹ë³„ ë¶ˆê°€ëŠ¥í•  ë•Œë„ ì§„í–‰í• ì§€ ì—¬ë¶€
    
    Returns:
        IdentifiedEstimand: ì‹ë³„ëœ ì¶”ì •ëŸ‰ ê°ì²´
    """
    return model.identify_effect(proceed_when_unidentifiable=proceed_when_unidentifiable)

def estimate_effect(model, identified_estimand, method_name, test_significance=True, logger=None):
    """
    ì¸ê³¼íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        model (CausalModel): ì¸ê³¼ëª¨ë¸ ê°ì²´
        identified_estimand: ì‹ë³„ëœ ì¶”ì •ëŸ‰ ê°ì²´
        method_name (str): ì¶”ì • ë°©ë²•ëª…
        test_significance (bool): í†µê³„ì  ìœ ì˜ì„± ê²€ì • ìˆ˜í–‰ ì—¬ë¶€
        logger: ë¡œê±° ê°ì²´ (ì„ íƒì‚¬í•­)
    
    Returns:
        CausalEstimate: ì¶”ì •ëœ ì¸ê³¼íš¨ê³¼ ê°ì²´
    """
    estimate = model.estimate_effect(
        identified_estimand,
        method_name=method_name,
        test_significance=test_significance
    )
    
    if logger:
        log_estimation_results(logger, estimate, method_name)
    
    return estimate

def run_validation_tests(model, identified_estimand, estimate, config, logger=None):
    """
    ë‹¤ì–‘í•œ ê²€ì¦ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        model (CausalModel): ì¸ê³¼ëª¨ë¸ ê°ì²´
        identified_estimand: ì‹ë³„ëœ ì¶”ì •ëŸ‰ ê°ì²´
        estimate: ì¶”ì •ëœ ì¸ê³¼íš¨ê³¼ ê°ì²´
        config (dict): ê²€ì¦ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        logger: ë¡œê±° ê°ì²´ (ì„ íƒì‚¬í•­)
    
    Returns:
        dict: ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    validation_results = {}
    
    # 1. ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸ (Placebo Treatment)
    try:
        placebo_config = config['placebo_treatment']
        refute_placebo = model.refute_estimate(
            identified_estimand,
            estimate,
            method_name=placebo_config['method'],
            placebo_type=placebo_config['placebo_type'],
            num_simulations=placebo_config['num_simulations']
        )
        validation_results['placebo'] = refute_placebo
    except Exception as e:
        validation_results['placebo'] = None
    
    # 2. ë¯¸ê´€ì¸¡ ê³µí†µ ì›ì¸ ì¶”ê°€ í…ŒìŠ¤íŠ¸
    try:
        unobserved_config = config['unobserved_confounder']
        refute_unobserved = model.refute_estimate(
            identified_estimand,
            estimate,
            method_name=unobserved_config['method'],
            confounders_effect_on_treatment=unobserved_config['confounders_effect_on_treatment'],
            confounders_effect_on_outcome=unobserved_config['confounders_effect_on_outcome'],
            effect_strength_on_treatment=unobserved_config['effect_strength_on_treatment'],
            effect_strength_on_outcome=unobserved_config['effect_strength_on_outcome'],
            num_simulations=unobserved_config['num_simulations']
        )
        validation_results['unobserved'] = refute_unobserved
    except Exception as e:
        validation_results['unobserved'] = None
    
    # 3. ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
    try:
        subset_config = config['data_subset']
        refute_subset = model.refute_estimate(
            identified_estimand,
            estimate,
            method_name=subset_config['method'],
            subset_fraction=subset_config['subset_fraction'],
            num_simulations=subset_config['num_simulations'],
            random_state=subset_config['random_state']
        )
        validation_results['subset'] = refute_subset
    except Exception as e:
        validation_results['subset'] = None
    
    # 4. ë”ë¯¸ ê²°ê³¼ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸
    try:
        dummy_config = config['dummy_outcome']
        refute_dummy = model.refute_estimate(
            identified_estimand,
            estimate,
            method_name=dummy_config['method'],
            num_simulations=dummy_config['num_simulations']
        )
        validation_results['dummy'] = refute_dummy
    except Exception as e:
        validation_results['dummy'] = None
    
    # ë¡œê¹…
    if logger:
        log_validation_results(logger, validation_results)
    
    return validation_results

def run_sensitivity_analysis(model, identified_estimand, estimate, config, logger=None):
    """
    ë¯¼ê°ë„ ë¶„ì„ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        model (CausalModel): ì¸ê³¼ëª¨ë¸ ê°ì²´
        identified_estimand: ì‹ë³„ëœ ì¶”ì •ëŸ‰ ê°ì²´
        estimate: ì¶”ì •ëœ ì¸ê³¼íš¨ê³¼ ê°ì²´
        config (dict): ë¯¼ê°ë„ ë¶„ì„ ì„¤ì •
        logger: ë¡œê±° ê°ì²´ (ì„ íƒì‚¬í•­)
    
    Returns:
        pd.DataFrame: ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
    """
    try:
        effect_range = config['effect_strength_range']
        num_points = config['num_points']
        num_simulations = config['num_simulations']
        
        grid = np.linspace(effect_range[0], effect_range[1], num_points)
        
        rows = []
        for i, et in enumerate(grid):
            for j, eo in enumerate(grid):
                try:
                    ref = model.refute_estimate(
                        identified_estimand, estimate,
                        method_name="add_unobserved_common_cause",
                        confounders_effect_on_treatment="binary_flip",
                        confounders_effect_on_outcome="linear",
                        effect_strength_on_treatment=et,
                        effect_strength_on_outcome=eo,
                        num_simulations=num_simulations
                    )
                    rows.append((et, eo, ref.new_effect))
                except Exception as e:
                    rows.append((et, eo, np.nan))
        
        sensitivity_df = pd.DataFrame(rows, columns=[
            "effect_strength_on_treatment", 
            "effect_strength_on_outcome", 
            "new_effect"
        ])
        
        if logger:
            log_sensitivity_analysis(logger, sensitivity_df, config)
        
        return sensitivity_df
        
    except Exception as e:
        if logger:
            logger.error(f"ë¯¼ê°ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

def create_sensitivity_heatmap(sensitivity_df, config, logger=None):
    """
    ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        sensitivity_df (pd.DataFrame): ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼
        config (dict): ì‹œê°í™” ì„¤ì •
        logger: ë¡œê±° ê°ì²´ (ì„ íƒì‚¬í•­)
    
    Returns:
        tuple: (matplotlib.figure.Figure, str) ìƒì„±ëœ ê·¸ë¦¼ ê°ì²´ì™€ íŒŒì¼ ê²½ë¡œ
    """
    if sensitivity_df.empty:
        return None
    
    try:
        
        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        pivot = sensitivity_df.pivot(
            index="effect_strength_on_treatment",
            columns="effect_strength_on_outcome",
            values="new_effect"
        ).sort_index(ascending=True)
        
        # íˆíŠ¸ë§µ ìƒì„±
        fig, ax = plt.subplots(figsize=config['figsize'], dpi=config['dpi'])
        
        # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
        im = ax.imshow(
            pivot.values,
            origin="lower",
            aspect="auto",
            extent=[
                pivot.columns.min(), pivot.columns.max(),
                pivot.index.min(), pivot.index.max()
            ],
            cmap='RdYlBu_r'
        )
        
        # ìƒ‰ìƒë§‰ëŒ€ ì¶”ê°€
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label("New Effect (after unobserved confounding)", fontsize=12)
        
        # 0-ì»¨íˆ¬ì–´ ë¼ì¸ ì¶”ê°€
        X, Y = np.meshgrid(pivot.columns.values, pivot.index.values)
        CS = ax.contour(X, Y, pivot.values, levels=[0.0], linewidths=2, colors='black')
        ax.clabel(CS, inline=True, fmt="effect=0", fontsize=10)
        
        # ìµœì†Œ ì ˆëŒ€ê°’ ì§€ì  ë§ˆì»¤
        abs_min_idx = np.unravel_index(
            np.nanargmin(np.abs(pivot.values)), 
            pivot.values.shape
        )
        et_star = pivot.index.values[abs_min_idx[0]]
        eo_star = pivot.columns.values[abs_min_idx[1]]
        ax.plot(eo_star, et_star, marker="o", markersize=8, color='red')
        ax.annotate(
            f"Min effect at (et={et_star:.2f}, eo={eo_star:.2f})",
            (eo_star, et_star), 
            xytext=(10, 10), 
            textcoords="offset points", 
            fontsize=10,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7)
        )
        
        # ì¶• ë ˆì´ë¸” ë° ì œëª©
        ax.set_xlabel("Effect Strength on Outcome (eo)", fontsize=12)
        ax.set_ylabel("Effect Strength on Treatment (et)", fontsize=12)
        ax.set_title("Sensitivity Analysis: Effect of Unobserved Confounders", fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        # ê·¸ë¦¼ ì €ì¥ (log í´ë”ì— ì €ì¥)
        output_path = None
        if config['save_plots']:
            # log í´ë” ê²½ë¡œ ìƒì„±
            script_dir = Path(__file__).parent
            log_dir = script_dir / "log"
            log_dir.mkdir(exist_ok=True)
            
            # íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"sensitivity_heatmap_{timestamp}.{config['plot_format']}"
            output_path = log_dir / filename
            
            plt.savefig(output_path, dpi=config['dpi'], bbox_inches='tight')
        
        # plt.show()  # GUI í™˜ê²½ì´ ì•„ë‹Œ ê²½ìš° ì£¼ì„ ì²˜ë¦¬
        
        # ë¡œê¹…
        if logger:
            log_heatmap_info(logger, output_path, config)
        
        return fig, output_path
        
    except Exception as e:
        if logger:
            logger.error(f"íˆíŠ¸ë§µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

def print_summary_report(estimate, validation_results, sensitivity_df):
    """
    ì „ì²´ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        estimate: ì¶”ì •ëœ ì¸ê³¼íš¨ê³¼ ê°ì²´
        validation_results (dict): ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        sensitivity_df (pd.DataFrame): ë¯¼ê°ë„ ë¶„ì„ ê²°ê³¼
    """
    print("\n" + "="*80)
    print("ğŸ“‹ ìµœì¢… ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ")
    print("="*80)
    
    # ê¸°ë³¸ ì¶”ì • ê²°ê³¼
    print(f"\nğŸ¯ ì£¼ìš” ì¶”ì • ê²°ê³¼:")
    print(f"  - ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ (ATE): {estimate.value:.6f}")
    if hasattr(estimate, 'p_value') and estimate.p_value is not None:
        significance = "ìœ ì˜í•¨" if estimate.p_value <= 0.05 else "ìœ ì˜í•˜ì§€ ì•ŠìŒ"
        print(f"  - í†µê³„ì  ìœ ì˜ì„±: {significance} (p-value: {estimate.p_value:.6f})")
    
    # ê²€ì¦ ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ”¬ ê²€ì¦ ê²°ê³¼ ìš”ì•½:")
    
    if validation_results.get('placebo'):
        placebo = validation_results['placebo']
        effect_change = abs(placebo.new_effect - placebo.estimated_effect)
        print(f"  - ê°€ìƒ ì›ì¸ í…ŒìŠ¤íŠ¸: {'í†µê³¼' if effect_change < 0.01 else 'ì‹¤íŒ¨'}")
    
    if validation_results.get('unobserved'):
        unobserved = validation_results['unobserved']
        change_rate = abs(unobserved.new_effect - unobserved.estimated_effect) / abs(unobserved.estimated_effect)
        print(f"  - ë¯¸ê´€ì¸¡ êµë€ í…ŒìŠ¤íŠ¸: {'ê°•ê±´í•¨' if change_rate < 0.2 else 'ë¯¼ê°í•¨'}")
    
    if validation_results.get('subset'):
        subset = validation_results['subset']
        print(f"  - ë¶€ë¶„í‘œë³¸ ì•ˆì •ì„±: ì¶”ì •ì¹˜ ë³€í™” í™•ì¸ë¨")
    
    if validation_results.get('dummy'):
        dummy = validation_results['dummy']
        print(f"  - ë”ë¯¸ ê²°ê³¼ í…ŒìŠ¤íŠ¸: {'í†µê³¼' if abs(dummy.new_effect) < 0.01 else 'ì‹¤íŒ¨'}")
    
    # ë¯¼ê°ë„ ë¶„ì„ ìš”ì•½
    if not sensitivity_df.empty:
        print(f"\nğŸ“ˆ ë¯¼ê°ë„ ë¶„ì„ ìš”ì•½:")
        print(f"  - ë¶„ì„ëœ ì¡°í•© ìˆ˜: {len(sensitivity_df)}")
        print(f"  - íš¨ê³¼ ë²”ìœ„: {sensitivity_df['new_effect'].min():.6f} ~ {sensitivity_df['new_effect'].max():.6f}")
        
        # íš¨ê³¼ê°€ 0ì— ê°€ê¹Œìš´ ì§€ì  ì°¾ê¸°
        min_abs_effect = sensitivity_df.loc[sensitivity_df['new_effect'].abs().idxmin()]
        print(f"  - ìµœì†Œ ì ˆëŒ€ íš¨ê³¼ ì§€ì : et={min_abs_effect['effect_strength_on_treatment']:.2f}, eo={min_abs_effect['effect_strength_on_outcome']:.2f}")
    
    print(f"\nâœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ!")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    # ë¡œê¹… ì„¤ì •
    log_filename = setup_logging(
        DATA_CONFIG['graph_file'], 
        DATA_CONFIG['treatment'], 
        LOGGING_CONFIG
    )
    logger = logging.getLogger(__name__) if log_filename else None
    
    try:
        # 1. ë°ì´í„° ë° ê·¸ë˜í”„ ë¡œë“œ
        df, causal_graph = load_data_and_graph(
            DATA_CONFIG['data_file'], 
            DATA_CONFIG['graph_file']
        )
        
        # 2. ì¸ê³¼ëª¨ë¸ ìƒì„±
        model = create_causal_model(
            df, 
            causal_graph, 
            DATA_CONFIG['treatment'], 
            DATA_CONFIG['outcome']
        )
        
        # 3. ì¸ê³¼íš¨ê³¼ ì‹ë³„
        identified_estimand = identify_effect(
            model, 
            ESTIMATION_CONFIG['proceed_when_unidentifiable']
        )
        
        # 4. ì¸ê³¼íš¨ê³¼ ì¶”ì •
        estimate = estimate_effect(
            model,
            identified_estimand,
            ESTIMATION_CONFIG['method'],
            ESTIMATION_CONFIG['test_significance'],
            logger
        )
        
        # 5. ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        validation_results = run_validation_tests(
            model, 
            identified_estimand, 
            estimate, 
            VALIDATION_CONFIG,
            logger
        )
        
        # 6. ë¯¼ê°ë„ ë¶„ì„
        sensitivity_df = run_sensitivity_analysis(
            model, 
            identified_estimand, 
            estimate, 
            VALIDATION_CONFIG['sensitivity_analysis'],
            logger
        )
        
        # 7. ë¯¼ê°ë„ ë¶„ì„ íˆíŠ¸ë§µ ìƒì„±
        heatmap_path = None
        if not sensitivity_df.empty:
            _, heatmap_path = create_sensitivity_heatmap(sensitivity_df, VISUALIZATION_CONFIG, logger)
        
        # 8. ìµœì¢… ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥
        print_summary_report(estimate, validation_results, sensitivity_df)
        
        # 9. ë¡œê¹… ì™„ë£Œ ë©”ì‹œì§€
        if logger:
            logger.info("="*60)
            logger.info("ë¶„ì„ ì™„ë£Œ")
            logger.info("="*60)
            logger.info(f"ë¶„ì„ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            if log_filename:
                logger.info(f"ë¡œê·¸ íŒŒì¼: {log_filename}")
            if heatmap_path:
                logger.info(f"íˆíŠ¸ë§µ íŒŒì¼: {heatmap_path}")
        
    except Exception as e:
        if logger:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main()
