"""
ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì—¬ëŸ¬ treatmentì™€ graph ì¡°í•©ìœ¼ë¡œ ì¸ê³¼ì¶”ë¡  ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import argparse
import json
import subprocess
import sys
import os
from pathlib import Path
from datetime import datetime
import itertools
from typing import List, Dict, Any, Optional
import time
import logging
import pandas as pd

# graph_parser ëª¨ë“ˆ ì„í¬íŠ¸ (src/__init__.pyë¥¼ ê±°ì¹˜ì§€ ì•Šê³  ì§ì ‘ ì„í¬íŠ¸)
# __init__.pyê°€ preprocessë¥¼ ì„í¬íŠ¸í•˜ë©´ì„œ ì˜ì¡´ì„± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
# ì§ì ‘ ê²½ë¡œì—ì„œ ëª¨ë“ˆì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
import importlib.util

def load_graph_parser():
    """graph_parser ëª¨ë“ˆì„ ì§ì ‘ ë¡œë“œí•©ë‹ˆë‹¤."""
    graph_parser_path = Path(__file__).parent / "src" / "graph_parser.py"
    spec = importlib.util.spec_from_file_location("graph_parser", graph_parser_path)
    graph_parser = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(graph_parser)
    return graph_parser

graph_parser = load_graph_parser()
find_all_graph_files = graph_parser.find_all_graph_files
extract_treatments_from_graph = graph_parser.extract_treatments_from_graph
get_treatments_from_all_graphs = graph_parser.get_treatments_from_all_graphs

# main ëª¨ë“ˆ ì„í¬íŠ¸ (ì „ì²˜ë¦¬ ë° ë¶„ì„ í•¨ìˆ˜ ì‚¬ìš©)
sys.path.insert(0, str(Path(__file__).parent))
from src import main as main_module
from src import estimation as estimation_module


def load_experiment_config(config_file: str) -> Dict[str, Any]:
    """ì‹¤í—˜ ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config


def run_single_experiment(
    merged_df_clean,
    graph_file: str,
    treatment: str,
    outcome: str,
    estimator: str,
    experiment_id: str,
    logger=None
) -> Dict[str, Any]:
    """ë‹¨ì¼ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤ (ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)."""
    start_time = datetime.now()
    try:
        result = main_module.run_analysis_without_preprocessing(
            merged_df_clean=merged_df_clean,
            graph_file=graph_file,
            treatment=treatment,
            outcome=outcome,
            estimator=estimator,
            logger=logger,
            experiment_id=experiment_id
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        metrics = result.get("metrics", {})
        estimate = result.get("estimate")
        validation_results = result.get("validation_results", {})
        
        # ATE ê°’ ì¶”ì¶œ
        ate_value = None
        if estimate and hasattr(estimate, 'value'):
            ate_value = estimate.value
        
        # Refutation ê²°ê³¼ ì¶”ì¶œ
        refutation_data = {}
        refutation_types = ['placebo', 'unobserved', 'subset', 'dummy']
        for ref_type in refutation_types:
            ref_result = validation_results.get(ref_type)
            if ref_result is not None:
                # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
                if ref_type == 'placebo':
                    effect_change = abs(ref_result.new_effect - ref_result.estimated_effect)
                    refutation_data[f'{ref_type}_passed'] = effect_change < 0.01
                elif ref_type == 'unobserved':
                    change_rate = abs(ref_result.new_effect - ref_result.estimated_effect) / abs(ref_result.estimated_effect) if abs(ref_result.estimated_effect) > 0 else float('inf')
                    refutation_data[f'{ref_type}_passed'] = change_rate < 0.2
                elif ref_type == 'subset':
                    effect_change = abs(ref_result.new_effect - ref_result.estimated_effect)
                    change_rate = abs(ref_result.estimated_effect) > 0 and abs(effect_change / ref_result.estimated_effect) or float('inf')
                    refutation_data[f'{ref_type}_passed'] = change_rate < 0.1
                elif ref_type == 'dummy':
                    refutation_data[f'{ref_type}_passed'] = abs(ref_result.new_effect) < 0.01
                
                # p-value ì¶”ì¶œ (estimation ëª¨ë“ˆì˜ í•¨ìˆ˜ ì‚¬ìš©)
                p_value = estimation_module.calculate_refutation_pvalue(ref_result, ref_type)
                refutation_data[f'{ref_type}_pvalue'] = p_value
            else:
                refutation_data[f'{ref_type}_passed'] = None
                refutation_data[f'{ref_type}_pvalue'] = None
        
        return {
            "experiment_id": experiment_id,
            "status": "success",
            "duration_seconds": duration,
            "graph": graph_file,
            "graph_name": Path(graph_file).stem,
            "treatment": treatment,
            "outcome": outcome,
            "estimator": estimator,
            "ate_value": ate_value,
            "metrics": metrics,
            "accuracy": metrics.get("accuracy") if metrics else None,
            "f1_score": metrics.get("f1_score") if metrics else None,
            "auc": metrics.get("auc") if metrics else None,
            "excel_path": result.get("excel_path"),
            "train_size": result.get("train_size"),
            "test_size": result.get("test_size"),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            **refutation_data
        }
    except Exception as e:
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        return {
            "experiment_id": experiment_id,
            "status": "failed",
            "duration_seconds": duration,
            "graph": graph_file,
            "treatment": treatment,
            "outcome": outcome,
            "estimator": estimator,
            "error": str(e),
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
        }


def run_batch_experiments(config: Dict[str, Any], base_dir: Path):
    """ë°°ì¹˜ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    data_dir = config.get("data_dir", "data")
    graphs = config.get("graphs", [])
    treatments = config.get("treatments", [])
    outcomes = config.get("outcomes", ["ACQ_180_YN"])
    estimators = config.get("estimators", ["tabpfn"])
    auto_extract_treatments = config.get("auto_extract_treatments", False)
    graph_data_dir = config.get("graph_data_dir", "graph_data")
    api_key = config.get("api_key", None)  # configì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    data_dir_path = base_dir / data_dir
    if not data_dir_path.is_absolute():
        data_dir_path = base_dir / data_dir
    
    # ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
    graph_files = []
    
    # auto_extract_treatmentsê°€ Trueì´ë©´ graph_data í´ë”ì—ì„œ ìë™ìœ¼ë¡œ ì°¾ê¸°
    if auto_extract_treatments:
        print(f"ğŸ” ê·¸ë˜í”„ íŒŒì¼ì—ì„œ ìë™ìœ¼ë¡œ treatment ì¶”ì¶œ ì¤‘...")
        found_graphs = find_all_graph_files(data_dir_path, graph_data_dir)
        graph_files = [str(g) for g in found_graphs]
        
        if not graph_files:
            print(f"âš ï¸ {graph_data_dir} í´ë”ì—ì„œ ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"âœ… {len(graph_files)}ê°œì˜ ê·¸ë˜í”„ íŒŒì¼ ë°œê²¬:")
            for g in graph_files:
                print(f"   - {Path(g).name}")
    else:
        # ìˆ˜ë™ìœ¼ë¡œ ì§€ì •ëœ ê·¸ë˜í”„ íŒŒì¼ë“¤
        for graph in graphs:
            if isinstance(graph, str):
                graph_path = base_dir / data_dir / graph
                if graph_path.exists():
                    graph_files.append(str(graph_path))
                else:
                    # graph_data í´ë”ì—ì„œ ì°¾ê¸°
                    graph_path = base_dir / data_dir / graph_data_dir / graph
                    if graph_path.exists():
                        graph_files.append(str(graph_path))
                    else:
                        # ì ˆëŒ€ ê²½ë¡œë¡œ ì‹œë„
                        graph_path = Path(graph)
                        if graph_path.exists():
                            graph_files.append(str(graph_path))
                        else:
                            print(f"âš ï¸ ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {graph}")
            else:
                print(f"âš ï¸ ì˜ëª»ëœ ê·¸ë˜í”„ ê²½ë¡œ: {graph}")
    
    if not graph_files:
        print("âŒ ìœ íš¨í•œ ê·¸ë˜í”„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # treatment ìë™ ì¶”ì¶œ
    graph_treatments_map = {}
    graph_outcomes_map = {}
    
    if auto_extract_treatments:
        print(f"\nğŸ” ê° ê·¸ë˜í”„ íŒŒì¼ì—ì„œ treatment ì •ë³´ ì¶”ì¶œ ì¤‘...")
        
        for graph_file in graph_files:
            graph_path = Path(graph_file)
            extracted_treatments = extract_treatments_from_graph(graph_path)
            
            if extracted_treatments:
                graph_treatments_map[graph_file] = [t["treatment_var"] for t in extracted_treatments if t.get("treatment_var")]
                # outcome ì¶”ì¶œ (ì²« ë²ˆì§¸ treatmentì—ì„œ)
                if extracted_treatments[0].get("outcome"):
                    graph_outcomes_map[graph_file] = extracted_treatments[0]["outcome"]
                print(f"   âœ… {graph_path.name}: {len(graph_treatments_map[graph_file])}ê°œì˜ treatment ë°œê²¬")
                for t in extracted_treatments:
                    if t.get("treatment_var"):
                        print(f"      - {t['treatment_var']}: {t.get('label', '')}")
            else:
                print(f"   âš ï¸ {graph_path.name}: treatment ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # treatmentê°€ ìë™ ì¶”ì¶œëœ ê²½ìš°, ê° ê·¸ë˜í”„ë³„ë¡œ ë‹¤ë¥¸ treatment ì‚¬ìš©
        if graph_treatments_map:
            print(f"\nğŸ“‹ ìë™ ì¶”ì¶œëœ treatment ì •ë³´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # ì‹¤í—˜ ì¡°í•© ìƒì„±
    # linear_regressionì„ ë¨¼ì € ì‹¤í–‰í•˜ê³ , ê·¸ ë‹¤ìŒ tabpfnì„ ì‹¤í–‰í•˜ë„ë¡ ìˆœì„œ ë³€ê²½
    # ë¹ ë¥¸ ê²°ê³¼ í™•ì¸ì„ ìœ„í•´ ë¹ ë¥¸ ì¶”ì •ê¸°(linear_regression)ë¥¼ ë¨¼ì € ì‹¤í–‰
    
    # estimators ë¦¬ìŠ¤íŠ¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì •ë ¬: linear_regression ë¨¼ì €, ê·¸ ë‹¤ìŒ tabpfn, ë‚˜ë¨¸ì§€
    sorted_estimators = []
    if "linear_regression" in estimators:
        sorted_estimators.append("linear_regression")
    if "tabpfn" in estimators:
        sorted_estimators.append("tabpfn")
    # ë‚˜ë¨¸ì§€ estimator ì¶”ê°€ (ì›ë˜ ìˆœì„œ ìœ ì§€)
    for est in estimators:
        if est not in sorted_estimators:
            sorted_estimators.append(est)
    
    if auto_extract_treatments and graph_treatments_map:
        # ê° ê·¸ë˜í”„ë³„ë¡œ í•´ë‹¹ ê·¸ë˜í”„ì˜ treatmentë§Œ ì‚¬ìš©
        experiment_combinations = []
        for graph_file in graph_files:
            graph_treatments = graph_treatments_map.get(graph_file, treatments)
            graph_outcome = graph_outcomes_map.get(graph_file, outcomes[0] if outcomes else "ACQ_180_YN")
            
            # í•´ë‹¹ ê·¸ë˜í”„ì˜ treatmentì™€ outcome ì¡°í•© ìƒì„±
            # sorted_estimators ìˆœì„œëŒ€ë¡œ ì‹¤í–‰ (linear_regression ë¨¼ì €, ê·¸ ë‹¤ìŒ tabpfn)
            for treatment in graph_treatments:
                for estimator in sorted_estimators:
                    experiment_combinations.append((graph_file, treatment, graph_outcome, estimator))
    else:
        # ê¸°ì¡´ ë°©ì‹: ëª¨ë“  ì¡°í•© ìƒì„±í•˜ë˜, estimator ìˆœì„œë¥¼ linear_regression ë¨¼ì €ë¡œ ë³€ê²½
        experiment_combinations = list(itertools.product(
            graph_files,
            treatments,
            outcomes,
            sorted_estimators
        ))
    
    total_experiments = len(experiment_combinations)
    print(f"\nğŸ“Š ì´ {total_experiments}ê°œì˜ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    if auto_extract_treatments and graph_treatments_map:
        print(f"   - ê·¸ë˜í”„: {len(graph_files)}ê°œ (ê° ê·¸ë˜í”„ë³„ treatment ìë™ ì¶”ì¶œ)")
        total_treatments = sum(len(t) for t in graph_treatments_map.values())
        print(f"   - ì´ Treatment: {total_treatments}ê°œ")
        print(f"   - Outcome: {len(set(graph_outcomes_map.values())) if graph_outcomes_map else len(outcomes)}ê°œ")
    else:
        print(f"   - ê·¸ë˜í”„: {len(graph_files)}ê°œ")
        print(f"   - Treatment: {len(treatments)}ê°œ")
        print(f"   - Outcome: {len(outcomes)}ê°œ")
    print(f"   - Estimator: {len(estimators)}ê°œ\n")
    
    # ============================================================
    # ì „ì²˜ë¦¬ë¥¼ í•œ ë²ˆë§Œ ìˆ˜í–‰
    # ============================================================
    print("="*80)
    print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ (í•œ ë²ˆë§Œ ìˆ˜í–‰)")
    print("="*80)
    
    preprocessing_start = time.time()
    
    # 1. ë°ì´í„° íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
    print("1ï¸âƒ£ ë°ì´í„° íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘ ì¤‘...")
    file_list, _ = main_module.load_all_data(str(data_dir_path), graph_file=None)
    
    # 2. ì „ì²˜ë¦¬ ë° ë³‘í•©
    print("2ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ì¤‘...")
    print("âš¡ JSON íŒŒì¼ 4ê°œ(ì´ë ¥ì„œ, ìê¸°ì†Œê°œì„œ, ì§ì—…í›ˆë ¨, ìê²©ì¦) ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")
    if api_key:
        print(f"ğŸ”‘ API í‚¤: config íŒŒì¼ì—ì„œ ì‚¬ìš©")
    else:
        print(f"âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    merged_df = main_module.preprocess_and_merge_data(file_list, str(data_dir_path), api_key=api_key)
    print(f"âœ… ìµœì¢… ë³‘í•© ë°ì´í„°: {len(merged_df)}ê±´, {len(merged_df.columns)}ê°œ ë³€ìˆ˜")
    
    # 3. ëª¨ë“  ê·¸ë˜í”„ì˜ ë³€ìˆ˜ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë°ì´í„° ì •ë¦¬
    print("3ï¸âƒ£ ëª¨ë“  ê·¸ë˜í”„ì˜ ë³€ìˆ˜ ìˆ˜ì§‘ ë° ë°ì´í„° ì •ë¦¬ ì¤‘...")
    
    # ëª¨ë“  ê·¸ë˜í”„ íŒŒì¼ì—ì„œ ë³€ìˆ˜ ìˆ˜ì§‘
    all_graph_variables = set()
    for graph_file in graph_files:
        graph_path = Path(graph_file)
        try:
            causal_graph = main_module.create_causal_graph(str(graph_path))
            all_graph_variables.update(causal_graph.nodes())
        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({graph_path.name}): {e}")
    
    print(f"ğŸ“‹ ëª¨ë“  ê·¸ë˜í”„ì—ì„œ ìˆ˜ì§‘ëœ ë³€ìˆ˜ ìˆ˜: {len(all_graph_variables)}ê°œ")
    
    # í•„ìˆ˜ ë³€ìˆ˜ (ëª¨ë“  treatment, outcome, ë³‘í•© í‚¤)
    all_treatments = set()
    all_outcomes = set()
    for graph_file in graph_files:
        if graph_file in graph_treatments_map:
            all_treatments.update(graph_treatments_map[graph_file])
        if graph_file in graph_outcomes_map:
            all_outcomes.add(graph_outcomes_map[graph_file])
    if not auto_extract_treatments:
        all_treatments.update(treatments)
    if not graph_outcomes_map:
        all_outcomes.update(outcomes)
    
    essential_vars = all_treatments | all_outcomes | {"SEEK_CUST_NO", "JHNT_CTN", "JHNT_MBN"}
    required_vars = list(all_graph_variables | essential_vars)
    
    # ë°ì´í„° ì •ë¦¬
    merged_df_clean = main_module.clean_dataframe_for_causal_model(
        merged_df, 
        required_vars=required_vars, 
        logger=None
    )
    
    # ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ ì œê±°
    data_variables = set(merged_df_clean.columns)
    vars_to_keep = (all_graph_variables | essential_vars) & data_variables
    vars_to_remove = data_variables - vars_to_keep
    
    if vars_to_remove:
        print(f"ğŸ—‘ï¸ ê·¸ë˜í”„ì— ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ ì œê±° ì¤‘ ({len(vars_to_remove)}ê°œ)...")
        merged_df_clean = merged_df_clean[list(vars_to_keep)]
    
    preprocessing_elapsed = time.time() - preprocessing_start
    print(f"â±ï¸ ì „ì²˜ë¦¬ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {preprocessing_elapsed:.2f}ì´ˆ")
    print(f"âœ… ì •ë¦¬ëœ ë°ì´í„°: {len(merged_df_clean)}ê±´, {len(merged_df_clean.columns)}ê°œ ë³€ìˆ˜")
    print("="*80 + "\n")
    
    # ê²°ê³¼ ì €ì¥
    results = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = base_dir / "log" / f"batch_experiments_{timestamp}.json"
    results_file.parent.mkdir(exist_ok=True)
    
    # CSV ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
    csv_results_file = base_dir / "log" / f"experiment_results_{timestamp}.csv"
    
    # CSV ì»¬ëŸ¼ ì •ì˜
    csv_columns = [
        'graph_name', 'treatment', 'estimator', 'ate_value',
        'placebo_passed', 'placebo_pvalue',
        'unobserved_passed', 'unobserved_pvalue',
        'subset_passed', 'subset_pvalue',
        'dummy_passed', 'dummy_pvalue',
        'f1_score', 'auc', 'duration_seconds'
    ]
    
    # CSV íŒŒì¼ ì´ˆê¸°í™” (í—¤ë”ë§Œ ì‘ì„±)
    pd.DataFrame(columns=csv_columns).to_csv(csv_results_file, index=False, encoding='utf-8-sig')
    
    # ë¡œê±° ì„¤ì • (ì„ íƒì )
    logger = None
    if not config.get("no_logs", False):
        log_dir = base_dir / "log"
        log_dir.mkdir(exist_ok=True)
        log_filename = f"batch_experiments_{timestamp}.log"
        log_filepath = log_dir / log_filename
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filepath, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        logger = logging.getLogger(__name__)
        logger.info(f"ë°°ì¹˜ ì‹¤í—˜ ì‹œì‘ - {timestamp}")
        logger.info(f"ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}")
    
    # ì‹¤í—˜ ì‹¤í–‰
    for idx, (graph_file, treatment, outcome, estimator) in enumerate(experiment_combinations, 1):
        experiment_id = f"exp_{idx:04d}_{Path(graph_file).stem}_{treatment}_{outcome}_{estimator}"
        
        print(f"\n[{idx}/{total_experiments}] ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
        
        result = run_single_experiment(
            merged_df_clean=merged_df_clean,
            graph_file=graph_file,
            treatment=treatment,
            outcome=outcome,
            estimator=estimator,
            experiment_id=experiment_id,
            logger=logger
        )
        
        results.append(result)
        
        # ì‹¤íŒ¨í•œ ê²½ìš° ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
        if result["status"] == "failed":
            print(f"âŒ ì‹¤íŒ¨: {experiment_id}")
            if result.get("stderr"):
                # stderrì˜ ë§ˆì§€ë§‰ ëª‡ ì¤„ë§Œ ì¶œë ¥ (ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ)
                stderr_lines = result["stderr"].strip().split('\n')
                error_preview = '\n'.join(stderr_lines[-10:])  # ë§ˆì§€ë§‰ 10ì¤„ë§Œ
                print(f"   ì—ëŸ¬: {error_preview}")
            elif result.get("error"):
                print(f"   ì—ëŸ¬: {result['error']}")
        
        # CSVì— ê²°ê³¼ ì¶”ê°€ (ì„±ê³µí•œ ê²½ìš°ë§Œ)
        if result["status"] == "success":
            csv_row = {
                'graph_name': result.get('graph_name', ''),
                'treatment': result.get('treatment', ''),
                'estimator': result.get('estimator', ''),
                'ate_value': result.get('ate_value'),
                'placebo_passed': result.get('placebo_passed'),
                'placebo_pvalue': result.get('placebo_pvalue'),
                'unobserved_passed': result.get('unobserved_passed'),
                'unobserved_pvalue': result.get('unobserved_pvalue'),
                'subset_passed': result.get('subset_passed'),
                'subset_pvalue': result.get('subset_pvalue'),
                'dummy_passed': result.get('dummy_passed'),
                'dummy_pvalue': result.get('dummy_pvalue'),
                'f1_score': result.get('f1_score'),
                'auc': result.get('auc'),
                'duration_seconds': result.get('duration_seconds')
            }
            
            # ê¸°ì¡´ CSV ì½ê¸°
            try:
                existing_df = pd.read_csv(csv_results_file, encoding='utf-8-sig')
            except (FileNotFoundError, pd.errors.EmptyDataError):
                existing_df = pd.DataFrame(columns=csv_columns)
            
            # ìƒˆ í–‰ ì¶”ê°€
            new_row_df = pd.DataFrame([csv_row])
            updated_df = pd.concat([existing_df, new_row_df], ignore_index=True)
            
            # CSV íŒŒì¼ ë®ì–´ì“°ê¸°
            updated_df.to_csv(csv_results_file, index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š CSV ê²°ê³¼ ì €ì¥: {csv_results_file}")
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (JSON)
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        success_count = sum(1 for r in results if r["status"] == "success")
        failed_count = sum(1 for r in results if r["status"] == "failed")
        print(f"\nâœ… ì„±ê³µ: {success_count}, âŒ ì‹¤íŒ¨: {failed_count}")
    
    # ìµœì¢… ìš”ì•½
    print(f"\n{'='*80}")
    print("ğŸ“‹ ë°°ì¹˜ ì‹¤í—˜ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"ì´ ì‹¤í—˜ ìˆ˜: {total_experiments}")
    print(f"ì„±ê³µ: {success_count}")
    print(f"ì‹¤íŒ¨: {failed_count}")
    print(f"JSON ê²°ê³¼ íŒŒì¼: {results_file}")
    print(f"CSV ê²°ê³¼ íŒŒì¼: {csv_results_file}")
    print(f"{'='*80}\n")
    
    # ì‹¤íŒ¨í•œ ì‹¤í—˜ ëª©ë¡ ì¶œë ¥
    if failed_count > 0:
        print("\nâŒ ì‹¤íŒ¨í•œ ì‹¤í—˜ ëª©ë¡:")
        for result in results:
            if result["status"] == "failed":
                print(f"\n  - {result['experiment_id']}")
                if result.get("stderr"):
                    # stderrì˜ ë§ˆì§€ë§‰ 5ì¤„ë§Œ ì¶œë ¥
                    stderr_lines = result["stderr"].strip().split('\n')
                    error_preview = '\n'.join(stderr_lines[-5:])
                    print(f"    ì—ëŸ¬: {error_preview}")
                elif result.get("error"):
                    print(f"    ì—ëŸ¬: {result['error']}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë°°ì¹˜ ì¸ê³¼ì¶”ë¡  ì‹¤í—˜ ì‹¤í–‰")
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸° (Dockerì—ì„œ ì‚¬ìš©)
    default_config = os.environ.get(
        "EXPERIMENT_CONFIG",
        "experiment_config.json"
    )
    
    parser.add_argument(
        "--config",
        type=str,
        default=default_config,
        nargs='?',  # ì„ íƒì  ì¸ìë¡œ ë§Œë“¤ê¸°
        help="ì‹¤í—˜ ì„¤ì • JSON íŒŒì¼ ê²½ë¡œ"
    )
    
    args = parser.parse_args()
    
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
    script_dir = Path(__file__).parent
    
    # config ì¸ìê°€ Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
    config_arg = args.config if args.config is not None else default_config
    
    # ì„¤ì • íŒŒì¼ ê²½ë¡œ ê²°ì • (ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ)
    if os.path.isabs(config_arg):
        config_path = Path(config_arg)
    else:
        config_path = script_dir / config_arg
    
    if not config_path.exists():
        # ìƒëŒ€ ê²½ë¡œë¡œ ì‹œë„
        config_path = Path(config_arg)
        if not config_path.exists():
            print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_arg}")
            print(f"   í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
            print(f"   ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬: {script_dir}")
            print(f"   ì‹œë„í•œ ê²½ë¡œ: {config_path}")
            print(f"\nì„¤ì • íŒŒì¼ ì˜ˆì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            create_example_config(script_dir / "experiment_config.json")
            return
    
    # ì„¤ì • ë¡œë“œ
    print(f"ğŸ“„ ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
    config = load_experiment_config(str(config_path))
    
    # ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰
    run_batch_experiments(config, script_dir)


def create_example_config(config_file: Path):
    """ì˜ˆì‹œ ì„¤ì • íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    example_config = {
        "data_dir": "data",
        "graph_data_dir": "graph_data",
        "api_key": None,
        "auto_extract_treatments": True,
        "graphs": [
            "main_graph",
            "dummy_graph"
        ],
        "treatments": [
            "ACCR_CD",
            "CARR_MYCT1",
            "NTR_BPLC_PSNT_WAGE_AMT"
        ],
        "outcomes": [
            "ACQ_180_YN"
        ],
        "estimators": [
            "tabpfn",
            "linear_regression"
        ],
        "no_logs": False,
        "verbose": False,
        "comment": "auto_extract_treatmentsê°€ trueì´ë©´ graphsì™€ treatmentsëŠ” ë¬´ì‹œë˜ê³ , ê° graph íŒŒì¼ì—ì„œ ìë™ìœ¼ë¡œ ì¶”ì¶œë©ë‹ˆë‹¤. api_keyëŠ” GPT API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš” (ì˜ˆ: \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"). nullì´ë©´ í™˜ê²½ë³€ìˆ˜ LLM_API_KEYë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
    }
    
    config_file.parent.mkdir(exist_ok=True)
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(example_config, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ì˜ˆì‹œ ì„¤ì • íŒŒì¼ ìƒì„±: {config_file}")
    print("\nì„¤ì • íŒŒì¼ì„ ìˆ˜ì •í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()

