version: '3.8'

services:
  causal-analysis:
    build:
      context: ..  # 프로젝트 루트를 빌드 컨텍스트로
      dockerfile: laborlab/Dockerfile
    container_name: dowhy-causal-analysis
    volumes:
      # 데이터 디렉토리 마운트
      - ./data:/app/laborlab/data:ro
      # 로그 디렉토리 마운트 (읽기/쓰기)
      - ./log:/app/laborlab/log
      # 설정 파일 마운트
      - ./experiment_config.json:/app/laborlab/experiment_config.json:ro
      # 소스 코드 마운트 (코드 수정 시 재빌드 불필요)
      - ./src:/app/laborlab/src
      - ./run_batch_experiments.py:/app/laborlab/run_batch_experiments.py
      # Python 캐시 제외를 위한 볼륨
      - python-cache:/root/.cache/pip
    environment:
      - PYTHONPATH=/app
      - TERMINAL_OUTPUT_DIR=/app/laborlab/log
      # 실험 설정 파일 경로 (환경변수로 오버라이드 가능)
      - EXPERIMENT_CONFIG=/app/laborlab/experiment_config.json
      - OLLAMA_HOST=http://ollama:11434
      # LLM API 키 (선택적, LLM 기능을 사용하려면 설정)
      # - LLM_API_KEY=your-api-key-here
    depends_on: [ollama]
    working_dir: /app/laborlab
    # 컨테이너를 계속 실행 상태로 유지 (코드 수정 후 재빌드 없이 실행 가능)
    # 직접 실행 방법:
    #   docker-compose exec causal-analysis python run_batch_experiments.py
    #   docker-compose exec causal-analysis python -m src.main --data-dir data --treatment ACCR_CD --outcome ACQ_180_YN --estimator tabpfn
    command: sleep infinity
    stdin_open: true
    tty: true
    # GPU 지원 (NVIDIA Docker 필요, 선택사항)
    # 주석 해제하여 GPU 사용 가능
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
  ollama:
    container_name: ollama
    image: ollama/ollama
    volumes:
      - ./ollama_models:/root/.ollama/models   
    ports:
      - "11434:11434"   # 내부만 쓰면 생략 가능
volumes:
  python-cache:

