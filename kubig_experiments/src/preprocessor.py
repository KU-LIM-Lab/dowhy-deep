# coding: utf-8
import json, math
from pathlib import Path
from typing import List, Dict, Any, Callable
import pandas as pd
import numpy as np
import logging

logger = logging.getLogger(__name__)

ROOT = Path(__file__).resolve().parent.parent / "data"

RAW_CSV = ROOT / "synthetic_data_raw.csv"

RESUME_DIR   = ROOT / "RESUME_JSON/ver1"
COVER_DIR    = ROOT / "COVERLETTERS_JSON/ver1"
TRAINING_DIR = ROOT / "TRAININGS_JSON"
LICENSE_DIR  = ROOT / "LICENSES_JSON"


def _read_json_safe(p: Path) -> Dict[str, Any]:
    with p.open("r", encoding="utf-8") as f:
        return json.load(f)

def _collect_json(d: Path) -> List[Path]:
    """ì£¼ì–´ì§„ ë””ë ‰í† ë¦¬ì™€ ê·¸ í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  JSON íŒŒì¼ì„ ìž¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘ (rglob ì‚¬ìš©)"""
    return sorted([p for p in d.rglob("*.json") if p.is_file()]) if d.exists() else []

def _none(x):
    # ê²°ì¸¡ì€ Noneìœ¼ë¡œ í†µì¼
    if x is None:
        return None
    if isinstance(x, float) and math.isnan(x):
        return None
    try:
        if pd.isna(x):
            return None
    except Exception:
        pass
    s = str(x)
    return None if s == "" else x

def _expand_list_columns(df_lists: pd.DataFrame, key_col: str, value_cols: List[str]) -> pd.DataFrame:
    """
    ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ Kê°œ(_1.._K)ë¡œ í™•ìž¥í•˜ì§€ ì•Šê³ ,
    ê° ì…€ì˜ 'ì²« ë²ˆì§¸ ê°’'ë§Œ ë‚¨ê²¨ 'ì›ëž˜ ì»¬ëŸ¼ëª…'ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.
      - ì…€ ê°’ì´ ë¦¬ìŠ¤íŠ¸ë©´: ê¸¸ì´>0ì´ë©´ lst[0], ë¹„ì–´ìžˆìœ¼ë©´ NaN
      - ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´: ì›ëž˜ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    """
    if df_lists.empty:
        return df_lists

    # key ì»¬ëŸ¼ë§Œ ë¨¼ì € ë³µì‚¬
    out = df_lists[[key_col]].copy() if key_col in df_lists.columns else pd.DataFrame()

    for c in value_cols:
        col = df_lists[c].apply(
            lambda v: (v[0] if isinstance(v, list) and len(v) > 0
                       else (np.nan if isinstance(v, list) else v))
        )
        # ì ‘ë¯¸ì‚¬ ì—†ì´ 'ì›ëž˜ ì»¬ëŸ¼ëª…'ìœ¼ë¡œ ì €ìž¥
        out[c] = col

    return out


# =========================
# 1) ì´ë ¥ì„œ íŒŒì„œ (ë²„ì „ë³„)
# =========================
RESUME_COLS = [
    "SEEK_CUST_NO","TMPL_SEQNO","RESUME_TITLE","BASIC_RESUME_YN",
    "RESUME_ITEM_CLCD","DS_RESUME_ITEM_CLCD","SEQNO",
    "RESUME_ITEM_1_CD","DS_RESUME_ITEM_1_CD",
    "RESUME_ITEM_2_CD","DS_RESUME_ITEM_2_CD",
    "RESUME_ITEM_3_CD","DS_RESUME_ITEM_3_CD",
    "RESUME_ITEM_4_CD","DS_RESUME_ITEM_4_CD",
    "RESUME_ITEM_1_NM","RESUME_ITEM_2_NM","RESUME_ITEM_3_NM",
    "RESUME_ITEM_1_VAL","HIST_STDT","HIST_ENDT",
    "HIST_ITEM_GRDCD","DS_HIST_ITEM_GRDCD","ETC_ITEM_CONT",
    "CAREER_MMCNT","RESUME_ITEM_5_CD","RESUME_ITEM_5_NM",
    "RESUME_ITEM_6_CD","RESUME_ITEM_6_NM",
    "COMP_NM_OPEN_YN","JSCD","SLRY_AMT","EMMD_OCCP_CL_YEAR"
]

def _process_resume_rows(rows: List[Dict[str, Any]]) -> pd.DataFrame:
    """ê³µí†µ í›„ì²˜ë¦¬ ë¡œì§ (Long í¬ë§·ì—ì„œ Wide í¬ë§·ìœ¼ë¡œ ë³€í™˜)"""
    if not rows:
        return pd.DataFrame(columns=RESUME_COLS)

    df_long = pd.DataFrame(rows, columns=RESUME_COLS)
    grouped = df_long.groupby("SEEK_CUST_NO", as_index=False)
    df_lists = grouped.agg({c: (lambda s: list(s)) for c in RESUME_COLS if c != "SEEK_CUST_NO"})
    if "SEEK_CUST_NO" not in df_lists.columns:
        df_lists.insert(0, "SEEK_CUST_NO", grouped["SEEK_CUST_NO"].first().values)

    value_cols = [c for c in df_lists.columns if c != "SEEK_CUST_NO"]
    return _expand_list_columns(df_lists, "SEEK_CUST_NO", value_cols)


def parse_resume_to_lists_ver1(paths: List[Path]) -> pd.DataFrame:
    """RESUME_JSON/ver1, ver3 íŒŒì„œ: CONTENTS/t/RESUME_CONTENTS/it êµ¬ì¡°"""
    rows = []
    for p in paths:
        try:
            data = _read_json_safe(p)
            seek = str(data.get("SEEK_CUST_NO", ""))

            for t in data.get("CONTENTS", []):
                if str(t.get("BASIC_RESUME_YN", "")) != "Y": continue

                base = {
                    "SEEK_CUST_NO": seek, "TMPL_SEQNO": _none(t.get("TMPL_SEQNO", None)),
                    "RESUME_TITLE": _none(t.get("RESUME_TITLE", None)),
                    "BASIC_RESUME_YN": _none(t.get("BASIC_RESUME_YN", None)),
                }
                # ver1/ver3ì˜ í•µì‹¬ íŠ¹ì§•: í•­ëª©ë“¤ì´ RESUME_CONTENTS ë¦¬ìŠ¤íŠ¸ ì•ˆì— ìžˆìŒ
                contents = t.get("RESUME_CONTENTS", [])
                
                if isinstance(contents, list) and len(contents) > 0:
                    for it in contents:
                        row = dict(base)
                        for k in RESUME_COLS:
                            if k in row: continue
                            row[k] = _none(it.get(k, None))
                        rows.append(row)
                else:
                    # í•­ëª©ì´ ì—†ìœ¼ë©´ ìƒë‹¨ë§Œ 1ê±´ìœ¼ë¡œ
                    row = dict(base)
                    for k in RESUME_COLS:
                        if k in row: continue
                        row[k] = None
                    rows.append(row)
        except Exception as e:
            logger.debug(f"Error parsing resume (ver1/3) file {p}: {e}")
            continue
    
    return _process_resume_rows(rows)


def parse_resume_to_lists_ver2(paths: List[Path]) -> pd.DataFrame:
    """RESUME_JSON/ver2 íŒŒì„œ: CONTENTS/tì— í•­ëª© í‚¤ë“¤ì´ í‰íƒ„í•˜ê²Œ ì¡´ìž¬"""
    rows = []
    for p in paths:
        try:
            data = _read_json_safe(p)
            seek = str(data.get("SEEK_CUST_NO", ""))

            for t in data.get("CONTENTS", []):
                # ver2ì˜ í•µì‹¬ íŠ¹ì§•: í•­ëª©ë“¤ì´ t ë ˆë²¨ì— í‰íƒ„í•˜ê²Œ ì¡´ìž¬í•¨
                if str(t.get("BASIC_RESUME_YN", "")) != "Y": continue

                base = {
                    "SEEK_CUST_NO": seek, "TMPL_SEQNO": _none(t.get("TMPL_SEQNO", None)),
                    "RESUME_TITLE": _none(t.get("RESUME_TITLE", None)),
                    "BASIC_RESUME_YN": _none(t.get("BASIC_RESUME_YN", None)),
                }
                
                row = dict(base)
                for k in RESUME_COLS:
                    if k in row: continue
                    # tì—ì„œ ì§ì ‘ ê°’ì„ ê°€ì ¸ì˜´
                    row[k] = _none(t.get(k, None))
                rows.append(row)
        except Exception as e:
            logger.debug(f"Error parsing resume (ver2) file {p}: {e}")
            continue

    return _process_resume_rows(rows)


def get_resume_parser(all_paths: List[Path]) -> Callable[[List[Path]], pd.DataFrame]:
    """
    ì´ë ¥ì„œ JSON ê²½ë¡œë¥¼ ìŠ¤ìº”í•˜ì—¬ ìœ íš¨í•œ íŒŒì„œ í•¨ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (RESUME_CONTENTS í‚¤ì˜ ì¡´ìž¬ ì—¬ë¶€ë¡œ ver1/ver3 vs ver2ë¥¼ íŒë³„)
    """
    if not all_paths:
        logger.warning("No resume JSON files found. Returning empty parser.")
        return lambda paths: pd.DataFrame(columns=RESUME_COLS)

    # ìƒ˜í”Œ íŒŒì¼ ë¡œë“œ
    sample_path = all_paths[0]
    try:
        data = _read_json_safe(sample_path)
        contents = data.get("CONTENTS", [])
        
        # ëŒ€í‘œ í…œí”Œë¦¿ í•­ëª©(contentsì˜ ì²« ë²ˆì§¸ í•­ëª©)ì˜ êµ¬ì¡°ë¥¼ í™•ì¸
        if contents and isinstance(contents, list) and len(contents) > 0:
            first_content = contents[0]
            
            # Ver1/Ver3 êµ¬ì¡° íŒë³„: RESUME_CONTENTS í‚¤ì˜ ì¡´ìž¬ ì—¬ë¶€
            if "RESUME_CONTENTS" in first_content:
                logger.info(f"Resume JSON structure detected as: ver1/ver3 (based on 'RESUME_CONTENTS' key).")
                # ver1ê³¼ ver3ëŠ” ë¡œì§ì´ ë™ì¼í•˜ë¯€ë¡œ ver1 íŒŒì„œ ì‚¬ìš©
                return lambda paths: parse_resume_to_lists_ver1(all_paths)
            else:
                # RESUME_CONTENTSê°€ ì—†ìœ¼ë©´ Ver2 êµ¬ì¡°ë¡œ ê°„ì£¼
                # Ver2 JSON íŒŒì¼ì´ Ver1 JSONì²˜ëŸ¼ ë³´ì´ë„ë¡ (RESUME_CONTENTSë¥¼ Noneìœ¼ë¡œ) ë§Œë“¤ ìˆ˜ë„ ìžˆìœ¼ë‚˜,
                # JSON êµ¬ì¡°ì— ë”°ë¼ Ver1ì´ Ver2 ë°ì´í„°ë¥¼ ì½ì–´ë„ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šì•„ Ver1ì´ ë¨¼ì € ì„ íƒë˜ëŠ” ë¬¸ì œë¥¼
                # íšŒí”¼í•˜ê¸° ìœ„í•´, RESUME_CONTENTSê°€ ì—†ìœ¼ë©´ Ver2ë¡œ ëª…í™•ížˆ ë¶„ë¦¬í•©ë‹ˆë‹¤.
                logger.info(f"Resume JSON structure detected as: ver2 (based on absence of 'RESUME_CONTENTS' key).")
                return lambda paths: parse_resume_to_lists_ver2(all_paths)

        else:
            logger.warning("Sample resume file is valid but 'CONTENTS' field is empty or invalid list.")
            return lambda paths: pd.DataFrame(columns=RESUME_COLS)

    except Exception as e:
        logger.error(f"Error during resume parser auto-detection for file {sample_path}: {e}")
        # ì˜ˆì™¸ ë°œìƒ ì‹œ, Ver1ì„ ë¨¼ì € ì‹œë„í•˜ì—¬ fallback
        logger.warning("Fallback: Attempting to use Ver1 parser due to detection error.")
        return lambda paths: parse_resume_to_lists_ver1(all_paths)


# =========================
# 2) ìžê¸°ì†Œê°œì„œ íŒŒì„œ (ë²„ì „ë³„)
# =========================
COVER_COLS = [
    "SEEK_CUST_NO","SFID_NO","BASS_SFID_YN",
    "SFID_IEM_SN","SFID_SJNM","SFID_IEM_SECD","DS_SFID_IEM_SECD","SFID_IEM_SJNM","SELF_INTRO_CONT"
]

def _process_cover_rows(rows: List[Dict[str, Any]]) -> pd.DataFrame:
    """ê³µí†µ í›„ì²˜ë¦¬ ë¡œì§ (Long í¬ë§·ì—ì„œ Wide í¬ë§·ìœ¼ë¡œ ë³€í™˜)"""
    if not rows:
        return pd.DataFrame(columns=COVER_COLS)

    df_long = pd.DataFrame(rows, columns=COVER_COLS)
    grouped = df_long.groupby("SEEK_CUST_NO", as_index=False)
    df_lists = grouped.agg({c: (lambda s: list(s)) for c in COVER_COLS if c != "SEEK_CUST_NO"})
    if "SEEK_CUST_NO" not in df_lists.columns:
        df_lists.insert(0, "SEEK_CUST_NO", grouped["SEEK_CUST_NO"].first().values)

    value_cols = [c for c in df_lists.columns if c != "SEEK_CUST_NO"]
    return _expand_list_columns(df_lists, "SEEK_CUST_NO", value_cols)


def parse_cover_to_lists_ver1(paths: List[Path]) -> pd.DataFrame:
    """COVERLETTERS_JSON/ver1 íŒŒì„œ: CONTENTS/s/COVERLETER_CONTENTS/it êµ¬ì¡° (ì˜¤íƒ€ í¬í•¨)"""
    rows = []
    for p in paths:
        try:
            data = _read_json_safe(p)
            seek = str(data.get("SEEK_CUST_NO", ""))
            for s in data.get("CONTENTS", []):
                sfid_no = _none(s.get("SFID_NO", None))
                bass    = _none(s.get("BASS_SFID_YN", None))
                # ver1ì˜ í•µì‹¬ íŠ¹ì§•: í•­ëª©ë“¤ì´ 'COVERLETER_CONTENTS' ë¦¬ìŠ¤íŠ¸ ì•ˆì— ìžˆìŒ (ì˜¤íƒ€ ì£¼ì˜)
                for it in s.get("COVERLETER_CONTENTS", []): 
                    rows.append({
                        "SEEK_CUST_NO": seek, "SFID_NO": sfid_no, "BASS_SFID_YN": bass,
                        "SFID_IEM_SN": _none(it.get("SFID_IEM_SN", None)),
                        "SFID_SJNM": _none(it.get("SFID_SJNM", None)),
                        "SFID_IEM_SECD": _none(it.get("SFID_IEM_SECD", None)),
                        "DS_SFID_IEM_SECD": _none(it.get("DS_SFID_IEM_SECD", None)),
                        "SFID_IEM_SJNM": _none(it.get("SFID_IEM_SJNM", None)),
                        "SELF_INTRO_CONT": _none(it.get("SELF_INTRO_CONT", None)),
                    })
        except Exception as e:
            logger.debug(f"Error parsing cover (ver1) file {p}: {e}")
            continue
    return _process_cover_rows(rows)


def parse_cover_to_lists_ver2(paths: List[Path]) -> pd.DataFrame:
    """COVERLETTERS_JSON/ver2 íŒŒì„œ: CONTENTS/sì— í•­ëª© í‚¤ë“¤ì´ í‰íƒ„í•˜ê²Œ ì¡´ìž¬"""
    rows = []
    for p in paths:
        try:
            data = _read_json_safe(p)
            seek = str(data.get("SEEK_CUST_NO", ""))
            for s in data.get("CONTENTS", []):
                # ver2ì˜ í•µì‹¬ íŠ¹ì§•: í•­ëª© í‚¤ë“¤ì´ s ë ˆë²¨ì— í‰íƒ„í•˜ê²Œ ì¡´ìž¬í•¨
                sfid_no = _none(s.get("SFID_NO", None))
                bass    = _none(s.get("BASS_SFID_YN", None))
                rows.append({
                    "SEEK_CUST_NO": seek, "SFID_NO": sfid_no, "BASS_SFID_YN": bass,
                    "SFID_IEM_SN": _none(s.get("SFID_IEM_SN", None)),
                    "SFID_SJNM": _none(s.get("SFID_SJNM", None)),
                    "SFID_IEM_SECD": _none(s.get("SFID_IEM_SECD", None)),
                    "DS_SFID_IEM_SECD": _none(s.get("DS_SFID_IEM_SECD", None)),
                    "SFID_IEM_SJNM": _none(s.get("SFID_IEM_SJNM", None)),
                    "SELF_INTRO_CONT": _none(s.get("SELF_INTRO_CONT", None)),
                })
        except Exception as e:
            logger.debug(f"Error parsing cover (ver2) file {p}: {e}")
            continue
    return _process_cover_rows(rows)


def get_cover_parser(all_paths: List[Path]) -> Callable[[List[Path]], pd.DataFrame]:
    """
    ìžê¸°ì†Œê°œì„œ JSON ê²½ë¡œë¥¼ ìŠ¤ìº”í•˜ì—¬ ìœ íš¨í•œ íŒŒì„œ í•¨ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (COVERLETER_CONTENTS í‚¤ì˜ ì¡´ìž¬ ì—¬ë¶€ë¡œ ver1 vs ver2ë¥¼ íŒë³„)
    """
    if not all_paths:
        logger.warning("No cover letter JSON files found. Returning empty parser.")
        return lambda paths: pd.DataFrame(columns=COVER_COLS)

    sample_path = all_paths[0]
    try:
        data = _read_json_safe(sample_path)
        contents = data.get("CONTENTS", [])
        
        if contents and isinstance(contents, list) and len(contents) > 0:
            first_content = contents[0]
            
            # Ver1 êµ¬ì¡° íŒë³„: COVERLETER_CONTENTS í‚¤ì˜ ì¡´ìž¬ ì—¬ë¶€
            if "COVERLETER_CONTENTS" in first_content:
                logger.info(f"Cover letter JSON structure detected as: ver1 (based on 'COVERLETER_CONTENTS' key).")
                return lambda paths: parse_cover_to_lists_ver1(all_paths)
            else:
                # COVERLETER_CONTENTSê°€ ì—†ìœ¼ë©´ Ver2 êµ¬ì¡°ë¡œ ê°„ì£¼
                logger.info(f"Cover letter JSON structure detected as: ver2 (based on absence of 'COVERLETER_CONTENTS' key).")
                return lambda paths: parse_cover_to_lists_ver2(all_paths)
        else:
            logger.warning("Sample cover letter file is valid but 'CONTENTS' field is empty or invalid list.")
            return lambda paths: pd.DataFrame(columns=COVER_COLS)

    except Exception as e:
        logger.error(f"Error during cover parser auto-detection for file {sample_path}: {e}")
        # ì˜ˆì™¸ ë°œìƒ ì‹œ, Ver1ì„ ë¨¼ì € ì‹œë„í•˜ì—¬ fallback
        logger.warning("Fallback: Attempting to use Ver1 parser due to detection error.")
        return lambda paths: parse_cover_to_lists_ver1(all_paths)


# =========================
# 3) ì§ì—…í›ˆë ¨ íŒŒì„œ (ê³µí†µ)
# =========================
TRAINING_BASE_COLS = [
    "CLOS_YM","JHNT_CTN","JHCR_DE","CRSE_ID","TGCR_TME",
    "TRNG_CRSN","TRNG_BGDE","TRNG_ENDE","TRNG_JSCD","KECO_CD","SORT_SN","ETL_DT",
]

def parse_train_to_lists(paths: List[Path]) -> pd.DataFrame:
    rows = []
    for p in paths:
        try:
            data = _read_json_safe(p)
        except Exception as e:
            logger.debug(f"Error parsing training file {p}: {e}")
            continue
        clos = _none(data.get("CLOS_YM", None))
        jhnt = _none(data.get("JHNT_CTN", None))
        jhcr_de = _none(data.get("JHCR_DE", None))
        contents = data.get("CONTENTS", [])
        if contents:
            for it in contents:
                sort_sn = it.get("SORT_SN", it.get("SORTN_SN", None))
                rows.append({
                    "CLOS_YM": clos, "JHNT_CTN": jhnt, "JHCR_DE": jhcr_de,
                    "CRSE_ID": _none(it.get("CRSE_ID", None)),
                    "TGCR_TME": _none(it.get("TGCR_TME", None)),
                    "TRNG_CRSN": _none(it.get("TRNG_CRSN", None)),
                    "TRNG_BGDE": _none(it.get("TRNG_BGDE", None)),
                    "TRNG_ENDE": _none(it.get("TRNG_ENDE", None)),
                    "TRNG_JSCD": _none(it.get("TRNG_JSCD", None)),
                    "KECO_CD": _none(it.get("KECO_CD", None)),
                    "SORT_SN": _none(sort_sn),
                    "ETL_DT": _none(it.get("ETL_DT", None)),
                })
        else:
            rows.append({
                "CLOS_YM": clos, "JHNT_CTN": jhnt, "JHCR_DE": jhcr_de,
                "CRSE_ID": None,"TGCR_TME": None,"TRNG_CRSN": None,"TRNG_BGDE": None,
                "TRNG_ENDE": None,"TRNG_JSCD": None,"KECO_CD": None,"SORT_SN": None,"ETL_DT": None,
            })

    if not rows:
        return pd.DataFrame(columns=TRAINING_BASE_COLS)

    df_long = pd.DataFrame(rows, columns=TRAINING_BASE_COLS)
    grouped = df_long.groupby("JHNT_CTN", as_index=False)
    df_lists = grouped.agg({c: (lambda s: list(s)) for c in TRAINING_BASE_COLS if c != "JHNT_CTN"})
    if "JHNT_CTN" not in df_lists.columns:
        df_lists.insert(0, "JHNT_CTN", grouped["JHNT_CTN"].first().values)

    value_cols = [c for c in df_lists.columns if c != "JHNT_CTN"]
    df_expanded = _expand_list_columns(df_lists, "JHNT_CTN", value_cols)
    return df_expanded

# =========================
# 4) ìžê²©ì¦ íŒŒì„œ (ê³µí†µ)
# =========================
LICENSE_BASE_COLS = ["CLOS_YM","JHNT_CTN","CRQF_CD","QULF_ITNM","QULF_LCNS_LCFN","ETL_DT"]

def parse_license_to_lists(paths: List[Path]) -> pd.DataFrame:
    rows = []
    for p in paths:
        try:
            data = _read_json_safe(p)
        except Exception as e:
            logger.debug(f"Error parsing license file {p}: {e}")
            continue
        clos = _none(data.get("CLOS_YM", None))
        jhnt = _none(data.get("JHNT_CTN", None))
        contents = data.get("CONTENTS", [])
        if contents:
            for it in contents:
                rows.append({
                    "CLOS_YM": clos, "JHNT_CTN": jhnt,
                    "CRQF_CD": _none(it.get("CRQF_CD", None)),
                    "QULF_ITNM": _none(it.get("QULF_ITNM", None)),
                    "QULF_LCNS_LCFN": _none(it.get("QULF_LCNS_LCFN", None)),
                    "ETL_DT": _none(it.get("ETL_DT", None)),
                })
        else:
            rows.append({
                "CLOS_YM": clos, "JHNT_CTN": jhnt,
                "CRQF_CD": None,"QULF_ITNM": None,"QULF_LCNS_LCFN": None,"ETL_DT": None,
            })

    if not rows:
        return pd.DataFrame(columns=LICENSE_BASE_COLS)

    df_long = pd.DataFrame(rows, columns=LICENSE_BASE_COLS)
    grouped = df_long.groupby("JHNT_CTN", as_index=False)
    df_lists = grouped.agg({c: (lambda s: list(s)) for c in LICENSE_BASE_COLS if c != "JHNT_CTN"})
    if "JHNT_CTN" not in df_lists.columns:
        df_lists.insert(0, "JHNT_CTN", grouped["JHNT_CTN"].first().values)

    value_cols = [c for c in df_lists.columns if c != "JHNT_CTN"]
    df_expanded = _expand_list_columns(df_lists, "JHNT_CTN", value_cols)
    return df_expanded

# =========================
# 5) ì¡°ë¦½: CSV LEFT ê¸°ì¤€
# =========================
def build_pipeline_wide(logger: logging.LoggerAdapter) -> pd.DataFrame:
    logger.info("Reading raw CSV data.")
    base = pd.read_csv(RAW_CSV, encoding="utf-8")
    for k in ["JHNT_MBN","JHNT_CTN"]:
        if k in base.columns:
            base[k] = base[k].astype(str)

    # 1. ì´ë ¥ì„œ ë°ì´í„° ì²˜ë¦¬
    logger.info(f"Collecting resume JSON files from {RESUME_DIR}.")
    all_resume_paths = _collect_json(RESUME_DIR)
    resume_parser_func = get_resume_parser(all_resume_paths)
    logger.info("Starting resume data parsing.")
    resume_df = resume_parser_func(all_resume_paths) 

    # 2. ìžê¸°ì†Œê°œì„œ ë°ì´í„° ì²˜ë¦¬
    logger.info(f"Collecting cover letter JSON files from {COVER_DIR}.")
    all_cover_paths = _collect_json(COVER_DIR)
    cover_parser_func = get_cover_parser(all_cover_paths)
    logger.info("Starting cover letter data parsing.")
    cover_df = cover_parser_func(all_cover_paths)

    # 3. ì§ì—…í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬
    logger.info(f"Collecting and parsing training data from {TRAINING_DIR}.")
    training_df = parse_train_to_lists(_collect_json(TRAINING_DIR))

    # 4. ìžê²©ì¦ ë°ì´í„° ì²˜ë¦¬
    logger.info(f"Collecting and parsing license data from {LICENSE_DIR}.")
    license_df  = parse_license_to_lists(_collect_json(LICENSE_DIR))

    out = base.copy()
    logger.info(f"Base DataFrame shape: {out.shape}")

    # ë°ì´í„° ë³‘í•©
    if not resume_df.empty and "JHNT_MBN" in out.columns:
        out = out.merge(resume_df, left_on="JHNT_MBN", right_on="SEEK_CUST_NO", how="left", suffixes=("", "_resume"))
        out = out.drop(columns=["SEEK_CUST_NO"], errors="ignore")
        logger.info(f"Merged resume data. Current shape: {out.shape}")

    if not cover_df.empty and "JHNT_MBN" in out.columns:
        out = out.merge(cover_df, left_on="JHNT_MBN", right_on="SEEK_CUST_NO", how="left", suffixes=("", "_cover"))
        out = out.drop(columns=["SEEK_CUST_NO"], errors="ignore")
        logger.info(f"Merged cover letter data. Current shape: {out.shape}")

    if not training_df.empty and "JHNT_CTN" in out.columns:
        out = out.merge(training_df, on="JHNT_CTN", how="left", suffixes=("", "_training"))
        logger.info(f"Merged training data. Current shape: {out.shape}")

    if not license_df.empty and "JHNT_CTN" in out.columns:
        out = out.merge(license_df, on="JHNT_CTN", how="left", suffixes=("", "_license"))
        logger.info(f"Merged license data. Current shape: {out.shape}")

    return out

# =========================
# 6) í›„ì²˜ë¦¬ (ì˜µì…˜)
# =========================
def postprocess(df: pd.DataFrame, logger: logging.LoggerAdapter) -> pd.DataFrame:
    logger.info("Starting postprocessing: Binary mapping and date calculation.")
    
    # ---- (1) ë°”ì´ë„ˆë¦¬ ë§¤í•‘ ----
    bin_map = {"ì˜ˆ":1, "ì•„ë‹ˆì˜¤":0, "ì•„ë‹ˆìš”":0, "í•„ìš”":1, "ë¶ˆí•„ìš”":0}
    mapped_cols = [] 
    
    for col in df.columns:
        if df[col].dtype == object:
            df[col] = df[col].replace(bin_map)
            if df[col].dtype != object:
                mapped_cols.append(col)
                
    if mapped_cols:
        logger.info(f"Successfully applied Binary Mapping to {len(mapped_cols)} columns: {', '.join(mapped_cols)}") 
    else:
        logger.info("No object columns were successfully converted by binary mapping.")

    # ---- (2) ë‚ ì§œ ì°¨ì´ ê³„ì‚°  ----
    date_diff_cols = [] 
    if "JHCR_DE" in df.columns:
        anchor = pd.to_datetime(df["JHCR_DE"], errors="coerce")
        date_cols = [c for c in df.columns if any(x in c.upper() for x in ["DE","DT","DATE","BGDE","ENDE","STDT","ENDT"])]
        
        for col in date_cols:
            if col == "JHCR_DE":
                continue
            
            vals = pd.to_datetime(df[col], errors="coerce")
            
            if not anchor.isna().all() and not vals.isna().all():
                diff = (vals - anchor).dt.days
                df[col] = diff.abs()
                date_diff_cols.append(col)
            else:
                 logger.warning(f"Skipped date diff for column '{col}' due to all-NaN anchor or all-NaN target date values.")

        if date_diff_cols:
            logger.info(f"Calculated Date Difference (days from JHCR_DE) for {len(date_diff_cols)} columns: {', '.join(date_diff_cols)}") # ðŸŒŸ ë¡œê¹… ì¶”ê°€
        else:
            logger.info("No date columns were processed for date difference calculation.")
    else:
        logger.warning("Anchor column 'JHCR_DE' not found. Skipping date difference calculation.")

    # ---- (3) ëª¨ë“  ê°’ì´ ê²°ì¸¡ì¸ ì»¬ëŸ¼ ì œê±° ----
    original_cols = df.shape[1]
    cols_to_drop = df.columns[df.isnull().all()].tolist()
    
    df = df.dropna(axis=1, how="all")
    dropped_cols_count = original_cols - df.shape[1]
    
    if dropped_cols_count > 0:
        logger.info(f"Dropped {dropped_cols_count} columns that were entirely missing values: {', '.join(cols_to_drop)}") # ðŸŒŸ ë¡œê¹… ì¶”ê°€
    
    logger.info("Postprocessing complete.")
    return df