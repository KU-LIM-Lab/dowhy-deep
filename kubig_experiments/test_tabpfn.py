import sys
import pandas as pd
import numpy as np

import tabpfn 
import torch 

from dowhy import CausalModel
from dowhy.datasets import linear_dataset
from dowhy.causal_estimators.tabpfn_estimator import TabpfnEstimator


def run_case(outcome_is_binary: bool, num_common_causes: int, debug: bool = False) -> tuple:
    """
    Core runner for testing TabPFN estimator with confidence intervals.
    Returns the estimated ATE (float) and confidence intervals (tuple).
    """
    true_ate = 10

    if debug:
        print(f"\nğŸ” [DEBUG] Starting test case: outcome_is_binary={outcome_is_binary}, num_common_causes={num_common_causes}")

    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    if debug:
        print("ğŸ“Š [DEBUG] Generating test data...")
    
    data = linear_dataset(
        beta=true_ate,
        num_common_causes=num_common_causes,
        num_instruments=2,
        num_effect_modifiers=1,
        num_samples=200,
        treatment_is_binary=True,
        stddev_treatment_noise=10,
        num_discrete_common_causes=1,
        outcome_is_binary=outcome_is_binary,
    )

    if debug:
        print(f"ğŸ“Š [DEBUG] Data shape: {data['df'].shape}")
        print(f"ğŸ“Š [DEBUG] Treatment: {data['treatment_name']}")
        print(f"ğŸ“Š [DEBUG] Outcome: {data['outcome_name']}")
        print(f"ğŸ“Š [DEBUG] Common causes: {data['common_causes_names']}")
        print(f"ğŸ“Š [DEBUG] Instruments: {data['instrument_names']}")
        print(f"ğŸ“Š [DEBUG] Effect modifiers: {data['effect_modifier_names']}")
        print(f"ğŸ“Š [DEBUG] Data columns: {list(data['df'].columns)}")
        print(f"ğŸ“Š [DEBUG] Data types:\n{data['df'].dtypes}")

    # # dowhyì˜ _encode ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìˆ˜ë™ìœ¼ë¡œ ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ê°€
    # data["df"]["W0"] = pd.cut(data["df"]["W0"], bins=3, labels=["low", "medium", "high"])

    # 2. CausalModel ìƒì„±
    model = CausalModel(
        data=data["df"],
        treatment=data["treatment_name"],
        outcome=data["outcome_name"],
        common_causes=data["common_causes_names"],
        instruments=data["instrument_names"],
        effect_modifiers=data["effect_modifier_names"],
        graph=data["gml_graph"],
    )

    # 3. ì¸ê³¼ íš¨ê³¼ ì‹ë³„
    if debug:
        print("ğŸ” [DEBUG] Identifying causal effect...")
    
    identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
    
    if debug:
        print(f"ğŸ” [DEBUG] Identified estimand: {identified_estimand}")

    # 4. ì¸ê³¼ íš¨ê³¼ ì¶”ì • (confidence intervals í¬í•¨)
    if debug:
        print("ğŸ“ˆ [DEBUG] Estimating causal effect with confidence intervals...")
    
    estimate = model.estimate_effect(
        identified_estimand,
        method_name="backdoor.tabpfn",
        method_params={"estimator": TabpfnEstimator, "N_ensemble_configurations": 8},
        confidence_intervals=True,  # confidence intervals í™œì„±í™”
    )

    if debug:
        print(f"ğŸ“ˆ [DEBUG] Estimate object: {type(estimate)}")
        print(f"ğŸ“ˆ [DEBUG] Estimate value: {estimate.value}")
        print(f"ğŸ“ˆ [DEBUG] Estimate attributes: {dir(estimate)}")

    # 5. ê²°ê³¼ ê²€ì¦
    if estimate is None:
        raise ValueError("ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ê°€ Noneì…ë‹ˆë‹¤.")
    
    if not isinstance(estimate.value, (float, np.floating)):
        raise TypeError("ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ëŠ” float íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    # 6. Confidence Intervals ê²€ì¦
    if debug:
        print("ğŸ“Š [DEBUG] Validating confidence intervals...")
    
    ci = None
    try:
        ci = estimate.get_confidence_intervals()
        if debug:
            print(f"ğŸ“Š [DEBUG] Raw CI result: {ci}")
            print(f"ğŸ“Š [DEBUG] CI type: {type(ci)}")
        
        if ci is None:
            raise ValueError("Confidence intervalsê°€ Noneì…ë‹ˆë‹¤.")
        
        if not isinstance(ci, (tuple, list)) or len(ci) != 2:
            raise TypeError("Confidence intervalsëŠ” (lower_bound, upper_bound) íŠœí”Œì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        lower_bound, upper_bound = ci
        if not all(isinstance(bound, (float, np.floating)) for bound in ci):
            raise TypeError("Confidence interval boundsëŠ” float íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        if lower_bound >= upper_bound:
            raise ValueError(f"Confidence intervalì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {lower_bound} >= {upper_bound}")
            
        if debug:
            print(f"ğŸ“Š [DEBUG] CI validation passed: [{lower_bound:.4f}, {upper_bound:.4f}]")
            
    except Exception as e:
        if debug:
            print(f"âŒ [DEBUG] CI validation failed: {str(e)}")
        raise ValueError(f"Confidence intervals ê²€ì¦ ì‹¤íŒ¨: {str(e)}")

    # ì—°ì†í˜• ê²°ê³¼(Regression)ì˜ ê²½ìš° í—ˆìš© ì˜¤ì°¨ ë‚´ í™•ì¸
    if not outcome_is_binary:
        # ì‘ì€ ë°ì´í„°ì…‹ê³¼ ë³µì¡í•œ ëª¨ë¸ì´ë¯€ë¡œ í—ˆìš© ì˜¤ì°¨ëŠ” í¬ê²Œ ì„¤ì •
        error_tolerance = 4
        diff = abs(float(estimate.value) - 10.0)
        if diff > error_tolerance:
            print(f"[WARN] Estimated ATE deviates from true ATE by {diff:.3f} (> {error_tolerance}).")

    # ê²°ê³¼ ì¶œë ¥
    kind = "binary" if outcome_is_binary else "continuous"
    print(f"\n[TabPFN Python] Case: {kind}")
    if not outcome_is_binary:
        print(f"  - True ATE: {true_ate}")
    print(f"  - Estimated ATE: {float(estimate.value):.4f}")
    print(f"  - 95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]")
    print(f"  - CI Width: {ci[1] - ci[0]:.4f}")

    return float(estimate.value), ci


def test_tabpfn_estimator_ate(debug: bool = False):
    """Test TabPFN estimator for both regression and classification cases with confidence intervals."""
    print("=" * 60)
    print("TabPFN Estimator Test with Confidence Intervals")
    print("=" * 60)
    
    test_cases = [
        (False, 5),  # Regression
        (True, 5),   # Classification
    ]
    
    results = []
    
    for i, (outcome_is_binary, num_common_causes) in enumerate(test_cases, 1):
        print(f"\n--- Test Case {i} ---")
        try:
            ate, ci = run_case(
                outcome_is_binary=outcome_is_binary, 
                num_common_causes=num_common_causes,
                debug=debug
            )
            results.append((outcome_is_binary, num_common_causes, ate, ci, "PASS"))
            print(f"âœ… Test Case {i} PASSED")
        except Exception as e:
            results.append((outcome_is_binary, num_common_causes, None, None, f"FAIL: {str(e)}"))
            print(f"âŒ Test Case {i} FAILED: {str(e)}")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("Test Results Summary")
    print("=" * 60)
    
    passed = 0
    failed = 0
    
    for i, (outcome_is_binary, num_common_causes, ate, ci, status) in enumerate(results, 1):
        case_type = "Classification" if outcome_is_binary else "Regression"
        print(f"Case {i} ({case_type}): {status}")
        if ate is not None:
            print(f"  - Estimated ATE: {ate:.4f}")
        if ci is not None:
            print(f"  - 95% CI: [{ci[0]:.4f}, {ci[1]:.4f}]")
            print(f"  - CI Width: {ci[1] - ci[0]:.4f}")
        passed += 1 if "PASS" in status else 0
        failed += 1 if "FAIL" in status else 0
    
    print(f"\nTotal: {passed} passed, {failed} failed")
    
    if failed == 0:
        print("All tests passed!")
        return True
    else:
        print("Some tests failed!")
        return False


def verify_estimator_usage():
    """Verify that TabpfnEstimator is being used correctly."""
    print("ğŸ” [VERIFICATION] Checking estimator usage...")
    print("=" * 60)
    
    # 1. ì§ì ‘ estimator í´ë˜ìŠ¤ í™•ì¸
    from dowhy.causal_estimators import get_class_object
    try:
        estimator_class = get_class_object("tabpfn_estimator")
        print(f"âœ… [VERIFICATION] get_class_object('tabpfn_estimator') = {estimator_class}")
        print(f"âœ… [VERIFICATION] Class name: {estimator_class.__name__}")
        print(f"âœ… [VERIFICATION] Module: {estimator_class.__module__}")
        print(f"âœ… [VERIFICATION] Is TabpfnEstimator? {estimator_class.__name__ == 'TabpfnEstimator'}")
    except Exception as e:
        print(f"âŒ [VERIFICATION] get_class_object failed: {e}")
        return False
    
    # 2. CausalModelì—ì„œ estimator ìƒì„± í™•ì¸
    try:
        from dowhy import CausalModel
        from dowhy.datasets import linear_dataset
        
        # ê°„ë‹¨í•œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        data = linear_dataset(
            beta=10,
            num_common_causes=2,
            num_instruments=0,
            num_effect_modifiers=0,
            num_samples=100,
            treatment_is_binary=True,
            outcome_is_binary=False,
        )
        
        model = CausalModel(
            data=data["df"],
            treatment=data["treatment_name"],
            outcome=data["outcome_name"],
            common_causes=data["common_causes_names"],
            graph=data["gml_graph"],
        )
        
        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
        
        # method_nameìœ¼ë¡œ estimator ìƒì„± í™•ì¸
        method_name = "backdoor.tabpfn"
        # num_components = len(method_name.split("."))
        str_arr = method_name.split(".", maxsplit=1)
        identifier_name = str_arr[0]  # "backdoor"
        estimator_name = str_arr[1]   # "tabpfn"
        
        print(f"âœ… [VERIFICATION] Method name: {method_name}")
        print(f"âœ… [VERIFICATION] Identifier: {identifier_name}")
        print(f"âœ… [VERIFICATION] Estimator name: {estimator_name}")
        
        # get_class_object í˜¸ì¶œ
        causal_estimator_class = get_class_object(estimator_name + "_estimator")
        print(f"âœ… [VERIFICATION] Retrieved class: {causal_estimator_class}")
        
        # Estimator ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        estimator_instance = causal_estimator_class(
            identified_estimand,
            test_significance=False,
            evaluate_effect_strength=False,
            confidence_intervals=False,
        )
        print(f"âœ… [VERIFICATION] Created instance: {type(estimator_instance)}")
        print(f"âœ… [VERIFICATION] Instance class name: {estimator_instance.__class__.__name__}")
        
        return True
        
    except Exception as e:
        print(f"âŒ [VERIFICATION] CausalModel test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_single_case_debug():
    """Debug single test case with detailed output."""
    print("ğŸ” [DEBUG MODE] Running single test case...")
    print("=" * 60)
    
    try:
        ate, ci = run_case(
            outcome_is_binary=False,  # Regression
            num_common_causes=5,
            debug=True
        )
        print(f"\n [SUCCESS] Test completed successfully!")
        print(f"  - ATE: {ate:.4f}")
        print(f"  - CI: [{ci[0]:.4f}, {ci[1]:.4f}]")
        return True
    except Exception as e:
        print(f"\nâŒ [ERROR] Test failed: {str(e)}")
        import traceback
        print(f"\nğŸ“‹ [TRACEBACK]:")
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Test TabPFN estimator with confidence intervals")
    parser.add_argument("--debug", action="store_true", help="Enable debugging prints")
    parser.add_argument("--single", action="store_true", help="Run single test case")
    parser.add_argument("--verify", action="store_true", help="Verify estimator usage")
    args = parser.parse_args()
    
    if args.verify:
        # Verify estimator usage
        success = verify_estimator_usage()
    elif args.single:
        # Run single test case with debug mode
        success = test_single_case_debug()
    else:
        # Run full test suite
        success = test_tabpfn_estimator_ate(debug=args.debug)
    
    sys.exit(0 if success else 1)