"""
DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ ì¸ê³¼ëª¨ë¸ êµ¬ì¶•, ì¶”ì •, ê²€ì¦ End-to-End íŒŒì´í”„ë¼ì¸

ìˆ˜ì • ì‚¬í•­:
- ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°(JSON) í†µí•© ë¡œë“œ
- JHNT_CTNì„ PKë¡œ ë°ì´í„° ë³‘í•©
- treatment íŒŒë¼ë¯¸í„°ë¥¼ argparserë¡œ ì…ë ¥ë°›ì•„ ë‹¤ì–‘í•œ ì‹¤í—˜ ì§€ì›
"""

import argparse
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import warnings
from pathlib import Path
import logging
from datetime import datetime
import os
import sys
import json

# DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import dowhy
from dowhy import CausalModel
import networkx as nx

# ë¡œì»¬ DoWhy ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))

# ëª¨ë“ˆ ì„í¬íŠ¸
from . import preprocess
from . import estimation

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# DoWhy ë¡œê±° ë ˆë²¨ ì„¤ì •
import logging as dowhy_logging
dowhy_logging.getLogger("dowhy.causal_estimator").setLevel(dowhy_logging.WARNING)
dowhy_logging.getLogger("dowhy.causal_estimators").setLevel(dowhy_logging.WARNING)

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False


def create_causal_graph(graph_file):
    """
    ê·¸ë˜í”„ íŒŒì¼ì„ ì½ì–´ì„œ NetworkX ì¸ê³¼ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ (DOT í˜•ì‹)
    
    Returns:
        nx.DiGraph: ì¸ê³¼ ê·¸ë˜í”„ ê°ì²´
    """
    try:
        # DOT íŒŒì¼ì„ ì½ì–´ì„œ NetworkX ê·¸ë˜í”„ë¡œ ë³€í™˜
        G = nx.drawing.nx_pydot.read_dot(graph_file)
        
        # ë¬¸ìì—´ ë…¸ë“œëª…ì„ ì •ë¦¬ (ë”°ì˜´í‘œ ì œê±°)
        node_mapping = {}
        for node in list(G.nodes()):
            if isinstance(node, str):
                clean_node = node.strip('"\'')
                if clean_node != node:
                    node_mapping[node] = clean_node
        
        # ë…¸ë“œëª… ë³€ê²½
        G = nx.relabel_nodes(G, node_mapping)
        
        # ë°©í–¥ì„± ê·¸ë˜í”„ë¡œ ë³€í™˜ (DOT íŒŒì¼ì´ ë¬´ë°©í–¥ì¼ ìˆ˜ ìˆìŒ)
        if not G.is_directed():
            G = G.to_directed()
        
        return G
        
    except Exception as e:
        print(f"âš ï¸ ê·¸ë˜í”„ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        print("ê¸°ë³¸ ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ê¸°ë³¸ ê·¸ë˜í”„ ìƒì„± (fallback)
        G = nx.DiGraph()
        G.add_node("ACCR_CD", label="í•™ë ¥ì½”ë“œ")
        G.add_node("ACQ_180_YN", label="180ì¼ì´ë‚´ì·¨ì—…ì—¬ë¶€")
        G.add_edge("ACCR_CD", "ACQ_180_YN")
        
        return G


def load_all_data(data_dir, graph_file):
    """
    ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°(JSON)ë¥¼ ëª¨ë‘ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        graph_file (str): ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        tuple: (ì •í˜•ë°ì´í„°_df, ë¹„ì •í˜•ë°ì´í„°_ë”•ì…”ë„ˆë¦¬, ì¸ê³¼ê·¸ë˜í”„)
    """
    data_path = Path(data_dir)
    
    # 1. ì •í˜• ë°ì´í„° ë¡œë“œ
    structured_data = pd.read_csv(data_path / "data.csv", encoding='utf-8')
    print(f"âœ… ì •í˜• ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(structured_data)}ê±´")
    
    # 2. ë¹„ì •í˜• ë°ì´í„°(JSON) ë¡œë“œ
    unstructured_data = {}
    
    json_files = [
        ("COVERLETTERS_JSON.json", "ìê¸°ì†Œê°œì„œ"),
        ("RESUME_JSON.json", "ì´ë ¥ì„œ"),
        ("TRAININGS_JSON.json", "ì§ì—…í›ˆë ¨"),
        ("LICENSES_JSON.json", "ìê²©ì¦")
    ]
    
    for filename, json_type in json_files:
        json_path = data_path / filename
        if json_path.exists():
            with open(json_path, 'r', encoding='utf-8') as f:
                unstructured_data[json_type] = json.load(f)
            print(f"âœ… {json_type} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(unstructured_data[json_type])}ê±´")
        else:
            print(f"âš ï¸ {json_type} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    # 3. ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ
    causal_graph = create_causal_graph(graph_file)
    print(f"âœ… ì¸ê³¼ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ: {causal_graph.number_of_nodes()}ê°œ ë…¸ë“œ, {causal_graph.number_of_edges()}ê°œ ì—£ì§€")
    
    return structured_data, unstructured_data, causal_graph


def preprocess_unstructured_data(unstructured_data, data_dir):
    """
    ë¹„ì •í˜• ë°ì´í„°(JSON)ë¥¼ ì •í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        unstructured_data (dict): ë¹„ì •í˜• ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        data_dir (str): ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
    """
    preprocessor = preprocess.Preprocessor([])
    
    processed_dfs = {}
    
    # ê° JSON íƒ€ì…ë³„ë¡œ ì „ì²˜ë¦¬ ìˆ˜í–‰
    for json_type, data in unstructured_data.items():
        try:
            # JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            if json_type == "ìê¸°ì†Œê°œì„œ":
                df = _convert_coverletters_to_df(data)
            elif json_type == "ì´ë ¥ì„œ":
                df = _convert_resume_to_df(data)
            elif json_type == "ì§ì—…í›ˆë ¨":
                df = _convert_trainings_to_df(data)
            elif json_type == "ìê²©ì¦":
                df = _convert_licenses_to_df(data)
            else:
                df = pd.DataFrame()
            
            if not df.empty:
                processed_dfs[json_type] = df
                print(f"âœ… {json_type} ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê±´")
            
        except Exception as e:
            print(f"âš ï¸ {json_type} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            processed_dfs[json_type] = pd.DataFrame()
    
    return processed_dfs


def _convert_coverletters_to_df(data):
    """ìê¸°ì†Œê°œì„œ JSONì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    rows = []
    for record in data:
        seek_cust_no = record.get("SEEK_CUST_NO")
        for coverletter in record.get("COVERLETTERS", []):
            row = {"SEEK_CUST_NO": seek_cust_no}
            row["SFID_NO"] = coverletter.get("SFID_NO")
            row["SFID_IEM_NUM"] = len(coverletter.get("ITEMS", []))
            row["SFID_LTTR_NUM"] = sum(
                len(item.get("SELF_INTRO_CONT", "")) 
                for item in coverletter.get("ITEMS", [])
            )
            rows.append(row)
    return pd.DataFrame(rows)


def _convert_resume_to_df(data):
    """ì´ë ¥ì„œ JSONì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    rows = []
    for record in data:
        seek_cust_no = record.get("SEEK_CUST_NO")
        # ê°„ë‹¨í•œ ì§‘ê³„ ì •ë³´ë§Œ ì¶”ì¶œ
        row = {"SEEK_CUST_NO": seek_cust_no}
        rows.append(row)
    return pd.DataFrame(rows)


def _convert_trainings_to_df(data):
    """ì§ì—…í›ˆë ¨ JSONì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    rows = []
    for record in data:
        seek_cust_no = record.get("SEEK_CUST_NO")
        jhnt_ctn = record.get("JHNT_CTN")
        for training in record.get("TRAININGS", []):
            row = {
                "SEEK_CUST_NO": seek_cust_no,
                "JHNT_CTN": jhnt_ctn,
                "KECO_CD": training.get("KECO_CD"),
                "TRNG_JSCD": training.get("TRNG_JSCD")
            }
            rows.append(row)
    
    df = pd.DataFrame(rows)
    # JHNT_CTNë³„ë¡œ ì§‘ê³„
    if not df.empty:
        df = df.groupby("JHNT_CTN").agg({
            "KECO_CD": lambda x: ",".join([str(v) for v in x if pd.notna(v)]) if len(x) > 0 else "",
            "TRNG_JSCD": lambda x: ",".join([str(v) for v in x if pd.notna(v)]) if len(x) > 0 else ""
        }).reset_index()
        return df
    else:
        return pd.DataFrame()


def _convert_licenses_to_df(data):
    """ìê²©ì¦ JSONì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    rows = []
    for record in data:
        seek_cust_no = record.get("SEEK_CUST_NO")
        jhnt_ctn = record.get("JHNT_CTN")
        licenses = record.get("LICENSES", [])
        row = {
            "SEEK_CUST_NO": seek_cust_no,
            "JHNT_CTN": jhnt_ctn,
            "CRQF_CT": len(licenses)
        }
        rows.append(row)
    return pd.DataFrame(rows)


def merge_all_data(structured_data, processed_dfs):
    """
    ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°ë¥¼ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        structured_data (pd.DataFrame): ì •í˜• ë°ì´í„°
        processed_dfs (dict): ì „ì²˜ë¦¬ëœ ë¹„ì •í˜• ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    
    Returns:
        pd.DataFrame: ë³‘í•©ëœ ë°ì´í„°í”„ë ˆì„
    """
    # ì •í˜• ë°ì´í„°ê°€ ê¸°ì¤€ì´ ë¨
    merged_df = structured_data.copy()
    
    # JHNT_CTN ë˜ëŠ” SEEK_CUST_NOë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
    for json_type, df in processed_dfs.items():
        if df.empty:
            continue
        
        # ë³‘í•© í‚¤ ê²°ì •
        if "JHNT_CTN" in df.columns:
            merge_key = "JHNT_CTN"
        elif "SEEK_CUST_NO" in df.columns:
            # SEEK_CUST_NOê°€ ìˆëŠ” ê²½ìš°, ì •í˜• ë°ì´í„°ì˜ JHNT_MBNê³¼ ë§¤í•‘ í•„ìš”
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ skipí•˜ê³  ë‚˜ì¤‘ì— êµ¬í˜„
            continue
        else:
            continue
        
        if merge_key in merged_df.columns:
            merged_df = merged_df.merge(
                df,
                on=merge_key,
                how="left",
                suffixes=('', f'_{json_type}')
            )
            print(f"âœ… {json_type} ë°ì´í„° ë³‘í•© ì™„ë£Œ")
    
    return merged_df


def setup_logging(args):
    """ë¡œê¹…ì„ ì„¤ì •í•˜ëŠ” í†µí•© í•¨ìˆ˜"""
    if args.no_logs:
        return None
    
    # log í´ë” ìƒì„±
    script_dir = Path(__file__).parent.parent
    log_dir = script_dir / "log"
    log_dir.mkdir(exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # í„°ë¯¸ë„ ì¶œë ¥ ë¡œê¹… ì„¤ì •
    output_dir = os.environ.get('TERMINAL_OUTPUT_DIR', 'log')
    terminal_output_file = os.path.join(output_dir, f'python_output_{timestamp}.log')
    
    # í„°ë¯¸ë„ ì¶œë ¥ì„ íŒŒì¼ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜
    import sys
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    
    class TeeOutput:
        def __init__(self, *files):
            self.files = files
        def write(self, obj):
            for f in self.files:
                f.write(obj)
                f.flush()
        def flush(self):
            for f in self.files:
                f.flush()
    
    output_file = open(terminal_output_file, 'w', encoding='utf-8')
    sys.stdout = TeeOutput(original_stdout, output_file)
    sys.stderr = TeeOutput(original_stderr, output_file)
    
    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    graph_name = Path(args.graph).stem
    log_filename = f"{graph_name}_{args.treatment}_{timestamp}.log"
    log_filepath = log_dir / log_filename
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=20,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filepath, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¶„ì„ ì‹œì‘ - {timestamp}")
    logger.info(f"ë°ì´í„°: {args.data_dir}, ê·¸ë˜í”„: {args.graph}")
    logger.info(f"ì²˜ì¹˜: {args.treatment}, ê²°ê³¼: {args.outcome}, ì¶”ì •ë°©ë²•: {args.estimator}")
    
    return logger


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="DoWhy ì¸ê³¼ì¶”ë¡  ë¶„ì„")
    
    parser.add_argument('--data-dir', type=str, required=True, help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--graph', type=str, required=True, help='ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--estimator', type=str, choices=['tabpfn', 'linear_regression', 'propensity_score', 'instrumental_variable'],
                       default='linear_regression', help='ì¶”ì • ë°©ë²•')
    parser.add_argument('--treatment', type=str, required=True, help='ì²˜ì¹˜ ë³€ìˆ˜ëª…')
    parser.add_argument('--outcome', type=str, required=True, help='ê²°ê³¼ ë³€ìˆ˜ëª…')
    parser.add_argument('--no-logs', action='store_true', help='ë¡œê·¸ ì €ì¥ ë¹„í™œì„±í™”')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ì¶œë ¥ í™œì„±í™”')
    
    return parser.parse_args()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    args = parse_arguments()
    logger = setup_logging(args)
    
    try:
        print(f"\nğŸš€ DoWhy ì¸ê³¼ì¶”ë¡  ë¶„ì„ ì‹œì‘")
        print(f"ğŸ“Š ë°ì´í„° ë””ë ‰í† ë¦¬: {args.data_dir}")
        print(f"ğŸ•¸ï¸ ê·¸ë˜í”„: {args.graph}")
        print(f"ğŸ¯ ì²˜ì¹˜: {args.treatment}, ğŸ“ˆ ê²°ê³¼: {args.outcome}")
        print(f"ğŸ”§ ì¶”ì •ë°©ë²•: {args.estimator}")
        print(f"ğŸ“¦ DoWhy ë²„ì „: {dowhy.__version__}")
        print("="*60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ì¤‘...")
        structured_data, unstructured_data, causal_graph = load_all_data(
            args.data_dir,
            args.graph
        )
        
        # 2. ë¹„ì •í˜• ë°ì´í„° ì „ì²˜ë¦¬
        print("2ï¸âƒ£ ë¹„ì •í˜• ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        processed_dfs = preprocess_unstructured_data(unstructured_data, args.data_dir)
        
        # 3. ë°ì´í„° ë³‘í•©
        print("3ï¸âƒ£ ë°ì´í„° ë³‘í•© ì¤‘...")
        merged_df = merge_all_data(structured_data, processed_dfs)
        print(f"âœ… ìµœì¢… ë³‘í•© ë°ì´í„°: {len(merged_df)}ê±´, {len(merged_df.columns)}ê°œ ë³€ìˆ˜")
        
        if logger:
            logger.info("="*60)
            logger.info("ë°ì´í„° ë¡œë“œ ë° ë³‘í•© ì™„ë£Œ")
            logger.info("="*60)
            logger.info(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {merged_df.shape}")
            logger.info(f"ë…¸ë“œ ìˆ˜: {causal_graph.number_of_nodes()}")
            logger.info(f"ì—£ì§€ ìˆ˜: {causal_graph.number_of_edges()}")
        
        # 4. ì¸ê³¼ëª¨ë¸ ìƒì„± ë° ë¶„ì„
        print("4ï¸âƒ£ ì¸ê³¼ëª¨ë¸ ìƒì„± ì¤‘...")
        model = CausalModel(
            data=merged_df,
            treatment=args.treatment,
            outcome=args.outcome,
            graph=causal_graph
        )
        
        printå·¥å» 5ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì‹ë³„ ì¤‘...")
        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)
        
        print("6ï¸âƒ£ ì¸ê³¼íš¨ê³¼ ì¶”ì • ì¤‘...")
        estimate = estimation.estimate_causal_effect(
            model,
            identified_estimand,
            args.estimator,
            logger
        )
        
        print("7ï¸âƒ£ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        validation_results = estimation.run_validation_tests(
            model,
            identified_estimand,
            estimate,
            logger
        )
        
        print("8ï¸âƒ£ ë¯¼ê°ë„ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
        sensitivity_df = estimation.run_sensitivity_analysis(
            model,
            identified_estimand,
            estimate,
            logger
        )
        
        print("9ï¸âƒ£ ì‹œê°í™” ìƒì„± ì¤‘...")
        heatmap_path = estimation.create_sensitivity_heatmap(
            sensitivity_df,
            logger
        ) if not sensitivity_df.empty else None
        
        print("ğŸ”Ÿ ìµœì¢… ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥ ì¤‘...")
        estimation.print_summary_report(estimate, validation_results, sensitivity_df)
        
        if logger:
            logger.info("ë¶„ì„ ì™„ë£Œ")
        
        print(f"\nâœ… ì „ì²´ ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        if logger:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
